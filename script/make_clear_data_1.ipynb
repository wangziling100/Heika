{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from six.moves import xrange\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "# from minisom import MiniSom\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ability_level_mapper(data, groups=None, col='front', how='naive', n_level=19, \n",
    "                         invert=True, parameters=None, divide_points=None, \n",
    "                         target_col_name='performance', balanced_scale=True, avg_perf=None):\n",
    "    \"\"\"\n",
    "    balanced_scale: allow to have nagative grades\n",
    "    \"\"\"\n",
    "    \n",
    "    # the raw data is divided into groups according to its exc_num, ability levels are calculated respectively\n",
    "    # how: 1 is mapping without any other processing, called 'naive'\n",
    "    origin = data.copy()\n",
    "    # if target col is imbalanced, here we seperate it in parts with different scale value\n",
    "    if parameters is not None:\n",
    "        col, how, n_level, invert, v_max, v_min, divide_points, balanced_scale, avg_perf = parameters\n",
    "        \n",
    "    if divide_points is not None:\n",
    "        assert n_level==len(divide_points), 'false length of imbalanced_data'\n",
    "        # elements in inbalaced_data are ordered increasely\n",
    "        origin[target_col_name] = 0\n",
    "        for i,v in enumerate(divide_points):\n",
    "            if invert:\n",
    "                origin.loc[(origin.performance==0) & (origin[col]<=v), target_col_name] = n_level-i+1\n",
    "            else:\n",
    "                origin.loc[(origin.performance==0) & (origin[col]<=v), target_col_name] = i+1\n",
    "        if invert:\n",
    "            origin.loc[origin.performance==0, target_col_name] = 1\n",
    "        else:\n",
    "            origin.loc[origin.performance==0, target_col_name] = n_level+1\n",
    "            \n",
    "        if balanced_scale and avg_perf is not None:\n",
    "            origin[target_col_name] -= avg_perf\n",
    "\n",
    "        parameters = (col, how, n_level, invert, None, None, divide_points, balanced_scale, avg_perf)\n",
    "        return origin, parameters\n",
    "        \n",
    "    if how == 'cluster':\n",
    "        origin = origin.reset_index()\n",
    "        centroids,_ = kmeans(whiten(origin[col]), k_or_guess=n_level+1)\n",
    "#         origin = origin.sort_values(by=[col])\n",
    "        clx,_ = vq(whiten(origin[col]),centroids)\n",
    "        \n",
    "        origin[target_col_name] = clx+1\n",
    "        tmp = origin[[target_col_name, col]]\n",
    "        \n",
    "        tmp = tmp[[target_col_name, col]].groupby([target_col_name]).mean()\n",
    "        tmp['tmp'] = 1\n",
    "        tmp = tmp.sort_values(by=col)\n",
    "        tmp = tmp['tmp'].cumsum().reset_index()\n",
    "        origin = origin.merge(tmp, how='left', on=[target_col_name])\n",
    "        origin[target_col_name] = origin['tmp']\n",
    "#         origin = origin.sort_values(by=['index'])\n",
    "        origin = origin.set_index(['index'])\n",
    "        divide_points = origin.groupby([target_col_name])[col].max().tolist()\n",
    "        divide_points = divide_points[:-1]\n",
    "        \n",
    "        if balanced_scale:\n",
    "            if avg_perf is None:\n",
    "                avg_perf = origin[target_col_name].mean()\n",
    "                avg_perf = int(avg_perf)\n",
    "            origin[target_col_name] -= avg_perf\n",
    "        else:\n",
    "            avg_perf = None\n",
    "        parameters = (col, how, n_level, invert, None, None, divide_points, balanced_scale, avg_perf)\n",
    "        \n",
    "        return origin, parameters\n",
    "        \n",
    "        \n",
    "    if groups == None:\n",
    "        v_max = origin[col].max()\n",
    "        v_min = origin[col].min()\n",
    "        \n",
    "        interval = (v_max- v_min)/n_level\n",
    "        assert interval!=0, 'zero dividend'\n",
    "        origin[target_col_name] = (origin[col]-v_min)/interval\n",
    "\n",
    "        origin[target_col_name] = origin[target_col_name].astype(int)\n",
    "        if invert:\n",
    "            origin[target_col_name] = n_level-origin[target_col_name]+1\n",
    "        else:\n",
    "            origin[target_col_name] = origin[target_col_name]+1\n",
    "\n",
    "        if balanced_scale:\n",
    "            if avg_perf is None:\n",
    "                avg_perf = origin[target_col_name].mean()\n",
    "                avg_perf = int(avg_perf)\n",
    "            origin[target_col_name] -= avg_perf\n",
    "        else:\n",
    "            avg_perf = None\n",
    "        parameters = (col, how, n_level, invert, None, None, divide_points, balanced_scale, avg_perf)\n",
    "        return origin, parameters\n",
    "    \n",
    "    if how == 1 or how=='naive':\n",
    "        tmp = origin[col]\n",
    "        for index, group in groups:\n",
    "            \n",
    "            v_max = group[col].max()\n",
    "            v_min = group[col].min()\n",
    "            \n",
    "            interval = (v_max-v_min)/n_level\n",
    "            \n",
    "            assert interval!=0, 'zero dividend'\n",
    "            \n",
    "            origin.loc[index, col] = (origin.loc[index, col]-v_min)/interval\n",
    "        origin[col] = origin[col].astype(int)\n",
    "        if invert:\n",
    "            origin[target_col_name] = n_level-origin[col]+1\n",
    "            \n",
    "        else:\n",
    "            origin[target_col_name] = origin[target_col_name]+1\n",
    "        origin[col] = tmp\n",
    "        \n",
    "        if balanced_scale:\n",
    "            if avg_perf is None:\n",
    "                avg_perf = origin[target_col_name].mean()\n",
    "                avg_perf = int(avg_perf)\n",
    "            origin[target_col_name] -= avg_perf\n",
    "        else:\n",
    "            avg_perf = None\n",
    "        parameters = (col, how, n_level, invert, None, None, divide_points, balanced_scale, avg_perf)\n",
    "        \n",
    "        return origin, parameters\n",
    "    \n",
    "\n",
    "def calc_force(df):\n",
    "    df['force'] = df['Fx']**2 + df['Fy']**2 + df['Fz']**2\n",
    "    df['force'] = df['force'].pow(1/2)\n",
    "    return df\n",
    "\n",
    "def calc_resultent_force(df):\n",
    "    # calc resultent force\n",
    "    df['resultent_force'] = (df['Fx']-df['Fx.1'])**2+(df['Fy']-df['Fy.1'])**2+(df['Fz']-df['Fz.1'])**2\n",
    "    df['resultent_force'] = df['resultent_force'].pow(1/2)\n",
    "    return df\n",
    "\n",
    "def calc_resultent_force_xy(df):\n",
    "    # calc resultent force on flat xoy\n",
    "    df['resultent_force_xy'] = (df['Fx']-df['Fx.1'])**2+(df['Fy']-df['Fy.1'])**2\n",
    "    df['resultent_force_xy'] = df['resultent_force_xy'].pow(1/2)\n",
    "    return df\n",
    "\n",
    "\n",
    "def calc_velocity(df, del_low=True):\n",
    "    df['velocity'] = df['Lx']**2 + df['Ly']**2 + df['Lz']**2\n",
    "    df['velocity'] = df['velocity'].pow(1/2)\n",
    "    if del_low:\n",
    "        df = del_low_velocity(df)\n",
    "    return df\n",
    "\n",
    "def calc_velocity_xy(df):\n",
    "    df['velocity_xy'] = df['Lx']**2 + df['Ly']**2\n",
    "    df['velocity_xy'] = df['velocity_xy'].pow(1/2)\n",
    "    return df\n",
    "\n",
    "def calc_force_velocity_angle_xy(df):\n",
    "    # calculate angle between walk direction and force direction\n",
    "    if 'resultent_force_xy' not in df.columns:\n",
    "        df = calc_resultent_force_xy(df)\n",
    "        \n",
    "    if 'velocity_xy' in df.columns: \n",
    "        df = calc_velocity_xy(df)\n",
    "    \n",
    "    df['angle_fv_xy'] = ((df['Fx']-df['Fx.1'])*df['Lx']+(df['Fy']-df['Fy.1'])*df['Ly'])/  \\\n",
    "        (df['velocity_xy']*df['resultent_force_xy'])\n",
    "    df['angle_fv_xy'] = df['angle_fv_xy'].apply(np.arccos)\n",
    "    tmp = df['angle_fv_xy'][df['angle_fv_xy']>np.pi/2]\n",
    "    tmp -= np.pi\n",
    "    \n",
    "    # here is maybe a bug\n",
    "    df.loc[df.angle_fv_xy>np.pi/2, 'angle_fv_xy'] = tmp\n",
    "    del tmp\n",
    "    return df\n",
    "\n",
    "def calc_torque_xy(df, threshold=0):\n",
    "    df['torque_xy'] = df['Mz']-df['DMz']\n",
    "    return df\n",
    "\n",
    "def calc_torque_xy_avg(df, threshold=0, n=100, dropna=True):\n",
    "    print('torque')\n",
    "    print(len(df))\n",
    "    if 'torque_xy' not in df.columns:\n",
    "        df = calc_torque_xy(df)\n",
    "        \n",
    "    df['torque_xy_avg'] = df.groupby(['uid','day','exc_num', 'exc_times'])['torque_xy'].\\\n",
    "        rolling(n).mean().reset_index()['torque_xy']\n",
    "        \n",
    "    if threshold!=0:\n",
    "        assert threshold>0, 'threshold must be positive'\n",
    "        df[(df.torque_xy_avg<threshold) & (df.torque_xy_avg>-threshold)] = 1\n",
    "        df[df.torque_xy_avg<=-threshold] = 0\n",
    "        df[df.torque_xy_avg>=threshold] = 2\n",
    "        \n",
    "    if dropna:\n",
    "        df = df.dropna()\n",
    "    print(len(df.dropna()))\n",
    "    return df\n",
    "\n",
    "def calc_abs_force_velocity_angle_xy(df):\n",
    "    # calculate absolute angle between walk direction and force direction\n",
    "    if 'resultent_force_xy' not in df.columns:\n",
    "        df = calc_resultent_force_xy(df)\n",
    "        \n",
    "    if 'velocity_xy' in df.columns: \n",
    "        df = calc_velocity_xy(df)\n",
    "    \n",
    "    df['abs_angle_fv_xy'] = ((df['Fx']-df['Fx.1'])*df['Lx']+(df['Fy']-df['Fy.1'])*df['Ly'])/  \\\n",
    "        (df['velocity_xy']*df['resultent_force_xy'])\n",
    "    df['abs_angle_fv_xy'] = abs(df['abs_angle_fv_xy'])\n",
    "    df['abs_angle_fv_xy'] = df['abs_angle_fv_xy'].apply(np.arccos)\n",
    "    return df\n",
    "\n",
    "def calc_torque_turning_freq(df,interval=100, groups=None, dropna=True):\n",
    "    # calculate the frequency of the changing of the direction of the torque on flat xoy\n",
    "    \n",
    "    print('torque turning freq:')\n",
    "    print(df.shape)\n",
    "    result = pd.Series()\n",
    "    if 'torque_xy' not in df.columns:\n",
    "        df = calc_torque_xy(df)\n",
    "    if groups == None:\n",
    "        groups = df[['uid', 'day', 'exc_num', 'exc_times', 'torque_xy']].   \\\n",
    "            groupby(['uid', 'day', 'exc_num', 'exc_times'])\n",
    "        \n",
    "    for index, group in groups:\n",
    "        # calculate the direction of the changing of the values\n",
    "        df_tmp = group.reset_index()\n",
    "        df_tmp['tt_freq'] = df_tmp['torque_xy']\n",
    "        df_tmp.loc[0, 'tt_freq'] = 0\n",
    "        tmp1 = df_tmp['torque_xy'][:-1].reset_index(drop=True)\n",
    "        tmp2 = df_tmp['torque_xy'][1:].reset_index(drop=True)\n",
    "\n",
    "        tmp3 = tmp2-tmp1\n",
    "        tmp3.index = tmp3.index+1\n",
    "        df_tmp.loc[1:, 'tt_freq'] = tmp3\n",
    "\n",
    "        # delete 0 value, and compare two adjecend value, if the direction changes, then the product is negative.\n",
    "        tmp3 = df_tmp[df_tmp['tt_freq']!=0][['tt_freq']]\n",
    "        tmp3 = tmp3.reset_index()\n",
    "        tmp1 = tmp3['tt_freq'][:-1].reset_index()\n",
    "        tmp2 = tmp3['tt_freq'][1:].reset_index()\n",
    "        tmp4 = tmp1*tmp2\n",
    "        tmp4.index += 1\n",
    "        tmp3[0, 'tt_freq'] = 0\n",
    "        tmp3.loc[1:, 'tt_freq'] = tmp4\n",
    "        tmp3.index = tmp3['index']\n",
    "        df_tmp.loc[df_tmp.tt_freq!=0, 'tt_freq'] = tmp3['tt_freq']\n",
    "\n",
    "        # count in interval\n",
    "        df_tmp.loc[df_tmp.tt_freq>=0, 'tt_freq'] = np.nan\n",
    "#         df_tmp['tt_freq'] = df_tmp['tt_freq'].rolling(interval).count()\n",
    "        df_tmp = df_tmp.set_index(['index'])\n",
    "        df_tmp = group_rolling_count(df_tmp, 'tt_freq', 'tt_freq', by=['uid', 'day', 'exc_num', 'exc_times'], n=interval)\n",
    "#         df_tmp.index = df_tmp['index']\n",
    "        \n",
    "        result = pd.concat([result, df_tmp['tt_freq']])\n",
    "        del tmp1\n",
    "        del tmp2\n",
    "        del tmp3\n",
    "        del tmp4\n",
    "#     print(result)\n",
    "    # index aligning\n",
    "    df['tt_freq'] = result\n",
    "#     print(all_data[['torque_xy','tt_freq']])\n",
    "\n",
    "    if dropna:\n",
    "        df = df.dropna()\n",
    "    print(df.shape)        \n",
    "    del result\n",
    "    return df   \n",
    "\n",
    "def calc_abs_angle_rotation_velocity_xy(df, groups=None):\n",
    "    # calculate relative absolute angle rotation of velocity on flat xoy\n",
    "    \n",
    "    if 'velocity_xy' not in df.columns:\n",
    "        df = calc_velocity_xy(df)\n",
    "        \n",
    "    result = pd.Series()\n",
    "    if groups == None:\n",
    "        groups = df[['uid', 'day', 'exc_num', 'exc_times', 'Lx', 'Ly', 'velocity_xy']].   \\\n",
    "            groupby(['uid', 'day', 'exc_num', 'exc_times'])\n",
    "        \n",
    "    for index, group in groups:\n",
    "        # calculate angle rotation from two adjecend time point\n",
    "        df_tmp = group.reset_index()\n",
    "        df_tmp['v_rotation'] = 0\n",
    "        tmp1 = df_tmp.loc[0:len(df_tmp)-2, ['Lx', 'Ly', 'velocity_xy']].reset_index(drop=True)\n",
    "        tmp2 = df_tmp.loc[1:, ['Lx', 'Ly', 'velocity_xy']].reset_index(drop=True)\n",
    "    \n",
    "        tmp3 = (tmp1['Lx']*tmp2['Lx']+tmp1['Ly']*tmp2['Ly'])/  \\\n",
    "        (tmp1['velocity_xy']*tmp2['velocity_xy'])\n",
    "#         tmp3 = tmp3.fillna(0)\n",
    "#         print(tmp3)\n",
    "        tmp3 = tmp3.apply(np.arccos)\n",
    "#         print(tmp3)\n",
    "        tmp3.index += 1\n",
    "        df_tmp.loc[1:, 'v_rotation'] = tmp3\n",
    "        df_tmp.loc[df_tmp.v_rotation>np.pi/2, 'v_rotation'] -= np.pi\n",
    "        df_tmp.loc[df_tmp.v_rotation<0.0001, 'v_rotation'] = 0\n",
    "        df_tmp['v_rotation'] = df_tmp['v_rotation'].fillna(0)\n",
    "#         print(df_tmp.loc[:, ['v_rotation', 'Lx', 'Ly']])\n",
    "        \n",
    "        df_tmp.index = df_tmp['index']\n",
    "        result = pd.concat([result, df_tmp['v_rotation']])\n",
    "        del tmp1\n",
    "        del tmp2\n",
    "        del tmp3\n",
    "        del df_tmp\n",
    "    df['v_abs_rotation'] = result\n",
    "    del result\n",
    "    return df\n",
    "\n",
    "def calc_angle_rotation_velocity_xy(df, groups=None):\n",
    "    # calculate relative angle rotation of velocity on flat xoy\n",
    "    \n",
    "    if 'velocity_xy' not in df.columns:\n",
    "        df = calc_velocity_xy(df)\n",
    "        \n",
    "    result = pd.Series()\n",
    "    if groups == None:\n",
    "        groups = df[['uid', 'day', 'exc_num', 'exc_times', 'Lx', 'Ly', 'velocity_xy']].   \\\n",
    "            groupby(['uid', 'day', 'exc_num', 'exc_times'])\n",
    "        \n",
    "    for index, group in groups:\n",
    "        # calculate angle rotation from two adjecend time point\n",
    "        df_tmp = group.reset_index()\n",
    "        df_tmp['v_rotation'] = 0\n",
    "#         tmp1 = df_tmp['Ly']/df_tmp['Lx']\n",
    "        tmp1 = np.arctan2(df_tmp['Ly'], df_tmp['Lx'])\n",
    "        tmp2 = tmp1[1:].reset_index(drop=True) - tmp1[:-1].reset_index(drop=True)\n",
    "\n",
    "        tmp2.index += 1\n",
    "        df_tmp.loc[1:, 'v_rotation'] = tmp2\n",
    "        df_tmp['v_rotation'] = df_tmp['v_rotation'].fillna(0)\n",
    "#         print(df_tmp.loc[:, ['v_rotation', 'Lx', 'Ly']])\n",
    "        \n",
    "        df_tmp.index = df_tmp['index']\n",
    "        result = pd.concat([result, df_tmp['v_rotation']])\n",
    "        del tmp1\n",
    "        del tmp2\n",
    "        del df_tmp\n",
    "    df['v_rotation'] = result\n",
    "    df.loc[df.v_rotation<-np.pi, 'v_rotation'] += 2*np.pi\n",
    "    df.loc[df.v_rotation>np.pi, 'v_rotation'] -= 2*np.pi\n",
    "    del result\n",
    "    return df\n",
    "\n",
    "def calc_velocity_angle(df):\n",
    "#     tan = df['Ly']/df['Lx']\n",
    "    df['v_angle'] = np.arctan2(df['Ly'], df['Lx'])\n",
    "#     del tan\n",
    "    return df\n",
    "\n",
    "def calc_velocity_angle_without_backward(df):\n",
    "    # here we calculate velocity angle regardless of move direction\n",
    "    if 'v_angle' not in df.columns:\n",
    "        df = calc_velocity_angle(df)\n",
    "        \n",
    "    df['va_wo'] = df['v_angle']\n",
    "    df.loc[(df.v_angle>np.pi/2) , 'va_wo'] = np.pi - df.loc[(df.v_angle>np.pi/2) , 'va_wo']\n",
    "    df.loc[(df.v_angle<-np.pi/2), 'va_wo'] += np.pi\n",
    "    return df\n",
    "\n",
    "def calc_sd_velocity(df, by=['day', 'uid', 'exc_num', 'exc_times'], n=100, dropna=True):\n",
    "    print('sd')\n",
    "    print(len(df))\n",
    "    if 'velocity' not in df.columns:\n",
    "        df = calc_velocity(df)\n",
    "#     df['v_sd'] = df.groupby(['uid','day','exc_num', 'exc_times'])['velocity'].\\\n",
    "#         rolling(n).std().reset_index()['velocity']\n",
    "    df = group_rolling_std(df, 'velocity', 'v_sd', by, n)\n",
    "    if dropna:\n",
    "        df = df.dropna()\n",
    "    print(len(df.dropna()))\n",
    "    return df\n",
    "\n",
    "def calc_velocity_skew(df, by=['day', 'uid', 'exc_num', 'exc_times'], n=100, dropna=True):\n",
    "    print('skew')\n",
    "    print(len(df))\n",
    "    if 'velocity' not in df.columns:\n",
    "        df = calc_velocity(df)\n",
    "#     df['v_skew'] = df.groupby(['uid','day','exc_num', 'exc_times'])['velocity'].\\\n",
    "#         rolling(n).skew().reset_index()['velocity']\n",
    "    df = group_rolling_skew(df, 'velocity', 'v_skew', by, n)\n",
    "    if dropna:\n",
    "        df = df.dropna()\n",
    "    print(len(df.dropna()))\n",
    "    return df\n",
    "\n",
    "def calc_velocity_kurtosis(df, by=['day', 'uid', 'exc_num', 'exc_times'], n=100, dropna=True):\n",
    "    print('kurtosis')\n",
    "    print(len(df))\n",
    "    if 'velocity' not in df.columns:\n",
    "        df = calc_velocity(df)\n",
    "#     df['v_kurt'] = df.groupby(['uid','day','exc_num', 'exc_times'])['velocity'].\\\n",
    "#         rolling(n).skew().reset_index()['velocity']\n",
    "\n",
    "    df = group_rolling_kurt(df, 'velocity', 'v_kurt', by, n)\n",
    "    if dropna:\n",
    "        df = df.dropna()\n",
    "    print(len(df.dropna()))\n",
    "    return df\n",
    "\n",
    "def calc_avg_velocity(df, by=['day', 'uid', 'exc_num', 'exc_times'], n=100, dropna=True):\n",
    "    if 'velocity' not in df.columns:\n",
    "        df = calc_velocity(df)\n",
    "        \n",
    "#     df['avg_velocity'] = df.groupby(['uid','day','exc_num', 'exc_times']).\\\n",
    "#         rolling(n).mean().reset_index(drop=True)['velocity']\n",
    "    \n",
    "    df = group_rolling_mean(df, 'velocity', 'avg_velocity', by, n)\n",
    "    \n",
    "    if dropna:\n",
    "        df = df.dropna()\n",
    "    print(len(df.dropna()))\n",
    "\n",
    "    return df\n",
    "\n",
    "def calc_velocity_deviation_score(df, a=10**-8, avg=False):\n",
    "    if avg:\n",
    "        if 'avg_velocity' not in df.columns:\n",
    "            df = calc_avg_velocity(df)\n",
    "        \n",
    "        df['vd_score'] = df['avg_velocity']/(a+df['deviation'])\n",
    "    else:\n",
    "        df = stardard_scaler(df, target_col='velocity')\n",
    "        df = stardard_scaler(df, target_col='deviation')\n",
    "        df['vd_score'] = df['velocity']/(a+df['deviation'])\n",
    "    return df\n",
    "\n",
    "def calc_reciprocal_deviation(df, bias=0.1):\n",
    "    if 'deviation' not in df.columns:\n",
    "        df = get_small_deviation(df)\n",
    "    \n",
    "    df['re_dev'] = 1/(df['deviation']+bias)\n",
    "    return df\n",
    "\n",
    "def calc_inverted_tt_freq(df, bias=0):\n",
    "    if 'tt_freq' not in df.columns:\n",
    "        df = calc_torque_turning_freq(df)\n",
    "    df['inv_tt_freq'] = df['tt_freq'].max()-df['tt_freq']+bias\n",
    "    return df\n",
    "\n",
    "def calc_inverted_force(df, bias=0):\n",
    "    if 'force' not in df.columns:\n",
    "        df = calc_force(df)\n",
    "    df['inv_force'] = df['force'].max()-df['force']+bias\n",
    "    return df\n",
    "\n",
    "def calc_score(df, avg=False):\n",
    "    if avg:\n",
    "        if 'avg_velocity' not in df.columns:\n",
    "            df = calc_avg_velocity(df)\n",
    "\n",
    "        df['score'] = df['avg_velocity']*df['deviation']\n",
    "    else:\n",
    "        df['score'] = df['velocity']*df['deviation']\n",
    "    return df\n",
    "\n",
    "def calc_assess_feature(data, c=0.1):\n",
    "    print('assess:', data.shape)\n",
    "    if 'velocity' not in data.columns:\n",
    "        data = calc_velocity(data)\n",
    "    if 'force' not in data.columns:\n",
    "        data = calc_force(data)\n",
    "    if 're_dev' not in data.columns:\n",
    "        data = calc_reciprocal_deviation(data)\n",
    "        \n",
    "    data = stardard_scaler(data, target_col='velocity')\n",
    "    data = stardard_scaler(data, target_col='re_dev')    \n",
    "    data = stardard_scaler(data, target_col='force')\n",
    "    data['assess'] = data['velocity']*data['re_dev'] / ((data['force']+c))\n",
    "    data = data.replace([-np.inf, np.inf], [np.nan, np.nan])\n",
    "    data = data.dropna()\n",
    "    print(data.loc[data.assess==data['assess'].max(),])\n",
    "    print('assess(drop_na):', data.shape)\n",
    "    return data\n",
    "\n",
    "def calc_angle_velocity_xy(df):\n",
    "    df['angle_velocity_xy'] = df['Az']\n",
    "    return df\n",
    "\n",
    "def calc_avg_angle_velocity_xy(df, by=['day', 'uid', 'exc_num', 'exc_times'], n=100, dropna=True):\n",
    "    print('angle_velocity')\n",
    "    print(len(df))\n",
    "    if 'angle_velocity_xy' not in df.columns:\n",
    "        df = calc_angle_velocity_xy(df)\n",
    "        \n",
    "    df = group_rolling_mean(df, 'Az', 'avg_av_xy', by, n)\n",
    "    if dropna:\n",
    "        df = df.dropna()\n",
    "    print(len(df.dropna()))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calc_va_sum(df, by=['day', 'uid', 'exc_num', 'exc_times'], n=100, dropna=True):\n",
    "    print('velocity angle cumsum')\n",
    "    print(len(df))\n",
    "    if 'va_wo' not in df.columns:\n",
    "        df = calc_velocity_angle_without_backward(df)\n",
    "        \n",
    "    df = group_rolling_sum(df, 'va_wo', 'sum_va', by, n)\n",
    "    if dropna:\n",
    "        df = df.dropna()\n",
    "    print(len(df.dropna()))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calc_confidence(df):\n",
    "    mx = df['front'].max()\n",
    "    mi = df['front'].min()\n",
    "    df['conf'] = 1-(df['front']-mi)/(mx-mi)\n",
    "    return df\n",
    "\n",
    "def get_small_deviation(df):\n",
    "    assert 'left' in df.columns, 'lack necessary columns'\n",
    "    assert 'right' in df.columns, 'lack necessary columns'\n",
    "    assert 'front' in df.columns, 'lack necessary columns'\n",
    "    print('get small deviation')\n",
    "    print(df.shape)\n",
    "    df['deviation'] = df[['front', 'left', 'right']].min(axis=1)\n",
    "    df = df.dropna(subset=['deviation'])\n",
    "    print(df.shape)\n",
    "    return df\n",
    "\n",
    "def del_outlier(df, by, test_col, threshold=10):\n",
    "    \n",
    "    # assume that the distribution of test column in each group are normal, \n",
    "    # the data outside 2 standard deviation are considered as outlier.\n",
    "    # the data with too few samples will be deleted\n",
    "    print('delete outlier')\n",
    "    print(len(df))\n",
    "    \n",
    "    if by is None:\n",
    "        mean = df[test_col].mean()\n",
    "        std = df[test_col].std()\n",
    "        \n",
    "        left = mean-2*std\n",
    "        right = mean+2*std\n",
    "        \n",
    "        df = df[(df[test_col]>left) & (df[test_col]<right)]\n",
    "        df = df.reset_index(drop=True)\n",
    "        print(len(df))\n",
    "        return df\n",
    "    \n",
    "    cols = df.columns\n",
    "    # delete data with a few samples\n",
    "    count = df[by+[test_col]].groupby(by).count()\n",
    "    count = count[count[test_col]>threshold].reset_index()\n",
    "    count = count[by]\n",
    "    df = df.merge(count, on=by)\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    \n",
    "    # delete outlier\n",
    "    mean = df[by+[test_col]].groupby(by).mean()\n",
    "    std = df[by+[test_col]].groupby(by).std()\n",
    "    left = mean-2*std\n",
    "    right = mean+2*std\n",
    "    left = left.reset_index()\n",
    "    right = right.reset_index()\n",
    "    left.columns = by+['left']\n",
    "    right.columns = by+['right']\n",
    "    \n",
    "    df = df.merge(left, on=by)\n",
    "    df = df.merge(right, on=by)\n",
    "    df['left'] = df[test_col]-df['left']\n",
    "    df['right'] = df[test_col] - df['right']\n",
    "    df = df[(df['left'])>0 & (df['right']<0)]\n",
    "    df = df[cols]\n",
    "    df = df.reset_index(drop=True)\n",
    "    print(len(df))\n",
    "    return df\n",
    "    \n",
    "    \n",
    "def set_inverted_task(df, tasks=[2.4, 3.4, 4.1, 4.2, 4.3]):\n",
    "    df['inverted'] = 0\n",
    "    for task in tasks:\n",
    "        df.loc[df.exc_num==task, 'inverted'] = 1\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def set_sidewards_task(df, tasks=[4.1, 4.2]):\n",
    "    df['sidewards'] = 0\n",
    "    for task in tasks:\n",
    "        df.loc[df.exc_num==task, 'sidewards'] = 1\n",
    "        \n",
    "    return df\n",
    "\n",
    "def is_disturbed(df):\n",
    "    df['disturbed'] = 0\n",
    "    df.loc[(df.DFx>0) | (df.DFy>0) | (df.DFz>0), 'disturbed'] = 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "def is_backward(df):\n",
    "    if 'v_angle' not in df.columns:\n",
    "        df = calc_velocity_angle(df)\n",
    "    df['backward'] = 0\n",
    "    df.loc[(df.v_angle>np.pi/2) | (df.v_angle<-np.pi/2), 'backward'] = 1\n",
    "    return df\n",
    "    \n",
    "\n",
    "\n",
    "def set_difficulty(df, def_cols=['v_angle', 'v_sd', 'torque_xy_avg'], levels=[20, 20, 20]):\n",
    "    print('set difficulty')\n",
    "    print(len(df))\n",
    "    df['diff_description'] = ''\n",
    "    for col in def_cols:\n",
    "        df['diff_description'] = df['diff_description'] + '_' +col+df[col].astype(str)\n",
    "        \n",
    "    if levels is not None:\n",
    "        for col, level in zip(def_cols, levels):\n",
    "            df,_ = ability_level_mapper(df, col=col, n_level=level, target_col_name=col)\n",
    "    df['difficulty'] = 1\n",
    "    cnt = 1\n",
    "    tmp = df.groupby(def_cols)[['difficulty']].mean().cumsum().reset_index()\n",
    "    df = df.drop(['difficulty'], axis=1)\n",
    "    df = df.merge(tmp, on=def_cols)\n",
    "#     print(df.head())\n",
    "#     print(len(df.groupby(def_cols).mean()))\n",
    "#     print(len(df.groupby(def_cols+['difficulty']).mean()))\n",
    "\n",
    "    tmp = df[['difficulty', 'diff_description']].drop_duplicates()\n",
    "    print(tmp)\n",
    "    tmp.to_csv('../data/diff_def.csv', index=False)\n",
    "    del tmp\n",
    "    print('difficulty description:')\n",
    "    print(df[['difficulty', 'diff_description']].drop_duplicates())\n",
    "    print('# of diff: ' +str(len(df['difficulty'].unique())))\n",
    "    print(len(df))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def difficulty_mapper(df, cols, levels, invert=True):\n",
    "    print('difficulty mapper')\n",
    "    for col, level in zip(cols, levels):\n",
    "        df,_ = ability_level_mapper(df, col= col, n_level=level, target_col_name=col, invert=invert)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def del_low_velocity(df, thres=0.05):\n",
    "    df = df[df['velocity']>thres]\n",
    "    return df\n",
    "\n",
    "def stardard_scaler(df, target_col):\n",
    "    mx = df[target_col].max()\n",
    "    mi = df[target_col].min()\n",
    "    df[target_col] = (df[target_col]-mi)/(mx-mi)\n",
    "    return df\n",
    "\n",
    "def group_rolling_mean(df, target_col, new_col, by, n):\n",
    "    assert 'index' not in df.columns, 'check you data columns, and ensure no index column in it'\n",
    "    df = df.reset_index()\n",
    "    tmp = df.groupby(by).rolling(n).mean()[target_col]\n",
    "    tmp = tmp.reset_index().sort_values(by).reset_index(drop=True)\n",
    "    df = df.sort_values(by).reset_index(drop=True)\n",
    "    df[new_col] = tmp[target_col]\n",
    "    del tmp\n",
    "    df = df.set_index('index')\n",
    "    return df\n",
    "\n",
    "def group_rolling_kurt(df, target_col, new_col, by, n):\n",
    "    assert 'index' not in df.columns, 'check you data columns, and ensure no index column in it'\n",
    "    df = df.reset_index()\n",
    "    tmp = df.groupby(by).rolling(n).kurt()[target_col]\n",
    "    tmp = tmp.reset_index().sort_values(by).reset_index(drop=True)\n",
    "    df = df.sort_values(by).reset_index(drop=True)\n",
    "    df[new_col] = tmp[target_col]\n",
    "    del tmp\n",
    "    df = df.set_index('index')\n",
    "    return df\n",
    "\n",
    "def group_rolling_skew(df, target_col, new_col, by, n):\n",
    "    assert 'index' not in df.columns, 'check you data columns, and ensure no index column in it'\n",
    "    df = df.reset_index()\n",
    "    tmp = df.groupby(by).rolling(n).skew()[target_col]\n",
    "    tmp = tmp.reset_index().sort_values(by).reset_index(drop=True)\n",
    "    df = df.sort_values(by).reset_index(drop=True)\n",
    "    df[new_col] = tmp[target_col]\n",
    "    del tmp\n",
    "    df = df.set_index('index')\n",
    "    return df\n",
    "\n",
    "def group_rolling_std(df, target_col, new_col, by, n):\n",
    "    assert 'index' not in df.columns, 'check you data columns, and ensure no index column in it'\n",
    "    df = df.reset_index()\n",
    "    tmp = df.groupby(by).rolling(n).std()[target_col]\n",
    "    tmp = tmp.reset_index().sort_values(by).reset_index(drop=True)\n",
    "    df = df.sort_values(by).reset_index(drop=True)\n",
    "    df[new_col] = tmp[target_col]\n",
    "    del tmp\n",
    "    df = df.set_index('index')\n",
    "    return df\n",
    "\n",
    "def group_rolling_count(df, target_col, new_col, by, n):\n",
    "    assert 'index' not in df.columns, 'check you data columns, and ensure no index column in it'\n",
    "    df = df.reset_index()\n",
    "    tmp = df.groupby(by).rolling(n).count()[target_col]\n",
    "    tmp = tmp.reset_index().sort_values(by).reset_index(drop=True)\n",
    "    df = df.sort_values(by).reset_index(drop=True)\n",
    "    df[new_col] = tmp[target_col]\n",
    "    del tmp\n",
    "    df = df.set_index('index')\n",
    "    return df\n",
    "\n",
    "def group_rolling_sum(df, target_col, new_col, by, n):\n",
    "    assert 'index' not in df.columns, 'check you data columns, and ensure no index column in it'\n",
    "    df = df.reset_index()\n",
    "    tmp = df.groupby(by).rolling(n).sum()[target_col]\n",
    "    tmp = tmp.reset_index().sort_values(by).reset_index(drop=True)\n",
    "    df = df.sort_values(by).reset_index(drop=True)\n",
    "    df[new_col] = tmp[target_col]\n",
    "    del tmp\n",
    "    df = df.set_index('index')\n",
    "    return df\n",
    "\n",
    "def def_env(test_col, diff_def, n_class, group, if_del_outlier):\n",
    "    \n",
    "    tmp = [test_col, diff_def, n_class, group, if_del_outlier]\n",
    "    with open('../data/parameter/def_env.p', 'wb') as f:\n",
    "        pickle.dump(tmp, f)\n",
    "        \n",
    "    del tmp\n",
    "    return\n",
    "    \n",
    "def add_necessary_cols(all_cols, cols):\n",
    "    for col in cols:\n",
    "        if col not in all_cols:\n",
    "            all_cols.append(col)\n",
    "            \n",
    "    return all_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the initial data is already cleared according to their length and correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clear outlier that recorded in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = pd.read_csv('../data/new_all_data_original.csv')\n",
    "\n",
    "# print(len(all_data))\n",
    "# outliers = []\n",
    "\n",
    "# with open('../data/deviation_curves_outlier.csv') as f:\n",
    "#     for line in f.readlines():\n",
    "#         tmp = line.split('\\n')[0].split(' ')\n",
    "       \n",
    "#         outliers.append([float(x) for x in tmp])\n",
    "        \n",
    "# groups = all_data.set_index(['day', 'exc_num', 'exc_times', 'uid'])\n",
    "# groups = groups.groupby(by=groups.index)\n",
    "\n",
    "# new_data = pd.DataFrame()\n",
    "\n",
    "# for index, group in groups:\n",
    "#     if len(group)<10:\n",
    "#         print('short length')\n",
    "#         print(index)\n",
    "#         continue\n",
    "#     day, exc_num, exc_times, uid = index\n",
    "#     exc_num = round(exc_num, 1)\n",
    "#     if [day, exc_num, exc_times, uid] in outliers:\n",
    "#         print([day, exc_num, exc_times, uid])\n",
    "#         print('in the outlier list')\n",
    "#         pass\n",
    "# #         tmp = group.reset_index()\n",
    "# #         sns.set_style('whitegrid')\n",
    "# #         f, ax= plt.subplots(figsize = (14, 10))\n",
    "# #         ax = sns.lineplot(x=tmp.index, y=\"front\", data=tmp)\n",
    "# #         ax.set_title(index) \n",
    "#     else:\n",
    "#         curr = group.reset_index()\n",
    "#         new_data = pd.concat([new_data, curr], axis=0)\n",
    "# all_data = new_data\n",
    "# del new_data\n",
    "# len(all_data)\n",
    "# all_data.to_csv('../data/new_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../data/step1_clear_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_avg_velocity(df, interval=20):\n",
    "#     if 'velocity' not in df.columns:\n",
    "#         df = calc_velocity(df)\n",
    "        \n",
    "#     df['avg_velocity'] = df['velocity'].rolling(interval, min_periods=1).mean()\n",
    "#     return df\n",
    "\n",
    "# def find_forwards_segment(df, exc=1.1, rotation_threshold=1, new_num=0.1):\n",
    "#     assert exc is not None, 'rewrite function find_forwards_segment first'\n",
    "#     tmp = df[df['exc_num']==exc]\n",
    "#     tmp = tmp.loc[(tmp.Lx>0) & ((tmp.Az<rotation_threshold) | (tmp.Az>-rotation_threshold))]\n",
    "#     tmp['exc_num'] = new_num\n",
    "#     df = pd.concat([df, tmp])\n",
    "#     df = df.reset_index(drop=True)\n",
    "#     del tmp\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = calc_avg_velocity(df)\n",
    "# # df = find_forwards_segment(df)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('../data/step1_clear_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load precleared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Ax', 'Ay', 'Az', 'DFx', 'DFy', 'DFz', 'DMx', 'DMy', 'DMz', 'Fx', 'Fy',\n",
      "       'Fz', 'Lx', 'Ly', 'Lz', 'Mx', 'My', 'Mz', 'day', 'exc_num', 'exc_times',\n",
      "       'front', 'left', 'path.x', 'path.x1', 'path.x2', 'path.y', 'path.y1',\n",
      "       'path.y2', 'pose.theta', 'pose.x', 'pose.y', 'record.t', 'right', 't',\n",
      "       'uid', 'datetime', 'pre_dist', 'path_diff', 'pre_x', 'pre_y', 'curr_x',\n",
      "       'curr_y', 'curr_dist'],\n",
      "      dtype='object')\n",
      "[ 1.1  1.5  2.1  2.2  3.1  3.2  4.1  4.2]\n"
     ]
    }
   ],
   "source": [
    "all_data = pd.read_csv('../data/data2_path.csv')\n",
    "print(all_data.columns)\n",
    "# print(all_data[(all_data['day']==1) & \n",
    "#                (all_data['uid']==7) & \n",
    "#                (all_data['exc_num']==1.1) & \n",
    "#                (all_data['exc_times']==1)][['Az']])\n",
    "print(all_data['exc_num'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(all_data[(all_data['day']==1) & \n",
    "#                (all_data['uid']==7) & \n",
    "#                (all_data['exc_num']==1.1) & \n",
    "#                (all_data['exc_times']==2)][['Az', 'angle_velocity_xy', 'avg_av_xy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = all_data[(all_data['day']==1) & \n",
    "#                ((all_data['uid']==7) | (all_data['uid']==4)) & \n",
    "#                (all_data['exc_num']==1.1) & \n",
    "#                (all_data['exc_times']==1)]\n",
    "# test = test.groupby(['day', 'uid', 'exc_num', 'exc_times']).head().reset_index()\n",
    "# print(test.head())\n",
    "# test['test'] = test.groupby(['day', 'uid', 'exc_num', 'exc_times']).rolling(100).mean().reset_index(drop=True)['Az']\n",
    "# # test['test'] = test['Az'].rolling(2).mean()\n",
    "# test.groupby(['day', 'uid', 'exc_num', 'exc_times']).head()\\\n",
    "#     [['day', 'uid', 'exc_num', 'exc_times','Az', 'test']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "test_col = 'inv_force'\n",
    "diff_def = [#'v_angle', #'v_skew',  \n",
    "            #'v_kurt',\n",
    "            #'sum_va',\n",
    "            #'avg_av_xy', \n",
    "            'inverted', \n",
    "            'sidewards',\n",
    "            'disturbed', 'backward', 'path_diff']\n",
    "mapper_cols = [#'v_angle', #'v_skew', #'v_kurt', \n",
    "               #'sum_va',\n",
    "               #'avg_av_xy'\n",
    "              ]\n",
    "\n",
    "mapper_levels = [#5, #10, \n",
    "                 #5,\n",
    "                 #5\n",
    "                ]\n",
    "n_class = mapper_levels+[2, 2, 2, 2, 3]\n",
    "group_unit = ['day', 'uid', 'exc_num', 'exc_times']\n",
    "group_cols = group_unit + [test_col]\n",
    "diff_cols = group_cols+diff_def\n",
    "\n",
    "selected_cols = group_cols+['difficulty']\n",
    "necessary_cols = ['deviation', 'velocity' #, 'conf'\n",
    "                 ]\n",
    "selected_cols1 = add_necessary_cols(diff_cols, necessary_cols)\n",
    "selected_cols = add_necessary_cols(selected_cols, necessary_cols)\n",
    "b_del_outlier = False\n",
    "def_env(test_col, diff_def, n_class, group_cols, b_del_outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get small deviation\n",
      "(347972, 44)\n",
      "(317966, 45)\n",
      "(317966, 46)\n",
      "main\n",
      "317966\n",
      "308324\n",
      "torque turning freq:\n",
      "(306505, 46)\n",
      "(297707, 48)\n",
      "main\n",
      "297707\n",
      "set difficulty\n",
      "297707\n",
      "        difficulty                                   diff_description\n",
      "0                1  _inverted0_sidewards0_disturbed0_backward0_pat...\n",
      "94295            4  _inverted0_sidewards0_disturbed0_backward1_pat...\n",
      "128049           7  _inverted0_sidewards0_disturbed1_backward0_pat...\n",
      "134190          10  _inverted0_sidewards0_disturbed1_backward1_pat...\n",
      "136609           2  _inverted0_sidewards0_disturbed0_backward0_pat...\n",
      "194712           5  _inverted0_sidewards0_disturbed0_backward1_pat...\n",
      "200172           6  _inverted0_sidewards0_disturbed0_backward1_pat...\n",
      "209537           3  _inverted0_sidewards0_disturbed0_backward0_pat...\n",
      "220988           8  _inverted0_sidewards0_disturbed1_backward0_pat...\n",
      "222624          13  _inverted1_sidewards1_disturbed0_backward0_pat...\n",
      "236517          15  _inverted1_sidewards1_disturbed0_backward1_pat...\n",
      "248391          16  _inverted1_sidewards1_disturbed0_backward1_pat...\n",
      "267326          14  _inverted1_sidewards1_disturbed0_backward0_pat...\n",
      "297514          11  _inverted0_sidewards0_disturbed1_backward1_pat...\n",
      "297528           9  _inverted0_sidewards0_disturbed1_backward0_pat...\n",
      "297540          12  _inverted0_sidewards0_disturbed1_backward1_pat...\n",
      "297551          17  _inverted1_sidewards1_disturbed1_backward0_pat...\n",
      "297629          18  _inverted1_sidewards1_disturbed1_backward1_pat...\n",
      "difficulty description:\n",
      "        difficulty                                   diff_description\n",
      "0                1  _inverted0_sidewards0_disturbed0_backward0_pat...\n",
      "94295            4  _inverted0_sidewards0_disturbed0_backward1_pat...\n",
      "128049           7  _inverted0_sidewards0_disturbed1_backward0_pat...\n",
      "134190          10  _inverted0_sidewards0_disturbed1_backward1_pat...\n",
      "136609           2  _inverted0_sidewards0_disturbed0_backward0_pat...\n",
      "194712           5  _inverted0_sidewards0_disturbed0_backward1_pat...\n",
      "200172           6  _inverted0_sidewards0_disturbed0_backward1_pat...\n",
      "209537           3  _inverted0_sidewards0_disturbed0_backward0_pat...\n",
      "220988           8  _inverted0_sidewards0_disturbed1_backward0_pat...\n",
      "222624          13  _inverted1_sidewards1_disturbed0_backward0_pat...\n",
      "236517          15  _inverted1_sidewards1_disturbed0_backward1_pat...\n",
      "248391          16  _inverted1_sidewards1_disturbed0_backward1_pat...\n",
      "267326          14  _inverted1_sidewards1_disturbed0_backward0_pat...\n",
      "297514          11  _inverted0_sidewards0_disturbed1_backward1_pat...\n",
      "297528           9  _inverted0_sidewards0_disturbed1_backward0_pat...\n",
      "297540          12  _inverted0_sidewards0_disturbed1_backward1_pat...\n",
      "297551          17  _inverted1_sidewards1_disturbed1_backward0_pat...\n",
      "297629          18  _inverted1_sidewards1_disturbed1_backward1_pat...\n",
      "# of diff: 18\n",
      "297707\n",
      "   day  uid  exc_num  exc_times   inv_force  difficulty  deviation  velocity\n",
      "0  1.0  1.0      1.1        1.0  484.201507           1   0.005096  0.113868\n",
      "1  1.0  1.0      1.1        1.0  486.018444           1   0.005096  0.113868\n",
      "2  1.0  1.0      1.1        1.0  488.478646           1   0.005096  0.113868\n",
      "3  1.0  1.0      1.1        1.0  489.455159           1   0.005096  0.113868\n",
      "4  1.0  1.0      1.1        1.0  494.271205           1   0.005096  0.113868\n",
      "(297707, 8)\n",
      "exc and difficulty\n",
      "1.1\n",
      "[ 1  4  7 10]\n",
      "1.5\n",
      "[ 1  4  7 10]\n",
      "2.1\n",
      "[1 4 2 5]\n",
      "2.2\n",
      "[ 1  4  7  2  5  6  3 12]\n",
      "3.1\n",
      "[1 4 2 5 6 3]\n",
      "3.2\n",
      "[ 1  4  7 10  2  5  6  3  8 11  9 12]\n",
      "4.1\n",
      "[13 15 17 18]\n",
      "4.2\n",
      "[16 14]\n"
     ]
    }
   ],
   "source": [
    "all_data = get_small_deviation(all_data)\n",
    "all_data = calc_reciprocal_deviation(all_data, bias=0.2)\n",
    "print(all_data.shape)\n",
    "all_data = all_data.drop(['left', 'right'], axis=1)\n",
    "all_data = calc_force(all_data)\n",
    "print('main')\n",
    "print(len(all_data))\n",
    "print(len(all_data.dropna()))\n",
    "all_data = calc_velocity(all_data)\n",
    "# all_data = calc_resultent_force(all_data)\n",
    "# all_data = calc_resultent_force_xy(all_data)\n",
    "# all_data = calc_velocity_xy(all_data)\n",
    "# all_data = calc_force_velocity_angle_xy(all_data)\n",
    "# all_data = calc_abs_force_velocity_angle_xy(all_data)\n",
    "# all_data = calc_torque_xy(all_data)\n",
    "all_data = calc_torque_turning_freq(all_data)\n",
    "all_data = calc_inverted_tt_freq(all_data, bias=1)\n",
    "all_data = calc_inverted_force(all_data, bias=1)\n",
    "# all_data = calc_assess_feature(all_data, c=0.3)\n",
    "# all_data = calc_velocity_angle(all_data)\n",
    "# all_data = calc_velocity_angle_without_backward(all_data)\n",
    "# stop()\n",
    "# all_data = calc_sd_velocity(all_data, by=group_unit, dropna=False)\n",
    "# all_data = calc_velocity_skew(all_data, by=group_unit, dropna=False)\n",
    "# all_data = calc_velocity_kurtosis(all_data, by=group_unit, dropna=False)\n",
    "# all_data = calc_torque_xy_avg(all_data, dropna=False)\n",
    "# all_data = calc_avg_velocity(all_data, by=group_unit, dropna=False)\n",
    "# all_data = calc_avg_angle_velocity_xy(all_data, by=group_unit, dropna=False)\n",
    "# all_data = calc_va_sum(all_data, by=group_unit, dropna=False)\n",
    "all_data = set_inverted_task(all_data)\n",
    "all_data = set_sidewards_task(all_data)\n",
    "all_data = is_disturbed(all_data)\n",
    "all_data = is_backward(all_data)\n",
    "# all_data = calc_confidence(all_data)\n",
    "# all_data = calc_velocity_deviation_score(all_data, a=1)\n",
    "# all_data = calc_score(all_data)\n",
    "# all_data = all_data.dropna()\n",
    "print('main')\n",
    "print(len(all_data))\n",
    "# print(all_data['sum_va'].describe())\n",
    "# sns.set_style('whitegrid')\n",
    "# f, ax= plt.subplots(figsize = (14, 10))\n",
    "# ax = sns.violinplot(y='sum_va', data=all_data)\n",
    "# ax.set_title(index)\n",
    "# plt.show()\n",
    "# all_data = del_outlier(all_data, None, 'v_skew')\n",
    "# all_data = del_outlier(all_data, None, 'v_kurt')\n",
    "# all_data = all_data[selected_cols1]\n",
    "# all_data = difficulty_mapper(all_data, cols=mapper_cols, levels=mapper_levels)\n",
    "all_data = set_difficulty(all_data, def_cols=diff_def, levels=None)\n",
    "\n",
    "if b_del_outlier:\n",
    "    all_data = del_outlier(all_data, \n",
    "                ['difficulty', 'uid', 'day', 'exc_num', 'exc_times'], \n",
    "                test_col=test_col)\n",
    "all_data = all_data.reset_index(drop=True)\n",
    "all_data = all_data[selected_cols]\n",
    "# groups = all_data[all_data['exc_num'] == 1.1].groupby(['day', 'uid', 'exc_num', 'exc_times'])\n",
    "print(all_data.head())\n",
    "\n",
    "print(all_data.shape)\n",
    "\n",
    "print('exc and difficulty')\n",
    "for index, group in all_data.groupby(['exc_num']):\n",
    "    print(index)\n",
    "    print(group['difficulty'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>uid</th>\n",
       "      <th>exc_num</th>\n",
       "      <th>exc_times</th>\n",
       "      <th>inv_force</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>deviation</th>\n",
       "      <th>velocity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>134190</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>466.849795</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.339451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134191</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>457.763586</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.339451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134192</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>454.741457</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.339451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134193</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>452.293833</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.339451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134194</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>456.637459</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.339451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134195</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>461.555557</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.359820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134196</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>468.070467</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.359820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134197</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>487.784050</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.359820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134198</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>484.602347</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.359820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134199</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>469.010489</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.422794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134200</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>456.029825</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.422794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134201</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>458.619577</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.422794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134202</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>458.619577</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.422794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134203</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>466.062803</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.422794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134204</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>470.253615</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.250337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134205</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>472.473094</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.250337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134206</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>470.670514</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.250337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134207</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>472.732107</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.250337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134208</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>483.128702</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.250337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134209</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>486.910157</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.250337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134210</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>486.635517</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.309806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134211</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>482.025958</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.309806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134212</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>467.649789</td>\n",
       "      <td>10</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.309806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134213</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>451.954609</td>\n",
       "      <td>10</td>\n",
       "      <td>0.021742</td>\n",
       "      <td>0.309806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134214</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>445.733410</td>\n",
       "      <td>10</td>\n",
       "      <td>0.021742</td>\n",
       "      <td>0.309806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134215</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>448.195796</td>\n",
       "      <td>10</td>\n",
       "      <td>0.021742</td>\n",
       "      <td>0.309806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134216</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>460.033925</td>\n",
       "      <td>10</td>\n",
       "      <td>0.021742</td>\n",
       "      <td>0.309806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134217</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>468.688058</td>\n",
       "      <td>10</td>\n",
       "      <td>0.021742</td>\n",
       "      <td>0.309806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134218</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>484.761745</td>\n",
       "      <td>10</td>\n",
       "      <td>0.021742</td>\n",
       "      <td>0.459700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134219</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>487.682785</td>\n",
       "      <td>10</td>\n",
       "      <td>0.021742</td>\n",
       "      <td>0.459700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136579</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>494.300952</td>\n",
       "      <td>10</td>\n",
       "      <td>0.087985</td>\n",
       "      <td>0.350981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136580</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>492.978259</td>\n",
       "      <td>10</td>\n",
       "      <td>0.087985</td>\n",
       "      <td>0.298983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136581</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>490.039228</td>\n",
       "      <td>10</td>\n",
       "      <td>0.087985</td>\n",
       "      <td>0.298983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136582</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>488.107269</td>\n",
       "      <td>10</td>\n",
       "      <td>0.087985</td>\n",
       "      <td>0.298983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136583</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>488.555981</td>\n",
       "      <td>10</td>\n",
       "      <td>0.087985</td>\n",
       "      <td>0.304273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136584</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>489.565607</td>\n",
       "      <td>10</td>\n",
       "      <td>0.087985</td>\n",
       "      <td>0.304273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136585</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>495.003166</td>\n",
       "      <td>10</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.304273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136586</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>496.775397</td>\n",
       "      <td>10</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.304273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136587</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>498.911346</td>\n",
       "      <td>10</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.304273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136588</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>497.672877</td>\n",
       "      <td>10</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.304273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136589</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>493.958416</td>\n",
       "      <td>10</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.304273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136590</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>490.863485</td>\n",
       "      <td>10</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.304273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136591</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>489.854363</td>\n",
       "      <td>10</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.297573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136592</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>490.311155</td>\n",
       "      <td>10</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.297573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136593</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>495.713846</td>\n",
       "      <td>10</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.297573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136594</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>497.185039</td>\n",
       "      <td>10</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.297573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136595</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>497.055254</td>\n",
       "      <td>10</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.297573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136596</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>493.490445</td>\n",
       "      <td>10</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.306050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136597</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>487.068238</td>\n",
       "      <td>10</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.306050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136598</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>485.775739</td>\n",
       "      <td>10</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.306050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136599</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>489.893901</td>\n",
       "      <td>10</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.341194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136600</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>495.635404</td>\n",
       "      <td>10</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.341194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136601</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>497.869024</td>\n",
       "      <td>10</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.341194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136602</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>497.246110</td>\n",
       "      <td>10</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.341194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136603</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>494.258693</td>\n",
       "      <td>10</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.327751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136604</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>491.642466</td>\n",
       "      <td>10</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.327751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136605</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>491.212255</td>\n",
       "      <td>10</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.327751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136606</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>491.212255</td>\n",
       "      <td>10</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.345371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136607</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>492.598804</td>\n",
       "      <td>10</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.345371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136608</th>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>493.153565</td>\n",
       "      <td>10</td>\n",
       "      <td>0.113235</td>\n",
       "      <td>0.345371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2419 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        day   uid  exc_num  exc_times   inv_force  difficulty  deviation  \\\n",
       "134190  1.0   1.0      1.5        1.0  466.849795          10   0.028111   \n",
       "134191  1.0   1.0      1.5        1.0  457.763586          10   0.028111   \n",
       "134192  1.0   1.0      1.5        1.0  454.741457          10   0.028111   \n",
       "134193  1.0   1.0      1.5        1.0  452.293833          10   0.028111   \n",
       "134194  1.0   1.0      1.5        1.0  456.637459          10   0.028111   \n",
       "134195  1.0   1.0      1.5        1.0  461.555557          10   0.028111   \n",
       "134196  1.0   1.0      1.5        1.0  468.070467          10   0.028111   \n",
       "134197  1.0   1.0      1.5        1.0  487.784050          10   0.028111   \n",
       "134198  1.0   1.0      1.5        1.0  484.602347          10   0.028111   \n",
       "134199  1.0   1.0      1.5        1.0  469.010489          10   0.028111   \n",
       "134200  1.0   1.0      1.5        1.0  456.029825          10   0.028111   \n",
       "134201  1.0   1.0      1.5        1.0  458.619577          10   0.028111   \n",
       "134202  1.0   1.0      1.5        1.0  458.619577          10   0.028111   \n",
       "134203  1.0   1.0      1.5        1.0  466.062803          10   0.028111   \n",
       "134204  1.0   1.0      1.5        1.0  470.253615          10   0.028111   \n",
       "134205  1.0   1.0      1.5        1.0  472.473094          10   0.028111   \n",
       "134206  1.0   1.0      1.5        1.0  470.670514          10   0.028111   \n",
       "134207  1.0   1.0      1.5        1.0  472.732107          10   0.028111   \n",
       "134208  1.0   1.0      1.5        1.0  483.128702          10   0.028111   \n",
       "134209  1.0   1.0      1.5        1.0  486.910157          10   0.028111   \n",
       "134210  1.0   1.0      1.5        1.0  486.635517          10   0.028111   \n",
       "134211  1.0   1.0      1.5        1.0  482.025958          10   0.028111   \n",
       "134212  1.0   1.0      1.5        1.0  467.649789          10   0.028111   \n",
       "134213  1.0   1.0      1.5        1.0  451.954609          10   0.021742   \n",
       "134214  1.0   1.0      1.5        1.0  445.733410          10   0.021742   \n",
       "134215  1.0   1.0      1.5        1.0  448.195796          10   0.021742   \n",
       "134216  1.0   1.0      1.5        1.0  460.033925          10   0.021742   \n",
       "134217  1.0   1.0      1.5        1.0  468.688058          10   0.021742   \n",
       "134218  1.0   1.0      1.5        1.0  484.761745          10   0.021742   \n",
       "134219  1.0   1.0      1.5        1.0  487.682785          10   0.021742   \n",
       "...     ...   ...      ...        ...         ...         ...        ...   \n",
       "136579  2.0  11.0      1.5        3.0  494.300952          10   0.087985   \n",
       "136580  2.0  11.0      1.5        3.0  492.978259          10   0.087985   \n",
       "136581  2.0  11.0      1.5        3.0  490.039228          10   0.087985   \n",
       "136582  2.0  11.0      1.5        3.0  488.107269          10   0.087985   \n",
       "136583  2.0  11.0      1.5        3.0  488.555981          10   0.087985   \n",
       "136584  2.0  11.0      1.5        3.0  489.565607          10   0.087985   \n",
       "136585  2.0  11.0      1.5        3.0  495.003166          10   0.113235   \n",
       "136586  2.0  11.0      1.5        3.0  496.775397          10   0.113235   \n",
       "136587  2.0  11.0      1.5        3.0  498.911346          10   0.113235   \n",
       "136588  2.0  11.0      1.5        3.0  497.672877          10   0.113235   \n",
       "136589  2.0  11.0      1.5        3.0  493.958416          10   0.113235   \n",
       "136590  2.0  11.0      1.5        3.0  490.863485          10   0.113235   \n",
       "136591  2.0  11.0      1.5        3.0  489.854363          10   0.113235   \n",
       "136592  2.0  11.0      1.5        3.0  490.311155          10   0.113235   \n",
       "136593  2.0  11.0      1.5        3.0  495.713846          10   0.113235   \n",
       "136594  2.0  11.0      1.5        3.0  497.185039          10   0.113235   \n",
       "136595  2.0  11.0      1.5        3.0  497.055254          10   0.113235   \n",
       "136596  2.0  11.0      1.5        3.0  493.490445          10   0.113235   \n",
       "136597  2.0  11.0      1.5        3.0  487.068238          10   0.113235   \n",
       "136598  2.0  11.0      1.5        3.0  485.775739          10   0.113235   \n",
       "136599  2.0  11.0      1.5        3.0  489.893901          10   0.113235   \n",
       "136600  2.0  11.0      1.5        3.0  495.635404          10   0.113235   \n",
       "136601  2.0  11.0      1.5        3.0  497.869024          10   0.113235   \n",
       "136602  2.0  11.0      1.5        3.0  497.246110          10   0.113235   \n",
       "136603  2.0  11.0      1.5        3.0  494.258693          10   0.113235   \n",
       "136604  2.0  11.0      1.5        3.0  491.642466          10   0.113235   \n",
       "136605  2.0  11.0      1.5        3.0  491.212255          10   0.113235   \n",
       "136606  2.0  11.0      1.5        3.0  491.212255          10   0.113235   \n",
       "136607  2.0  11.0      1.5        3.0  492.598804          10   0.113235   \n",
       "136608  2.0  11.0      1.5        3.0  493.153565          10   0.113235   \n",
       "\n",
       "        velocity  \n",
       "134190  0.339451  \n",
       "134191  0.339451  \n",
       "134192  0.339451  \n",
       "134193  0.339451  \n",
       "134194  0.339451  \n",
       "134195  0.359820  \n",
       "134196  0.359820  \n",
       "134197  0.359820  \n",
       "134198  0.359820  \n",
       "134199  0.422794  \n",
       "134200  0.422794  \n",
       "134201  0.422794  \n",
       "134202  0.422794  \n",
       "134203  0.422794  \n",
       "134204  0.250337  \n",
       "134205  0.250337  \n",
       "134206  0.250337  \n",
       "134207  0.250337  \n",
       "134208  0.250337  \n",
       "134209  0.250337  \n",
       "134210  0.309806  \n",
       "134211  0.309806  \n",
       "134212  0.309806  \n",
       "134213  0.309806  \n",
       "134214  0.309806  \n",
       "134215  0.309806  \n",
       "134216  0.309806  \n",
       "134217  0.309806  \n",
       "134218  0.459700  \n",
       "134219  0.459700  \n",
       "...          ...  \n",
       "136579  0.350981  \n",
       "136580  0.298983  \n",
       "136581  0.298983  \n",
       "136582  0.298983  \n",
       "136583  0.304273  \n",
       "136584  0.304273  \n",
       "136585  0.304273  \n",
       "136586  0.304273  \n",
       "136587  0.304273  \n",
       "136588  0.304273  \n",
       "136589  0.304273  \n",
       "136590  0.304273  \n",
       "136591  0.297573  \n",
       "136592  0.297573  \n",
       "136593  0.297573  \n",
       "136594  0.297573  \n",
       "136595  0.297573  \n",
       "136596  0.306050  \n",
       "136597  0.306050  \n",
       "136598  0.306050  \n",
       "136599  0.341194  \n",
       "136600  0.341194  \n",
       "136601  0.341194  \n",
       "136602  0.341194  \n",
       "136603  0.327751  \n",
       "136604  0.327751  \n",
       "136605  0.327751  \n",
       "136606  0.345371  \n",
       "136607  0.345371  \n",
       "136608  0.345371  \n",
       "\n",
       "[2419 rows x 8 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[all_data['difficulty']==10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tmp_col = 'v_sd'\n",
    "# # tmp = all_data[(all_data['day']==1) & \n",
    "# #                (all_data['uid']==7) & \n",
    "# #                (all_data['exc_num']==1.1) & \n",
    "# #                (all_data['exc_times']==2)][tmp_col]\n",
    "# tmp = all_data.groupby(group_unit).head()\n",
    "# print(tmp)\n",
    "# del tmp\n",
    "# # print(tmp['Az'].rolling(100).mean().head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### delete outlier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(len(all_data))\n",
    "# all_data = all_data.dropna()\n",
    "# print(len(all_data))\n",
    "# # \n",
    "# l1 = len(all_data[['uid', 'day', 'exc_num', 'exc_times']].drop_duplicates())\n",
    "# print(l1)\n",
    "# upper = all_data[test_col].quantile(0.999)\n",
    "# print(upper)\n",
    "# outlier = all_data[all_data[test_col]>upper]\n",
    "# outlier = outlier[['uid', 'day', 'exc_num', 'exc_times']].drop_duplicates()\n",
    "# all_data = all_data[all_data[test_col]<=upper]\n",
    "# all_data,_ = ability_level_mapper(all_data, col=test_col, n_level=19, target_col_name='perf', invert=True)\n",
    "# print('outlier')\n",
    "# print(len(outlier))\n",
    "# print(len(outlier)/l1)\n",
    "# print(all_data[[test_col, 'perf']].describe())\n",
    "# # del all_data\n",
    "# del outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   day  uid  exc_num  exc_times   inv_force  difficulty  deviation  velocity\n",
      "0  1.0  1.0      1.1        1.0  484.201507           1   0.005096  0.113868\n",
      "1  1.0  1.0      1.1        1.0  486.018444           1   0.005096  0.113868\n",
      "2  1.0  1.0      1.1        1.0  488.478646           1   0.005096  0.113868\n",
      "3  1.0  1.0      1.1        1.0  489.455159           1   0.005096  0.113868\n",
      "4  1.0  1.0      1.1        1.0  494.271205           1   0.005096  0.113868\n"
     ]
    }
   ],
   "source": [
    "print(all_data.head())\n",
    "all_data.to_csv('../data/step1_clear_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'avg_av_xy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-683425ac07a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_av_xy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlist1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'exc_num'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'difficulty'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlist2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'exc_num'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'difficulty'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ml3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4374\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4375\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4376\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'avg_av_xy'"
     ]
    }
   ],
   "source": [
    "print(all_data.avg_av_xy.describe())\n",
    "list1 = all_data[all_data['exc_num']==1.1]['difficulty'].unique()\n",
    "list2 = all_data[all_data['exc_num']==2.1]['difficulty'].unique()\n",
    "l1 = len(list1)\n",
    "l3 = len(list2)\n",
    "print('diff in 1.1: '+str(l1))\n",
    "print('diff in 2.1: '+str(l3))\n",
    "common = list(set(list1).intersection(list2))\n",
    "print('the common diff: ' +str(len(common)))\n",
    "print('percentage: ' +str(len(common)/l1),str(len(common)/l3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tmp = all_data[all_data.v_angle>3.1]\n",
    "# col = 'avg_av_xy'\n",
    "# tmp = all_data.loc[#((all_data.uid==1) | (all_data.uid==3)) & \n",
    "#                    ((all_data.exc_num==1.3) | #(all_data.exc_num==2.3) | \n",
    "#                                                               (all_data.exc_num==1.1))]\n",
    "\n",
    "# # tmp = all_data.loc[((all_data.uid==1) | (all_data.uid==3))]\n",
    "# # tmp = tmp.loc[tmp.exc_num!=4.2]\n",
    "# sns.set_style('whitegrid')\n",
    "# f, ax= plt.subplots(figsize = (14, 10))\n",
    "# ax = sns.violinplot(x=\"exc_num\", y=col, hue='uid', data=tmp)\n",
    "\n",
    "# tmp = all_data.loc[((all_data.uid==1) | (all_data.uid==11)) & ((all_data.exc_num==1.3) | #(all_data.exc_num==2.3) | \n",
    "#                                                              (all_data.exc_num==1.1))]\n",
    "# sns.set_style('whitegrid')\n",
    "# f, ax= plt.subplots(figsize = (14, 10))\n",
    "# ax = sns.violinplot(x=\"exc_num\", y=col, hue='uid', data=tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # del tmp\n",
    "# # tmp = all_data.copy()\n",
    "# tmp = all_data.loc[#((all_data.uid==2) | (all_data.uid==6)) \n",
    "#                     (all_data.day==2) \n",
    "# #                    & (all_data.exc_num==1.3) \n",
    "# #                    & (all_data.exc_times==1)\n",
    "#                   ]\n",
    "# cnt = 0\n",
    "# by_cols = ['v_angle', 'v_sd',\n",
    "#            'torque_xy_avg', 'inverted', 'day', 'exc_num', 'exc_times'\n",
    "#           ]\n",
    "# for index,group in tmp.groupby(by_cols):\n",
    "    \n",
    "#     if len(group)<100 or len(group['uid'].unique())<8:\n",
    "#         continue\n",
    "#     print(len(group))\n",
    "#     cnt += 1\n",
    "#     if cnt>10: continue\n",
    "#     group = group.reset_index()\n",
    "#     sns.set_style('whitegrid')\n",
    "#     f, ax= plt.subplots(figsize = (14, 10))\n",
    "#     ax = sns.violinplot(x=\"v_angle\", y='velocity', hue='uid', data=group)\n",
    "#     ax.set_title(index)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col = 'sum_va'\n",
    "tmp = all_data.loc[(all_data.uid==3) & (all_data.day==1) & (all_data.exc_num==1.3) & (all_data.exc_times==1)]\n",
    "sns.set_style('whitegrid')\n",
    "f, ax= plt.subplots(figsize = (14, 10))\n",
    "ax = sns.lineplot(x=tmp.index, y=col, data=tmp)\n",
    "\n",
    "tmp = all_data.loc[((all_data.uid==4)) \n",
    "                   & (all_data.day==1) & (all_data.exc_num==1.1) & (all_data.exc_times==1)]\n",
    "sns.set_style('whitegrid')\n",
    "f, ax= plt.subplots(figsize = (14, 10))\n",
    "ax = sns.lineplot(x=tmp.index, y=col, data=tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # parameters\n",
    "# target_col = 'velocity'\n",
    "# target_exc = None\n",
    "# cnt = 0\n",
    "\n",
    "\n",
    "# if target_exc is None:\n",
    "#     groups = all_data[['day', 'uid', 'exc_num', 'exc_times', target_col]].groupby(['day', 'uid', 'exc_num', 'exc_times'])\n",
    "# else:\n",
    "#     tmp = all_data[all_data['exc_num']==target_exc]\n",
    "#     assert len(tmp)>10, 'empty data'\n",
    "#     groups = tmp[['day', 'uid', 'exc_num', 'exc_times', target_col]].groupby(['day', 'uid', 'exc_num', 'exc_times'])\n",
    "#     del tmp\n",
    "\n",
    "\n",
    "\n",
    "# for index, group in groups:\n",
    "#     if cnt>10:\n",
    "#         break\n",
    "#     cnt += 1\n",
    "    \n",
    "# #     print(index)\n",
    "#     tmp = group.reset_index(drop=True)\n",
    "#     tmp = tmp.reset_index()\n",
    "# #     print(group)\n",
    "#     sns.set_style('whitegrid')\n",
    "#     f, ax= plt.subplots(figsize = (14, 10))\n",
    "#     ax = sns.lineplot(x=\"index\", y=target_col, data=tmp)\n",
    "#     ax.set_title(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce initial deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_header(data, head=500, has_tail=False):\n",
    "    \n",
    "    groups = data.set_index(['day', 'exc_num', 'exc_times', 'uid'])\n",
    "    groups = groups.groupby(groups.index)\n",
    "    result = pd.DataFrame()\n",
    "    \n",
    "    for index, group in groups:\n",
    "        \n",
    "        if has_tail:\n",
    "            assert len(group)>(head*2), 'outlier'\n",
    "        else:\n",
    "            assert len(group)>head, 'outlier'\n",
    "            \n",
    "        header = [i+1 for i in xrange(head)]\n",
    "        if has_tail:\n",
    "            tailer = header[::-1]\n",
    "            middle = [head for i in xrange(len(group)-head*2)]\n",
    "            weights = header+middle+tailer\n",
    "\n",
    "        else:\n",
    "            middle = [head for i in xrange(len(group)-head)]\n",
    "\n",
    "            weights = header+middle\n",
    "            \n",
    "        weights = pd.Series(weights)    \n",
    "        weights = weights/head\n",
    "        assert len(group)==len(weights), 'different length between group and weights'\n",
    "        curr = group.reset_index()\n",
    "        curr['front'] =curr['front']*weights\n",
    "        result = pd.concat([result, curr])\n",
    "        del curr\n",
    "        \n",
    "    return result.reset_index(drop=True)\n",
    "                            \n",
    "\n",
    "# new_data = reduce_header(new_data,  head=300, has_tail=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(new_data))\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.to_csv('../data/step1_clear_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
