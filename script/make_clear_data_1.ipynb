{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from six.moves import xrange\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from minisom import MiniSom\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ability_level_mapper(data, groups=None, col='front', how='naive', n_level=19, invert=True,\n",
    "                         parameters=None,target_col_name='performance'):\n",
    "    # the raw data is divided into groups according to its exc_num, ability levels are calculated respectively\n",
    "    # how: 1 is mapping without any other processing, called 'naive'\n",
    "    origin = data.copy()\n",
    "    \n",
    "    if parameters is not None:\n",
    "        col, how, n_level, invert, v_max, v_min = parameters\n",
    "        interval = (v_max- v_min)/n_level\n",
    "        assert interval!=0, 'zero dividend'\n",
    "        origin[target_col_name] = (origin[col]-v_min)/interval\n",
    "        \n",
    "        origin[target_col_name] = origin[target_col_name].astype(int)\n",
    "        if invert:\n",
    "            origin[target_col_name] = n_level-origin[target_col_name]+1\n",
    "        else:\n",
    "            origin[target_col_name] = origin[target_col_name]+1\n",
    "        \n",
    "        return origin, parameters\n",
    "        \n",
    "    if groups == None:\n",
    "        v_max = origin[col].max()\n",
    "        v_min = origin[col].min()\n",
    "        \n",
    "        interval = (v_max- v_min)/n_level\n",
    "        assert interval!=0, 'zero dividend'\n",
    "        origin[target_col_name] = (origin[col]-v_min)/interval\n",
    "        \n",
    "        origin[target_col_name] = origin[target_col_name].astype(int)\n",
    "        if invert:\n",
    "            origin[target_col_name] = n_level-origin[target_col_name]+1\n",
    "        else:\n",
    "            origin[target_col_name] = origin[target_col_name]+1\n",
    "        \n",
    "        parameters = (col, how, n_level, invert, v_max, v_min )\n",
    "        return origin, parameters\n",
    "    \n",
    "    if how == 1 or how=='naive':\n",
    "        tmp = origin[col]\n",
    "        for index, group in groups:\n",
    "            \n",
    "            v_max = group[col].max()\n",
    "            v_min = group[col].min()\n",
    "            \n",
    "            interval = (v_max-v_min)/n_level\n",
    "            \n",
    "            assert interval!=0, 'zero dividend'\n",
    "            \n",
    "            origin.loc[index, col] = (origin.loc[index, col]-v_min)/interval\n",
    "        origin[col] = origin[col].astype(int)\n",
    "        if invert:\n",
    "            origin[target_col_name] = n_level-origin[col]+1\n",
    "            \n",
    "        else:\n",
    "            origin[target_col_name] = origin[target_col_name]+1\n",
    "        origin[col] = tmp\n",
    "        parameters = (col, how, n_level, invert, v_max, v_min )\n",
    "        del tmp\n",
    "        return origin, parameters\n",
    "\n",
    "def calc_force(df):\n",
    "    df['force'] = df['Fx']**2 + df['Fy']**2 + df['Fz']**2\n",
    "    df['force'] = df['force'].pow(1/2)\n",
    "    return df\n",
    "\n",
    "def calc_resultent_force(df):\n",
    "    # calc resultent force\n",
    "    df['resultent_force'] = (df['Fx']-df['Fx.1'])**2+(df['Fy']-df['Fy.1'])**2+(df['Fz']-df['Fz.1'])**2\n",
    "    df['resultent_force'] = df['resultent_force'].pow(1/2)\n",
    "    return df\n",
    "\n",
    "def calc_resultent_force_xy(df):\n",
    "    # calc resultent force on flat xoy\n",
    "    df['resultent_force_xy'] = (df['Fx']-df['Fx.1'])**2+(df['Fy']-df['Fy.1'])**2\n",
    "    df['resultent_force_xy'] = df['resultent_force_xy'].pow(1/2)\n",
    "    return df\n",
    "\n",
    "\n",
    "def calc_velocity(df, del_low=True):\n",
    "    df['velocity'] = df['Lx']**2 + df['Ly']**2 + df['Lz']**2\n",
    "    df['velocity'] = df['velocity'].pow(1/2)\n",
    "    if del_low:\n",
    "        df = del_low_velocity(df)\n",
    "    return df\n",
    "\n",
    "def calc_velocity_xy(df):\n",
    "    df['velocity_xy'] = df['Lx']**2 + df['Ly']**2\n",
    "    df['velocity_xy'] = df['velocity_xy'].pow(1/2)\n",
    "    return df\n",
    "\n",
    "def calc_force_velocity_angle_xy(df):\n",
    "    # calculate angle between walk direction and force direction\n",
    "    if 'resultent_force_xy' not in df.columns:\n",
    "        df = calc_resultent_force_xy(df)\n",
    "        \n",
    "    if 'velocity_xy' in df.columns: \n",
    "        df = calc_velocity_xy(df)\n",
    "    \n",
    "    df['angle_fv_xy'] = ((df['Fx']-df['Fx.1'])*df['Lx']+(df['Fy']-df['Fy.1'])*df['Ly'])/  \\\n",
    "        (df['velocity_xy']*df['resultent_force_xy'])\n",
    "    df['angle_fv_xy'] = df['angle_fv_xy'].apply(np.arccos)\n",
    "    tmp = df['angle_fv_xy'][df['angle_fv_xy']>np.pi/2]\n",
    "    tmp -= np.pi\n",
    "    \n",
    "    # here is maybe a bug\n",
    "    df.loc[df.angle_fv_xy>np.pi/2, 'angle_fv_xy'] = tmp\n",
    "    del tmp\n",
    "    return df\n",
    "\n",
    "def calc_torque_xy(df, threshold=0):\n",
    "    df['torque_xy'] = df['Mz']-df['DMz']\n",
    "    return df\n",
    "\n",
    "def calc_torque_xy_avg(df, threshold=0, n=100, dropna=True):\n",
    "    print('torque')\n",
    "    print(len(df))\n",
    "    if 'torque_xy' not in df.columns:\n",
    "        df = calc_torque_xy(df)\n",
    "        \n",
    "    df['torque_xy_avg'] = df.groupby(['uid','day','exc_num', 'exc_times'])['torque_xy'].\\\n",
    "        rolling(n).mean().reset_index()['torque_xy']\n",
    "        \n",
    "    if threshold!=0:\n",
    "        assert threshold>0, 'threshold must be positive'\n",
    "        df[(df.torque_xy_avg<threshold) & (df.torque_xy_avg>-threshold)] = 1\n",
    "        df[df.torque_xy_avg<=-threshold] = 0\n",
    "        df[df.torque_xy_avg>=threshold] = 2\n",
    "        \n",
    "    if dropna:\n",
    "        df = df.dropna()\n",
    "    print(len(df.dropna()))\n",
    "    return df\n",
    "\n",
    "def calc_abs_force_velocity_angle_xy(df):\n",
    "    # calculate absolute angle between walk direction and force direction\n",
    "    if 'resultent_force_xy' not in df.columns:\n",
    "        df = calc_resultent_force_xy(df)\n",
    "        \n",
    "    if 'velocity_xy' in df.columns: \n",
    "        df = calc_velocity_xy(df)\n",
    "    \n",
    "    df['abs_angle_fv_xy'] = ((df['Fx']-df['Fx.1'])*df['Lx']+(df['Fy']-df['Fy.1'])*df['Ly'])/  \\\n",
    "        (df['velocity_xy']*df['resultent_force_xy'])\n",
    "    df['abs_angle_fv_xy'] = abs(df['abs_angle_fv_xy'])\n",
    "    df['abs_angle_fv_xy'] = df['abs_angle_fv_xy'].apply(np.arccos)\n",
    "    return df\n",
    "\n",
    "def calc_torque_turning_freq(df,interval=100, groups=None, dropna=True):\n",
    "    # calculate the frequency of the changing of the direction of the torque on flat xoy\n",
    "    \n",
    "    print('torque turning freq:')\n",
    "    print(df.shape)\n",
    "    result = pd.Series()\n",
    "    if 'torque_xy' not in df.columns:\n",
    "        df = calc_torque_xy(df)\n",
    "    if groups == None:\n",
    "        groups = df[['uid', 'day', 'exc_num', 'exc_times', 'torque_xy']].   \\\n",
    "            groupby(['uid', 'day', 'exc_num', 'exc_times'])\n",
    "        \n",
    "    for index, group in groups:\n",
    "        # calculate the direction of the changing of the values\n",
    "        df_tmp = group.reset_index()\n",
    "        df_tmp['tt_freq'] = df_tmp['torque_xy']\n",
    "        df_tmp.loc[0, 'tt_freq'] = 0\n",
    "        tmp1 = df_tmp['torque_xy'][:-1].reset_index(drop=True)\n",
    "        tmp2 = df_tmp['torque_xy'][1:].reset_index(drop=True)\n",
    "\n",
    "        tmp3 = tmp2-tmp1\n",
    "        tmp3.index = tmp3.index+1\n",
    "        df_tmp.loc[1:, 'tt_freq'] = tmp3\n",
    "\n",
    "        # delete 0 value, and compare two adjecend value, if the direction changes, then the product is negative.\n",
    "        tmp3 = df_tmp[df_tmp['tt_freq']!=0][['tt_freq']]\n",
    "        tmp3 = tmp3.reset_index()\n",
    "        tmp1 = tmp3['tt_freq'][:-1].reset_index()\n",
    "        tmp2 = tmp3['tt_freq'][1:].reset_index()\n",
    "        tmp4 = tmp1*tmp2\n",
    "        tmp4.index += 1\n",
    "        tmp3[0, 'tt_freq'] = 0\n",
    "        tmp3.loc[1:, 'tt_freq'] = tmp4\n",
    "        tmp3.index = tmp3['index']\n",
    "        df_tmp.loc[df_tmp.tt_freq!=0, 'tt_freq'] = tmp3['tt_freq']\n",
    "\n",
    "        # count in interval\n",
    "        df_tmp.loc[df_tmp.tt_freq>=0, 'tt_freq'] = np.nan\n",
    "#         df_tmp['tt_freq'] = df_tmp['tt_freq'].rolling(interval).count()\n",
    "        df_tmp = df_tmp.set_index(['index'])\n",
    "        df_tmp = group_rolling_count(df_tmp, 'tt_freq', 'tt_freq', by=['uid', 'day', 'exc_num', 'exc_times'], n=interval)\n",
    "#         df_tmp.index = df_tmp['index']\n",
    "        \n",
    "        result = pd.concat([result, df_tmp['tt_freq']])\n",
    "        del tmp1\n",
    "        del tmp2\n",
    "        del tmp3\n",
    "        del tmp4\n",
    "#     print(result)\n",
    "    # index aligning\n",
    "    df['tt_freq'] = result\n",
    "#     print(all_data[['torque_xy','tt_freq']])\n",
    "\n",
    "    if dropna:\n",
    "        df = df.dropna()\n",
    "    print(df.shape)        \n",
    "    del result\n",
    "    return df   \n",
    "\n",
    "def calc_abs_angle_rotation_velocity_xy(df, groups=None):\n",
    "    # calculate relative absolute angle rotation of velocity on flat xoy\n",
    "    \n",
    "    if 'velocity_xy' not in df.columns:\n",
    "        df = calc_velocity_xy(df)\n",
    "        \n",
    "    result = pd.Series()\n",
    "    if groups == None:\n",
    "        groups = df[['uid', 'day', 'exc_num', 'exc_times', 'Lx', 'Ly', 'velocity_xy']].   \\\n",
    "            groupby(['uid', 'day', 'exc_num', 'exc_times'])\n",
    "        \n",
    "    for index, group in groups:\n",
    "        # calculate angle rotation from two adjecend time point\n",
    "        df_tmp = group.reset_index()\n",
    "        df_tmp['v_rotation'] = 0\n",
    "        tmp1 = df_tmp.loc[0:len(df_tmp)-2, ['Lx', 'Ly', 'velocity_xy']].reset_index(drop=True)\n",
    "        tmp2 = df_tmp.loc[1:, ['Lx', 'Ly', 'velocity_xy']].reset_index(drop=True)\n",
    "    \n",
    "        tmp3 = (tmp1['Lx']*tmp2['Lx']+tmp1['Ly']*tmp2['Ly'])/  \\\n",
    "        (tmp1['velocity_xy']*tmp2['velocity_xy'])\n",
    "#         tmp3 = tmp3.fillna(0)\n",
    "#         print(tmp3)\n",
    "        tmp3 = tmp3.apply(np.arccos)\n",
    "#         print(tmp3)\n",
    "        tmp3.index += 1\n",
    "        df_tmp.loc[1:, 'v_rotation'] = tmp3\n",
    "        df_tmp.loc[df_tmp.v_rotation>np.pi/2, 'v_rotation'] -= np.pi\n",
    "        df_tmp.loc[df_tmp.v_rotation<0.0001, 'v_rotation'] = 0\n",
    "        df_tmp['v_rotation'] = df_tmp['v_rotation'].fillna(0)\n",
    "#         print(df_tmp.loc[:, ['v_rotation', 'Lx', 'Ly']])\n",
    "        \n",
    "        df_tmp.index = df_tmp['index']\n",
    "        result = pd.concat([result, df_tmp['v_rotation']])\n",
    "        del tmp1\n",
    "        del tmp2\n",
    "        del tmp3\n",
    "        del df_tmp\n",
    "    df['v_abs_rotation'] = result\n",
    "    del result\n",
    "    return df\n",
    "\n",
    "def calc_angle_rotation_velocity_xy(df, groups=None):\n",
    "    # calculate relative angle rotation of velocity on flat xoy\n",
    "    \n",
    "    if 'velocity_xy' not in df.columns:\n",
    "        df = calc_velocity_xy(df)\n",
    "        \n",
    "    result = pd.Series()\n",
    "    if groups == None:\n",
    "        groups = df[['uid', 'day', 'exc_num', 'exc_times', 'Lx', 'Ly', 'velocity_xy']].   \\\n",
    "            groupby(['uid', 'day', 'exc_num', 'exc_times'])\n",
    "        \n",
    "    for index, group in groups:\n",
    "        # calculate angle rotation from two adjecend time point\n",
    "        df_tmp = group.reset_index()\n",
    "        df_tmp['v_rotation'] = 0\n",
    "#         tmp1 = df_tmp['Ly']/df_tmp['Lx']\n",
    "        tmp1 = np.arctan2(df_tmp['Ly'], df_tmp['Lx'])\n",
    "        tmp2 = tmp1[1:].reset_index(drop=True) - tmp1[:-1].reset_index(drop=True)\n",
    "\n",
    "        tmp2.index += 1\n",
    "        df_tmp.loc[1:, 'v_rotation'] = tmp2\n",
    "        df_tmp['v_rotation'] = df_tmp['v_rotation'].fillna(0)\n",
    "#         print(df_tmp.loc[:, ['v_rotation', 'Lx', 'Ly']])\n",
    "        \n",
    "        df_tmp.index = df_tmp['index']\n",
    "        result = pd.concat([result, df_tmp['v_rotation']])\n",
    "        del tmp1\n",
    "        del tmp2\n",
    "        del df_tmp\n",
    "    df['v_rotation'] = result\n",
    "    df.loc[df.v_rotation<-np.pi, 'v_rotation'] += 2*np.pi\n",
    "    df.loc[df.v_rotation>np.pi, 'v_rotation'] -= 2*np.pi\n",
    "    del result\n",
    "    return df\n",
    "\n",
    "def calc_velocity_angle(df):\n",
    "#     tan = df['Ly']/df['Lx']\n",
    "    df['v_angle'] = np.arctan2(df['Ly'], df['Lx'])\n",
    "#     del tan\n",
    "    return df\n",
    "\n",
    "def calc_velocity_angle_without_backward(df):\n",
    "    # here we calculate velocity angle regardless of move direction\n",
    "    if 'v_angle' not in df.columns:\n",
    "        df = calc_velocity_angle(df)\n",
    "        \n",
    "    df['va_wo'] = df['v_angle']\n",
    "    df.loc[(df.v_angle>np.pi/2) , 'va_wo'] = np.pi - df.loc[(df.v_angle>np.pi/2) , 'va_wo']\n",
    "    df.loc[(df.v_angle<-np.pi/2), 'va_wo'] += np.pi\n",
    "    return df\n",
    "\n",
    "def calc_sd_velocity(df, by=['day', 'uid', 'exc_num', 'exc_times'], n=100, dropna=True):\n",
    "    print('sd')\n",
    "    print(len(df))\n",
    "    if 'velocity' not in df.columns:\n",
    "        df = calc_velocity(df)\n",
    "#     df['v_sd'] = df.groupby(['uid','day','exc_num', 'exc_times'])['velocity'].\\\n",
    "#         rolling(n).std().reset_index()['velocity']\n",
    "    df = group_rolling_std(df, 'velocity', 'v_sd', by, n)\n",
    "    if dropna:\n",
    "        df = df.dropna()\n",
    "    print(len(df.dropna()))\n",
    "    return df\n",
    "\n",
    "def calc_velocity_skew(df, by=['day', 'uid', 'exc_num', 'exc_times'], n=100, dropna=True):\n",
    "    print('skew')\n",
    "    print(len(df))\n",
    "    if 'velocity' not in df.columns:\n",
    "        df = calc_velocity(df)\n",
    "#     df['v_skew'] = df.groupby(['uid','day','exc_num', 'exc_times'])['velocity'].\\\n",
    "#         rolling(n).skew().reset_index()['velocity']\n",
    "    df = group_rolling_skew(df, 'velocity', 'v_skew', by, n)\n",
    "    if dropna:\n",
    "        df = df.dropna()\n",
    "    print(len(df.dropna()))\n",
    "    return df\n",
    "\n",
    "def calc_velocity_kurtosis(df, by=['day', 'uid', 'exc_num', 'exc_times'], n=100, dropna=True):\n",
    "    print('kurtosis')\n",
    "    print(len(df))\n",
    "    if 'velocity' not in df.columns:\n",
    "        df = calc_velocity(df)\n",
    "#     df['v_kurt'] = df.groupby(['uid','day','exc_num', 'exc_times'])['velocity'].\\\n",
    "#         rolling(n).skew().reset_index()['velocity']\n",
    "\n",
    "    df = group_rolling_kurt(df, 'velocity', 'v_kurt', by, n)\n",
    "    if dropna:\n",
    "        df = df.dropna()\n",
    "    print(len(df.dropna()))\n",
    "    return df\n",
    "\n",
    "def calc_avg_velocity(df, by=['day', 'uid', 'exc_num', 'exc_times'], n=100, dropna=True):\n",
    "    if 'velocity' not in df.columns:\n",
    "        df = calc_velocity(df)\n",
    "        \n",
    "#     df['avg_velocity'] = df.groupby(['uid','day','exc_num', 'exc_times']).\\\n",
    "#         rolling(n).mean().reset_index(drop=True)['velocity']\n",
    "    \n",
    "    df = group_rolling_mean(df, 'velocity', 'avg_velocity', by, n)\n",
    "    \n",
    "    if dropna:\n",
    "        df = df.dropna()\n",
    "    print(len(df.dropna()))\n",
    "\n",
    "    return df\n",
    "\n",
    "def calc_velocity_deviation_score(df, a=10**-8, avg=False):\n",
    "    if avg:\n",
    "        if 'avg_velocity' not in df.columns:\n",
    "            df = calc_avg_velocity(df)\n",
    "        \n",
    "        df['vd_score'] = df['avg_velocity']/(a+df['deviation'])\n",
    "    else:\n",
    "        df = stardard_scaler(df, target_col='velocity')\n",
    "        df = stardard_scaler(df, target_col='deviation')\n",
    "        df['vd_score'] = df['velocity']/(a+df['deviation'])\n",
    "    return df\n",
    "\n",
    "def calc_reciprocal_deviation(df, bias=0.1):\n",
    "    if 'deviation' not in df.columns:\n",
    "        df = get_small_deviation(df)\n",
    "    \n",
    "    df['re_dev'] = 1/(df['deviation']+bias)\n",
    "    return df\n",
    "\n",
    "def calc_inverted_tt_freq(df, bias=0):\n",
    "    if 'tt_freq' not in df.columns:\n",
    "        df = calc_torque_turning_freq(df)\n",
    "    df['inv_tt_freq'] = df['tt_freq'].max()-df['tt_freq']+bias\n",
    "    return df\n",
    "\n",
    "def calc_score(df, avg=False):\n",
    "    if avg:\n",
    "        if 'avg_velocity' not in df.columns:\n",
    "            df = calc_avg_velocity(df)\n",
    "\n",
    "        df['score'] = df['avg_velocity']*df['deviation']\n",
    "    else:\n",
    "        df['score'] = df['velocity']*df['deviation']\n",
    "    return df\n",
    "\n",
    "def calc_angle_velocity_xy(df):\n",
    "    df['angle_velocity_xy'] = df['Az']\n",
    "    return df\n",
    "\n",
    "def calc_avg_angle_velocity_xy(df, by=['day', 'uid', 'exc_num', 'exc_times'], n=100, dropna=True):\n",
    "    print('angle_velocity')\n",
    "    print(len(df))\n",
    "    if 'angle_velocity_xy' not in df.columns:\n",
    "        df = calc_angle_velocity_xy(df)\n",
    "        \n",
    "    df = group_rolling_mean(df, 'Az', 'avg_av_xy', by, n)\n",
    "    if dropna:\n",
    "        df = df.dropna()\n",
    "    print(len(df.dropna()))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calc_va_sum(df, by=['day', 'uid', 'exc_num', 'exc_times'], n=100, dropna=True):\n",
    "    print('velocity angle cumsum')\n",
    "    print(len(df))\n",
    "    if 'va_wo' not in df.columns:\n",
    "        df = calc_velocity_angle_without_backward(df)\n",
    "        \n",
    "    df = group_rolling_sum(df, 'va_wo', 'sum_va', by, n)\n",
    "    if dropna:\n",
    "        df = df.dropna()\n",
    "    print(len(df.dropna()))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calc_confidence(df):\n",
    "    mx = df['front'].max()\n",
    "    mi = df['front'].min()\n",
    "    df['conf'] = 1-(df['front']-mi)/(mx-mi)\n",
    "    return df\n",
    "\n",
    "def get_small_deviation(df):\n",
    "    assert 'left' in df.columns, 'lack necessary columns'\n",
    "    assert 'right' in df.columns, 'lack necessary columns'\n",
    "    assert 'front' in df.columns, 'lack necessary columns'\n",
    "    print('get small deviation')\n",
    "    print(df.shape)\n",
    "    df['deviation'] = df[['front', 'left', 'right']].min(axis=1)\n",
    "    df = df.dropna(subset=['deviation'])\n",
    "    print(df.shape)\n",
    "    return df\n",
    "\n",
    "def del_outlier(df, by, test_col, threshold=10):\n",
    "    \n",
    "    # assume that the distribution of test column in each group are normal, \n",
    "    # the data outside 2 standard deviation are considered as outlier.\n",
    "    # the data with too few samples will be deleted\n",
    "    print('delete outlier')\n",
    "    print(len(df))\n",
    "    \n",
    "    if by is None:\n",
    "        mean = df[test_col].mean()\n",
    "        std = df[test_col].std()\n",
    "        \n",
    "        left = mean-2*std\n",
    "        right = mean+2*std\n",
    "        \n",
    "        df = df[(df[test_col]>left) & (df[test_col]<right)]\n",
    "        df = df.reset_index(drop=True)\n",
    "        print(len(df))\n",
    "        return df\n",
    "    \n",
    "    cols = df.columns\n",
    "    # delete data with a few samples\n",
    "    count = df[by+[test_col]].groupby(by).count()\n",
    "    count = count[count[test_col]>threshold].reset_index()\n",
    "    count = count[by]\n",
    "    df = df.merge(count, on=by)\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    \n",
    "    # delete outlier\n",
    "    mean = df[by+[test_col]].groupby(by).mean()\n",
    "    std = df[by+[test_col]].groupby(by).std()\n",
    "    left = mean-2*std\n",
    "    right = mean+2*std\n",
    "    left = left.reset_index()\n",
    "    right = right.reset_index()\n",
    "    left.columns = by+['left']\n",
    "    right.columns = by+['right']\n",
    "    \n",
    "    df = df.merge(left, on=by)\n",
    "    df = df.merge(right, on=by)\n",
    "    df['left'] = df[test_col]-df['left']\n",
    "    df['right'] = df[test_col] - df['right']\n",
    "    df = df[(df['left'])>0 & (df['right']<0)]\n",
    "    df = df[cols]\n",
    "    df = df.reset_index(drop=True)\n",
    "    print(len(df))\n",
    "    return df\n",
    "    \n",
    "    \n",
    "def set_inverted_task(df, tasks=[2.4, 3.4, 4.1, 4.2, 4.3]):\n",
    "    df['inverted'] = 0\n",
    "    for task in tasks:\n",
    "        df.loc[df.exc_num==task, 'inverted'] = 1\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def set_sidewards_task(df, tasks=[4.1, 4.2]):\n",
    "    df['sidewards'] = 0\n",
    "    for task in tasks:\n",
    "        df.loc[df.exc_num==task, 'sidewards'] = 1\n",
    "        \n",
    "    return df\n",
    "\n",
    "def is_disturbed(df):\n",
    "    df['disturbed'] = 0\n",
    "    df.loc[(df.DFx>0) | (df.DFy>0) | (df.DFz>0), 'disturbed'] = 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "def is_backward(df):\n",
    "    if 'v_angle' not in df.columns:\n",
    "        df = calc_velocity_angle(df)\n",
    "    df['backward'] = 0\n",
    "    df.loc[(df.v_angle>np.pi/2) | (df.v_angle<-np.pi/2), 'backward'] = 1\n",
    "    return df\n",
    "    \n",
    "\n",
    "\n",
    "def set_difficulty(df, def_cols=['v_angle', 'v_sd', 'torque_xy_avg'], levels=[20, 20, 20]):\n",
    "    print('set difficulty')\n",
    "    print(len(df))\n",
    "    df['diff_description'] = ''\n",
    "    for col in def_cols:\n",
    "        df['diff_description'] = df['diff_description'] + '_' +col+df[col].astype(str)\n",
    "        \n",
    "    if levels is not None:\n",
    "        for col, level in zip(def_cols, levels):\n",
    "            df,_ = ability_level_mapper(df, col=col, n_level=level, target_col_name=col)\n",
    "    df['difficulty'] = 1\n",
    "    cnt = 1\n",
    "    tmp = df.groupby(def_cols)[['difficulty']].mean().cumsum().reset_index()\n",
    "    df = df.drop(['difficulty'], axis=1)\n",
    "    df = df.merge(tmp, on=def_cols)\n",
    "#     print(df.head())\n",
    "#     print(len(df.groupby(def_cols).mean()))\n",
    "#     print(len(df.groupby(def_cols+['difficulty']).mean()))\n",
    "\n",
    "    tmp = df[['difficulty', 'diff_description']].drop_duplicates()\n",
    "    print(tmp)\n",
    "    tmp.to_csv('../data/diff_def.csv', index=False)\n",
    "    del tmp\n",
    "    print('difficulty description:')\n",
    "    print(df[['difficulty', 'diff_description']].drop_duplicates())\n",
    "    print('# of diff: ' +str(len(df['difficulty'].unique())))\n",
    "    print(len(df))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def difficulty_mapper(df, cols, levels, invert=True):\n",
    "    print('difficulty mapper')\n",
    "    for col, level in zip(cols, levels):\n",
    "        df,_ = ability_level_mapper(df, col= col, n_level=level, target_col_name=col, invert=invert)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def del_low_velocity(df, thres=0.05):\n",
    "    df = df[df['velocity']>thres]\n",
    "    return df\n",
    "\n",
    "def stardard_scaler(df, target_col):\n",
    "    mx = df[target_col].max()\n",
    "    mi = df[target_col].min()\n",
    "    df[target_col] = (df[target_col]-mi)/(mx-mi)\n",
    "    return df\n",
    "\n",
    "def group_rolling_mean(df, target_col, new_col, by, n):\n",
    "    assert 'index' not in df.columns, 'check you data columns, and ensure no index column in it'\n",
    "    df = df.reset_index()\n",
    "    tmp = df.groupby(by).rolling(n).mean()[target_col]\n",
    "    tmp = tmp.reset_index().sort_values(by).reset_index(drop=True)\n",
    "    df = df.sort_values(by).reset_index(drop=True)\n",
    "    df[new_col] = tmp[target_col]\n",
    "    del tmp\n",
    "    df = df.set_index('index')\n",
    "    return df\n",
    "\n",
    "def group_rolling_kurt(df, target_col, new_col, by, n):\n",
    "    assert 'index' not in df.columns, 'check you data columns, and ensure no index column in it'\n",
    "    df = df.reset_index()\n",
    "    tmp = df.groupby(by).rolling(n).kurt()[target_col]\n",
    "    tmp = tmp.reset_index().sort_values(by).reset_index(drop=True)\n",
    "    df = df.sort_values(by).reset_index(drop=True)\n",
    "    df[new_col] = tmp[target_col]\n",
    "    del tmp\n",
    "    df = df.set_index('index')\n",
    "    return df\n",
    "\n",
    "def group_rolling_skew(df, target_col, new_col, by, n):\n",
    "    assert 'index' not in df.columns, 'check you data columns, and ensure no index column in it'\n",
    "    df = df.reset_index()\n",
    "    tmp = df.groupby(by).rolling(n).skew()[target_col]\n",
    "    tmp = tmp.reset_index().sort_values(by).reset_index(drop=True)\n",
    "    df = df.sort_values(by).reset_index(drop=True)\n",
    "    df[new_col] = tmp[target_col]\n",
    "    del tmp\n",
    "    df = df.set_index('index')\n",
    "    return df\n",
    "\n",
    "def group_rolling_std(df, target_col, new_col, by, n):\n",
    "    assert 'index' not in df.columns, 'check you data columns, and ensure no index column in it'\n",
    "    df = df.reset_index()\n",
    "    tmp = df.groupby(by).rolling(n).std()[target_col]\n",
    "    tmp = tmp.reset_index().sort_values(by).reset_index(drop=True)\n",
    "    df = df.sort_values(by).reset_index(drop=True)\n",
    "    df[new_col] = tmp[target_col]\n",
    "    del tmp\n",
    "    df = df.set_index('index')\n",
    "    return df\n",
    "\n",
    "def group_rolling_count(df, target_col, new_col, by, n):\n",
    "    assert 'index' not in df.columns, 'check you data columns, and ensure no index column in it'\n",
    "    df = df.reset_index()\n",
    "    tmp = df.groupby(by).rolling(n).count()[target_col]\n",
    "    tmp = tmp.reset_index().sort_values(by).reset_index(drop=True)\n",
    "    df = df.sort_values(by).reset_index(drop=True)\n",
    "    df[new_col] = tmp[target_col]\n",
    "    del tmp\n",
    "    df = df.set_index('index')\n",
    "    return df\n",
    "\n",
    "def group_rolling_sum(df, target_col, new_col, by, n):\n",
    "    assert 'index' not in df.columns, 'check you data columns, and ensure no index column in it'\n",
    "    df = df.reset_index()\n",
    "    tmp = df.groupby(by).rolling(n).sum()[target_col]\n",
    "    tmp = tmp.reset_index().sort_values(by).reset_index(drop=True)\n",
    "    df = df.sort_values(by).reset_index(drop=True)\n",
    "    df[new_col] = tmp[target_col]\n",
    "    del tmp\n",
    "    df = df.set_index('index')\n",
    "    return df\n",
    "\n",
    "def def_env(test_col, diff_def, n_class, group, if_del_outlier):\n",
    "    \n",
    "    tmp = [test_col, diff_def, n_class, group, if_del_outlier]\n",
    "    with open('../data/parameter/def_env.p', 'wb') as f:\n",
    "        pickle.dump(tmp, f)\n",
    "        \n",
    "    del tmp\n",
    "    return\n",
    "    \n",
    "def add_necessary_cols(all_cols, cols):\n",
    "    for col in cols:\n",
    "        if col not in all_cols:\n",
    "            all_cols.append(col)\n",
    "            \n",
    "    return all_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the initial data is already cleared according to their length and correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clear outlier that recorded in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = pd.read_csv('../data/new_all_data_original.csv')\n",
    "\n",
    "# print(len(all_data))\n",
    "# outliers = []\n",
    "\n",
    "# with open('../data/deviation_curves_outlier.csv') as f:\n",
    "#     for line in f.readlines():\n",
    "#         tmp = line.split('\\n')[0].split(' ')\n",
    "       \n",
    "#         outliers.append([float(x) for x in tmp])\n",
    "        \n",
    "# groups = all_data.set_index(['day', 'exc_num', 'exc_times', 'uid'])\n",
    "# groups = groups.groupby(by=groups.index)\n",
    "\n",
    "# new_data = pd.DataFrame()\n",
    "\n",
    "# for index, group in groups:\n",
    "#     if len(group)<10:\n",
    "#         print('short length')\n",
    "#         print(index)\n",
    "#         continue\n",
    "#     day, exc_num, exc_times, uid = index\n",
    "#     exc_num = round(exc_num, 1)\n",
    "#     if [day, exc_num, exc_times, uid] in outliers:\n",
    "#         print([day, exc_num, exc_times, uid])\n",
    "#         print('in the outlier list')\n",
    "#         pass\n",
    "# #         tmp = group.reset_index()\n",
    "# #         sns.set_style('whitegrid')\n",
    "# #         f, ax= plt.subplots(figsize = (14, 10))\n",
    "# #         ax = sns.lineplot(x=tmp.index, y=\"front\", data=tmp)\n",
    "# #         ax.set_title(index) \n",
    "#     else:\n",
    "#         curr = group.reset_index()\n",
    "#         new_data = pd.concat([new_data, curr], axis=0)\n",
    "# all_data = new_data\n",
    "# del new_data\n",
    "# len(all_data)\n",
    "# all_data.to_csv('../data/new_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../data/step1_clear_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_avg_velocity(df, interval=20):\n",
    "#     if 'velocity' not in df.columns:\n",
    "#         df = calc_velocity(df)\n",
    "        \n",
    "#     df['avg_velocity'] = df['velocity'].rolling(interval, min_periods=1).mean()\n",
    "#     return df\n",
    "\n",
    "# def find_forwards_segment(df, exc=1.1, rotation_threshold=1, new_num=0.1):\n",
    "#     assert exc is not None, 'rewrite function find_forwards_segment first'\n",
    "#     tmp = df[df['exc_num']==exc]\n",
    "#     tmp = tmp.loc[(tmp.Lx>0) & ((tmp.Az<rotation_threshold) | (tmp.Az>-rotation_threshold))]\n",
    "#     tmp['exc_num'] = new_num\n",
    "#     df = pd.concat([df, tmp])\n",
    "#     df = df.reset_index(drop=True)\n",
    "#     del tmp\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = calc_avg_velocity(df)\n",
    "# # df = find_forwards_segment(df)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('../data/step1_clear_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load precleared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Ax', 'Ay', 'Az', 'DFx', 'DFy', 'DFz', 'DMx', 'DMy', 'DMz', 'Fx', 'Fy',\n",
      "       'Fz', 'Lx', 'Ly', 'Lz', 'Mx', 'My', 'Mz', 'day', 'exc_num', 'exc_times',\n",
      "       'front', 'left', 'path.x', 'path.x1', 'path.x2', 'path.y', 'path.y1',\n",
      "       'path.y2', 'pose.theta', 'pose.x', 'pose.y', 'record.t', 'right', 't',\n",
      "       'uid', 'datetime', 'pre_dist', 'path_diff', 'pre_x', 'pre_y', 'curr_x',\n",
      "       'curr_y', 'curr_dist'],\n",
      "      dtype='object')\n",
      "[ 1.1  1.5  2.1  2.2  3.1  3.2  4.1  4.2]\n"
     ]
    }
   ],
   "source": [
    "all_data = pd.read_csv('../data/data2_path.csv')\n",
    "print(all_data.columns)\n",
    "# print(all_data[(all_data['day']==1) & \n",
    "#                (all_data['uid']==7) & \n",
    "#                (all_data['exc_num']==1.1) & \n",
    "#                (all_data['exc_times']==1)][['Az']])\n",
    "print(all_data['exc_num'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(all_data[(all_data['day']==1) & \n",
    "#                (all_data['uid']==7) & \n",
    "#                (all_data['exc_num']==1.1) & \n",
    "#                (all_data['exc_times']==2)][['Az', 'angle_velocity_xy', 'avg_av_xy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = all_data[(all_data['day']==1) & \n",
    "#                ((all_data['uid']==7) | (all_data['uid']==4)) & \n",
    "#                (all_data['exc_num']==1.1) & \n",
    "#                (all_data['exc_times']==1)]\n",
    "# test = test.groupby(['day', 'uid', 'exc_num', 'exc_times']).head().reset_index()\n",
    "# print(test.head())\n",
    "# test['test'] = test.groupby(['day', 'uid', 'exc_num', 'exc_times']).rolling(100).mean().reset_index(drop=True)['Az']\n",
    "# # test['test'] = test['Az'].rolling(2).mean()\n",
    "# test.groupby(['day', 'uid', 'exc_num', 'exc_times']).head()\\\n",
    "#     [['day', 'uid', 'exc_num', 'exc_times','Az', 'test']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "test_col = 'inv_tt_freq'\n",
    "diff_def = [#'v_angle', #'v_skew',  \n",
    "            #'v_kurt',\n",
    "            #'sum_va',\n",
    "            #'avg_av_xy', \n",
    "            'inverted', \n",
    "            'sidewards',\n",
    "            'disturbed', 'backward', 'path_diff']\n",
    "mapper_cols = [#'v_angle', #'v_skew', #'v_kurt', \n",
    "               #'sum_va',\n",
    "               #'avg_av_xy'\n",
    "              ]\n",
    "\n",
    "mapper_levels = [#5, #10, \n",
    "                 #5,\n",
    "                 #5\n",
    "                ]\n",
    "n_class = mapper_levels+[2, 2, 2, 2, 3]\n",
    "group_unit = ['day', 'uid', 'exc_num', 'exc_times']\n",
    "group_cols = group_unit + [test_col]\n",
    "diff_cols = group_cols+diff_def\n",
    "\n",
    "selected_cols = group_cols+['difficulty']\n",
    "necessary_cols = ['deviation', 'velocity' #, 'conf'\n",
    "                 ]\n",
    "selected_cols1 = add_necessary_cols(diff_cols, necessary_cols)\n",
    "selected_cols = add_necessary_cols(selected_cols, necessary_cols)\n",
    "b_del_outlier = True\n",
    "def_env(test_col, diff_def, n_class, group_cols, b_del_outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get small deviation\n",
      "(347972, 44)\n",
      "(317966, 45)\n",
      "(317966, 46)\n",
      "main\n",
      "317966\n",
      "308324\n",
      "torque turning freq:\n",
      "(306505, 46)\n",
      "(297707, 48)\n",
      "main\n",
      "297707\n",
      "set difficulty\n",
      "297707\n",
      "        difficulty                                   diff_description\n",
      "0                1  _inverted0_sidewards0_disturbed0_backward0_pat...\n",
      "94295            4  _inverted0_sidewards0_disturbed0_backward1_pat...\n",
      "128049           7  _inverted0_sidewards0_disturbed1_backward0_pat...\n",
      "134190          10  _inverted0_sidewards0_disturbed1_backward1_pat...\n",
      "136609           2  _inverted0_sidewards0_disturbed0_backward0_pat...\n",
      "194712           5  _inverted0_sidewards0_disturbed0_backward1_pat...\n",
      "200172           6  _inverted0_sidewards0_disturbed0_backward1_pat...\n",
      "209537           3  _inverted0_sidewards0_disturbed0_backward0_pat...\n",
      "220988           8  _inverted0_sidewards0_disturbed1_backward0_pat...\n",
      "222624          13  _inverted1_sidewards1_disturbed0_backward0_pat...\n",
      "236517          15  _inverted1_sidewards1_disturbed0_backward1_pat...\n",
      "248391          16  _inverted1_sidewards1_disturbed0_backward1_pat...\n",
      "267326          14  _inverted1_sidewards1_disturbed0_backward0_pat...\n",
      "297514          11  _inverted0_sidewards0_disturbed1_backward1_pat...\n",
      "297528           9  _inverted0_sidewards0_disturbed1_backward0_pat...\n",
      "297540          12  _inverted0_sidewards0_disturbed1_backward1_pat...\n",
      "297551          17  _inverted1_sidewards1_disturbed1_backward0_pat...\n",
      "297629          18  _inverted1_sidewards1_disturbed1_backward1_pat...\n",
      "difficulty description:\n",
      "        difficulty                                   diff_description\n",
      "0                1  _inverted0_sidewards0_disturbed0_backward0_pat...\n",
      "94295            4  _inverted0_sidewards0_disturbed0_backward1_pat...\n",
      "128049           7  _inverted0_sidewards0_disturbed1_backward0_pat...\n",
      "134190          10  _inverted0_sidewards0_disturbed1_backward1_pat...\n",
      "136609           2  _inverted0_sidewards0_disturbed0_backward0_pat...\n",
      "194712           5  _inverted0_sidewards0_disturbed0_backward1_pat...\n",
      "200172           6  _inverted0_sidewards0_disturbed0_backward1_pat...\n",
      "209537           3  _inverted0_sidewards0_disturbed0_backward0_pat...\n",
      "220988           8  _inverted0_sidewards0_disturbed1_backward0_pat...\n",
      "222624          13  _inverted1_sidewards1_disturbed0_backward0_pat...\n",
      "236517          15  _inverted1_sidewards1_disturbed0_backward1_pat...\n",
      "248391          16  _inverted1_sidewards1_disturbed0_backward1_pat...\n",
      "267326          14  _inverted1_sidewards1_disturbed0_backward0_pat...\n",
      "297514          11  _inverted0_sidewards0_disturbed1_backward1_pat...\n",
      "297528           9  _inverted0_sidewards0_disturbed1_backward0_pat...\n",
      "297540          12  _inverted0_sidewards0_disturbed1_backward1_pat...\n",
      "297551          17  _inverted1_sidewards1_disturbed1_backward0_pat...\n",
      "297629          18  _inverted1_sidewards1_disturbed1_backward1_pat...\n",
      "# of diff: 18\n",
      "297707\n",
      "delete outlier\n",
      "297707\n",
      "293446\n",
      "   day  uid  exc_num  exc_times  inv_tt_freq  difficulty  deviation  velocity\n",
      "0  1.0  1.0      1.1        1.0         63.0           1   0.005096  0.113868\n",
      "1  1.0  1.0      1.1        1.0         63.0           1   0.005096  0.113868\n",
      "2  1.0  1.0      1.1        1.0         63.0           1   0.005096  0.113868\n",
      "3  1.0  1.0      1.1        1.0         63.0           1   0.005096  0.113868\n",
      "4  1.0  1.0      1.1        1.0         63.0           1   0.005096  0.113868\n",
      "(293446, 8)\n"
     ]
    }
   ],
   "source": [
    "all_data = get_small_deviation(all_data)\n",
    "all_data = calc_reciprocal_deviation(all_data, bias=0.1)\n",
    "print(all_data.shape)\n",
    "all_data = all_data.drop(['left', 'right'], axis=1)\n",
    "all_data = calc_force(all_data)\n",
    "print('main')\n",
    "print(len(all_data))\n",
    "print(len(all_data.dropna()))\n",
    "all_data = calc_velocity(all_data)\n",
    "# all_data = calc_resultent_force(all_data)\n",
    "# all_data = calc_resultent_force_xy(all_data)\n",
    "# all_data = calc_velocity_xy(all_data)\n",
    "# all_data = calc_force_velocity_angle_xy(all_data)\n",
    "# all_data = calc_abs_force_velocity_angle_xy(all_data)\n",
    "# all_data = calc_torque_xy(all_data)\n",
    "all_data = calc_torque_turning_freq(all_data)\n",
    "all_data = calc_inverted_tt_freq(all_data, bias=1)\n",
    "# all_data = calc_velocity_angle(all_data)\n",
    "# all_data = calc_velocity_angle_without_backward(all_data)\n",
    "# stop()\n",
    "# all_data = calc_sd_velocity(all_data, by=group_unit, dropna=False)\n",
    "# all_data = calc_velocity_skew(all_data, by=group_unit, dropna=False)\n",
    "# all_data = calc_velocity_kurtosis(all_data, by=group_unit, dropna=False)\n",
    "# all_data = calc_torque_xy_avg(all_data, dropna=False)\n",
    "# all_data = calc_avg_velocity(all_data, by=group_unit, dropna=False)\n",
    "# all_data = calc_avg_angle_velocity_xy(all_data, by=group_unit, dropna=False)\n",
    "# all_data = calc_va_sum(all_data, by=group_unit, dropna=False)\n",
    "all_data = set_inverted_task(all_data)\n",
    "all_data = set_sidewards_task(all_data)\n",
    "all_data = is_disturbed(all_data)\n",
    "all_data = is_backward(all_data)\n",
    "# all_data = calc_confidence(all_data)\n",
    "# all_data = calc_velocity_deviation_score(all_data, a=1)\n",
    "# all_data = calc_score(all_data)\n",
    "# all_data = all_data.dropna()\n",
    "print('main')\n",
    "print(len(all_data))\n",
    "# print(all_data['sum_va'].describe())\n",
    "# sns.set_style('whitegrid')\n",
    "# f, ax= plt.subplots(figsize = (14, 10))\n",
    "# ax = sns.violinplot(y='sum_va', data=all_data)\n",
    "# ax.set_title(index)\n",
    "# plt.show()\n",
    "# all_data = del_outlier(all_data, None, 'v_skew')\n",
    "# all_data = del_outlier(all_data, None, 'v_kurt')\n",
    "# all_data = all_data[selected_cols1]\n",
    "# all_data = difficulty_mapper(all_data, cols=mapper_cols, levels=mapper_levels)\n",
    "all_data = set_difficulty(all_data, def_cols=diff_def, levels=None)\n",
    "\n",
    "if b_del_outlier:\n",
    "    all_data = del_outlier(all_data, \n",
    "                ['difficulty', 'uid', 'day', 'exc_num', 'exc_times'], \n",
    "                test_col=test_col)\n",
    "all_data = all_data.reset_index(drop=True)\n",
    "all_data = all_data[selected_cols]\n",
    "# groups = all_data[all_data['exc_num'] == 1.1].groupby(['day', 'uid', 'exc_num', 'exc_times'])\n",
    "print(all_data.head())\n",
    "\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tmp_col = 'v_sd'\n",
    "# # tmp = all_data[(all_data['day']==1) & \n",
    "# #                (all_data['uid']==7) & \n",
    "# #                (all_data['exc_num']==1.1) & \n",
    "# #                (all_data['exc_times']==2)][tmp_col]\n",
    "# tmp = all_data.groupby(group_unit).head()\n",
    "# print(tmp)\n",
    "# del tmp\n",
    "# # print(tmp['Az'].rolling(100).mean().head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### delete outlier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(len(all_data))\n",
    "# all_data = all_data.dropna()\n",
    "# print(len(all_data))\n",
    "# # \n",
    "# l1 = len(all_data[['uid', 'day', 'exc_num', 'exc_times']].drop_duplicates())\n",
    "# print(l1)\n",
    "# upper = all_data[test_col].quantile(0.999)\n",
    "# print(upper)\n",
    "# outlier = all_data[all_data[test_col]>upper]\n",
    "# outlier = outlier[['uid', 'day', 'exc_num', 'exc_times']].drop_duplicates()\n",
    "# all_data = all_data[all_data[test_col]<=upper]\n",
    "# all_data,_ = ability_level_mapper(all_data, col=test_col, n_level=19, target_col_name='perf', invert=True)\n",
    "# print('outlier')\n",
    "# print(len(outlier))\n",
    "# print(len(outlier)/l1)\n",
    "# print(all_data[[test_col, 'perf']].describe())\n",
    "# # del all_data\n",
    "# del outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   day  uid  exc_num  exc_times  inv_tt_freq  difficulty  deviation  velocity\n",
      "0  1.0  1.0      1.1        1.0         63.0           1   0.005096  0.113868\n",
      "1  1.0  1.0      1.1        1.0         63.0           1   0.005096  0.113868\n",
      "2  1.0  1.0      1.1        1.0         63.0           1   0.005096  0.113868\n",
      "3  1.0  1.0      1.1        1.0         63.0           1   0.005096  0.113868\n",
      "4  1.0  1.0      1.1        1.0         63.0           1   0.005096  0.113868\n"
     ]
    }
   ],
   "source": [
    "print(all_data.head())\n",
    "all_data.to_csv('../data/step1_clear_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'avg_av_xy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-683425ac07a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_av_xy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlist1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'exc_num'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'difficulty'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlist2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'exc_num'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'difficulty'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ml3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4374\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4375\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4376\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'avg_av_xy'"
     ]
    }
   ],
   "source": [
    "print(all_data.avg_av_xy.describe())\n",
    "list1 = all_data[all_data['exc_num']==1.1]['difficulty'].unique()\n",
    "list2 = all_data[all_data['exc_num']==2.1]['difficulty'].unique()\n",
    "l1 = len(list1)\n",
    "l3 = len(list2)\n",
    "print('diff in 1.1: '+str(l1))\n",
    "print('diff in 2.1: '+str(l3))\n",
    "common = list(set(list1).intersection(list2))\n",
    "print('the common diff: ' +str(len(common)))\n",
    "print('percentage: ' +str(len(common)/l1),str(len(common)/l3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tmp = all_data[all_data.v_angle>3.1]\n",
    "# col = 'avg_av_xy'\n",
    "# tmp = all_data.loc[#((all_data.uid==1) | (all_data.uid==3)) & \n",
    "#                    ((all_data.exc_num==1.3) | #(all_data.exc_num==2.3) | \n",
    "#                                                               (all_data.exc_num==1.1))]\n",
    "\n",
    "# # tmp = all_data.loc[((all_data.uid==1) | (all_data.uid==3))]\n",
    "# # tmp = tmp.loc[tmp.exc_num!=4.2]\n",
    "# sns.set_style('whitegrid')\n",
    "# f, ax= plt.subplots(figsize = (14, 10))\n",
    "# ax = sns.violinplot(x=\"exc_num\", y=col, hue='uid', data=tmp)\n",
    "\n",
    "# tmp = all_data.loc[((all_data.uid==1) | (all_data.uid==11)) & ((all_data.exc_num==1.3) | #(all_data.exc_num==2.3) | \n",
    "#                                                              (all_data.exc_num==1.1))]\n",
    "# sns.set_style('whitegrid')\n",
    "# f, ax= plt.subplots(figsize = (14, 10))\n",
    "# ax = sns.violinplot(x=\"exc_num\", y=col, hue='uid', data=tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # del tmp\n",
    "# # tmp = all_data.copy()\n",
    "# tmp = all_data.loc[#((all_data.uid==2) | (all_data.uid==6)) \n",
    "#                     (all_data.day==2) \n",
    "# #                    & (all_data.exc_num==1.3) \n",
    "# #                    & (all_data.exc_times==1)\n",
    "#                   ]\n",
    "# cnt = 0\n",
    "# by_cols = ['v_angle', 'v_sd',\n",
    "#            'torque_xy_avg', 'inverted', 'day', 'exc_num', 'exc_times'\n",
    "#           ]\n",
    "# for index,group in tmp.groupby(by_cols):\n",
    "    \n",
    "#     if len(group)<100 or len(group['uid'].unique())<8:\n",
    "#         continue\n",
    "#     print(len(group))\n",
    "#     cnt += 1\n",
    "#     if cnt>10: continue\n",
    "#     group = group.reset_index()\n",
    "#     sns.set_style('whitegrid')\n",
    "#     f, ax= plt.subplots(figsize = (14, 10))\n",
    "#     ax = sns.violinplot(x=\"v_angle\", y='velocity', hue='uid', data=group)\n",
    "#     ax.set_title(index)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col = 'sum_va'\n",
    "tmp = all_data.loc[(all_data.uid==3) & (all_data.day==1) & (all_data.exc_num==1.3) & (all_data.exc_times==1)]\n",
    "sns.set_style('whitegrid')\n",
    "f, ax= plt.subplots(figsize = (14, 10))\n",
    "ax = sns.lineplot(x=tmp.index, y=col, data=tmp)\n",
    "\n",
    "tmp = all_data.loc[((all_data.uid==4)) \n",
    "                   & (all_data.day==1) & (all_data.exc_num==1.1) & (all_data.exc_times==1)]\n",
    "sns.set_style('whitegrid')\n",
    "f, ax= plt.subplots(figsize = (14, 10))\n",
    "ax = sns.lineplot(x=tmp.index, y=col, data=tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # parameters\n",
    "# target_col = 'velocity'\n",
    "# target_exc = None\n",
    "# cnt = 0\n",
    "\n",
    "\n",
    "# if target_exc is None:\n",
    "#     groups = all_data[['day', 'uid', 'exc_num', 'exc_times', target_col]].groupby(['day', 'uid', 'exc_num', 'exc_times'])\n",
    "# else:\n",
    "#     tmp = all_data[all_data['exc_num']==target_exc]\n",
    "#     assert len(tmp)>10, 'empty data'\n",
    "#     groups = tmp[['day', 'uid', 'exc_num', 'exc_times', target_col]].groupby(['day', 'uid', 'exc_num', 'exc_times'])\n",
    "#     del tmp\n",
    "\n",
    "\n",
    "\n",
    "# for index, group in groups:\n",
    "#     if cnt>10:\n",
    "#         break\n",
    "#     cnt += 1\n",
    "    \n",
    "# #     print(index)\n",
    "#     tmp = group.reset_index(drop=True)\n",
    "#     tmp = tmp.reset_index()\n",
    "# #     print(group)\n",
    "#     sns.set_style('whitegrid')\n",
    "#     f, ax= plt.subplots(figsize = (14, 10))\n",
    "#     ax = sns.lineplot(x=\"index\", y=target_col, data=tmp)\n",
    "#     ax.set_title(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce initial deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_header(data, head=500, has_tail=False):\n",
    "    \n",
    "    groups = data.set_index(['day', 'exc_num', 'exc_times', 'uid'])\n",
    "    groups = groups.groupby(groups.index)\n",
    "    result = pd.DataFrame()\n",
    "    \n",
    "    for index, group in groups:\n",
    "        \n",
    "        if has_tail:\n",
    "            assert len(group)>(head*2), 'outlier'\n",
    "        else:\n",
    "            assert len(group)>head, 'outlier'\n",
    "            \n",
    "        header = [i+1 for i in xrange(head)]\n",
    "        if has_tail:\n",
    "            tailer = header[::-1]\n",
    "            middle = [head for i in xrange(len(group)-head*2)]\n",
    "            weights = header+middle+tailer\n",
    "\n",
    "        else:\n",
    "            middle = [head for i in xrange(len(group)-head)]\n",
    "\n",
    "            weights = header+middle\n",
    "            \n",
    "        weights = pd.Series(weights)    \n",
    "        weights = weights/head\n",
    "        assert len(group)==len(weights), 'different length between group and weights'\n",
    "        curr = group.reset_index()\n",
    "        curr['front'] =curr['front']*weights\n",
    "        result = pd.concat([result, curr])\n",
    "        del curr\n",
    "        \n",
    "    return result.reset_index(drop=True)\n",
    "                            \n",
    "\n",
    "# new_data = reduce_header(new_data,  head=300, has_tail=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(new_data))\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.to_csv('../data/step1_clear_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
