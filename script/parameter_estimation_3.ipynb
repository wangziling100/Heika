{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from six.moves import xrange\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cMGRM(theta, alpha=1.5, b=0.5, c=0):\n",
    "    # MGRM cumulative probability\n",
    "    return math.e**(alpha*(theta-b+c))/(1+math.e**(alpha*(theta-b+c)))\n",
    "\n",
    "def cMGRM_user_df(data, theta, c):\n",
    "    # Matrix operation\n",
    "    return math.e**(data['alpha']*(theta-data['b']+c))/(1+math.e**(data['alpha']*(theta-data['b']+c)))\n",
    "\n",
    "def cMGRM_exc_df(data, alpha, b, c):\n",
    "    # Matrix operation\n",
    "    return math.e**(alpha*(data['theta']-b+c))/(1+math.e**(alpha*(data['theta']-b+c)))\n",
    "\n",
    "def model(theta, alpha, b, cl=[1,0,-0.6], start=1):\n",
    "    sum = start*1\n",
    "    for c in cl:\n",
    "        sum += cMGRM(theta, alpha, b, c)\n",
    "    return sum\n",
    "\n",
    "def model_user_df(data, theta, cl, start=1):\n",
    "    sum = pd.Series(np.zeros(len(data)))\n",
    "    sum += start*1\n",
    "    for c in cl:\n",
    "        sum += cMGRM_user_df(data, theta, c)\n",
    "    return sum\n",
    "\n",
    "def model_exc_df(data, alpha, b, cl, start=1):\n",
    "    sum = pd.Series(np.zeros(len(data)))\n",
    "    sum += start*1\n",
    "    for c in cl:\n",
    "        sum += cMGRM_exc_df(data, alpha, b, c)\n",
    "        \n",
    "    return sum\n",
    "\n",
    "def partial_derivative_model_alpha(theta, alpha, b, cl):\n",
    "    # partial derivative of model to alpha\n",
    "    sum = 0\n",
    "    for c in cl:\n",
    "        curr = cMGRM(theta, alpha, b, c)*(1-cMGRM(theta, alpha, b, c))*math.e**(alpha*(theta-b+c))*(theta-b+c)\n",
    "        sum += curr\n",
    "        \n",
    "    return sum\n",
    "\n",
    "def partial_derivative_model_b(theta, alpha, b, cl):\n",
    "    # partial derivative of model to b\n",
    "    sum = 0\n",
    "    for c in cl:\n",
    "        curr = cMGRM(theta, alpha, b, c)*(1-cMGRM(theta, alpha, b, c))*math.e**(alpha*(theta-b+c))*(-alpha)\n",
    "        sum += curr\n",
    "        \n",
    "    return sum\n",
    "        \n",
    "\n",
    "def partial_derivative_model_theta(theta, alpha, b, cl):\n",
    "    # partial derivative of model to theta\n",
    "    sum = 0\n",
    "    for c in cl:\n",
    "        curr = cMGRM(theta, alpha, b, c)*(1-cMGRM(theta, alpha, b, c))*math.e**(alpha*(theta-b+c))*(alpha)\n",
    "        sum += curr\n",
    "        \n",
    "    return sum\n",
    "\n",
    "def partial_derivative_loss(theta, alpha, b, cl, y):\n",
    "    # partial derivative of loss function to alpha\n",
    "    # loss = (model(theta)-y))**2\n",
    "    \n",
    "    gradient_alpha = 2*(model(theta, alpha, b, cl)-y)*partial_derivative_model_alpha(theta, alpha, b, cl)\n",
    "    gradient_b = 2*(model(theta, alpha, b, cl)-y)*partial_derivative_model_b(theta, alpha, b, cl)\n",
    "    gradient_theta = 2*(model(theta, alpha, b, cl)-y)*partial_derivative_model_theta(theta, alpha, b, cl)\n",
    "    \n",
    "    return [gradient_alpha, gradient_b, gradient_theta]\n",
    "\n",
    "def partial_derivative_model_alpha2_b2_df(data, alpha, b, cl):\n",
    "    # partial derivative of model to alpha\n",
    "    \n",
    "    g_alpha = 0\n",
    "    g_b = 0\n",
    "    \n",
    "    \n",
    "    for c in cl:\n",
    "        curr = cMGRM_exc_df(data, alpha, b, c)*(1-cMGRM_exc_df(data, alpha, b, c))*(-data['theta']+b-c)\n",
    "        g_alpha += curr\n",
    "        curr = cMGRM_exc_df(data, alpha, b, c)*(1-cMGRM_exc_df(data, alpha, b, c))*(alpha)\n",
    "        g_b += curr\n",
    "        \n",
    "    return g_alpha, g_b\n",
    "\n",
    "\n",
    "\n",
    "def partial_derivative_model_theta2_df(data, theta, cl):\n",
    "    # partial derivative of model to theta\n",
    "    g_theta = 0\n",
    "    for c in cl:\n",
    "        curr = cMGRM_user_df(data,theta, c)*(1-cMGRM_user_df(data,theta, c))*(-data['alpha'])\n",
    "        g_theta += curr\n",
    "     \n",
    "    return g_theta\n",
    "\n",
    "def partial_derivative_loss2(theta, alpha, b, cl, y):\n",
    "    # partial derivative of loss function to alpha\n",
    "    # loss = (model(theta)-y))**2\n",
    "    \n",
    "    gradient_alpha = 2*(model(theta, alpha, b, cl)-y)*partial_derivative_model_alpha2(theta, alpha, b, cl)\n",
    "    gradient_b = 2*(model(theta, alpha, b, cl)-y)*partial_derivative_model_b2(theta, alpha, b, cl)\n",
    "    gradient_theta = 2*(model(theta, alpha, b, cl)-y)*partial_derivative_model_theta2(theta, alpha, b, cl)\n",
    "    \n",
    "    return [gradient_alpha, gradient_b, gradient_theta]\n",
    "\n",
    "def loss(theta, alpha, b, cl, y):\n",
    "    return (model(theta, alpha, b, cl)-y)**2\n",
    "\n",
    "def sum_loss(data, parameters, cl):\n",
    "    sum = 0\n",
    "    uid_p, exc_p = parameters\n",
    "    for index, group in data.groupby(['day', 'uid', 'exc_num']):\n",
    "        day, uid, exc = index\n",
    "        alpha = exc_p.loc[exc_p.exc_num==exc, 'alpha'].reset_index(drop=True)[0]\n",
    "        b = exc_p.loc[exc_p.exc_num==exc, 'b'].reset_index(drop=True)[0]\n",
    "        theta = uid_p.loc[(uid_p.day==day) & (uid_p.uid==uid), 'theta'].reset_index(drop=True)[0]\n",
    "        y = group['performance'].reset_index(drop=True)[0]\n",
    "        curr = loss(theta, alpha, b, cl, y)\n",
    "        sum += curr\n",
    "        \n",
    "    mean = sum/len(data)\n",
    "    return sum, mean\n",
    "\n",
    "def calc_gradient_user(data, parameters, cl):\n",
    "    # calc sum loss for a certain user, so we select all loss by this user\n",
    "    gradient_theta = pd.DataFrame()\n",
    "    uid_p, exc_p = parameters\n",
    "    \n",
    "    days = data['day'].unique()\n",
    "    \n",
    "    for day in days:\n",
    "        tmp = uid_p[uid_p['day'] == day]\n",
    "        uids = tmp['uid'].unique()\n",
    "        for uid in uids:\n",
    "            tmp2 = tmp[tmp['uid'] == uid]\n",
    "            tmp2 = tmp2.reset_index()\n",
    "            theta = tmp2['theta'][0]\n",
    "            \n",
    "            excs = data.set_index(['day', 'uid']).loc[(day, uid), 'exc_num'].unique()\n",
    "            excs_para_of_a_certain_user = exc_p.set_index(['exc_num']).loc[(excs)].reset_index()\n",
    "            \n",
    "            expected_value = model_user_df(excs_para_of_a_certain_user, theta=theta, cl=cl)\n",
    "            \n",
    "            y = data.set_index(['day', 'uid']).loc[(day, uid), ['exc_num', 'performance']]\n",
    "            excs2 = y['exc_num']\n",
    "            assert np.array_equal(excs, excs2.as_matrix()), \\\n",
    "            'false alignment between expected value and average value'\n",
    "            y = y['performance'].reset_index(drop=True)\n",
    "               \n",
    "            diff = expected_value-y\n",
    "            g_theta = partial_derivative_model_theta2_df(excs_para_of_a_certain_user, theta, cl)\n",
    "            g_theta = 2*np.sum(diff*g_theta)\n",
    "            \n",
    "            \n",
    "            curr = pd.DataFrame(np.array([day, uid, g_theta]).reshape(1,-1), columns=['day', 'uid', 'theta'])\n",
    "            gradient_theta = pd.concat([gradient_theta, curr])\n",
    "     \n",
    "    gradient_theta = gradient_theta.reset_index(drop=True)\n",
    "    return gradient_theta\n",
    "\n",
    "def calc_gradient_exc(data, parameters, cl):\n",
    "    # calc sum loss for a certain exercise, so we select all loss by this exercise\n",
    "    gradient_alpha_b = pd.DataFrame()\n",
    "    uid_p, exc_p = parameters\n",
    "    \n",
    "    excs = data['exc_num'].unique()\n",
    "    for exc in excs:\n",
    "        \n",
    "        tmp = exc_p[exc_p['exc_num']==exc].reset_index()\n",
    "        alpha = tmp['alpha'][0]\n",
    "        b = tmp['b'][0]\n",
    "        days = data.loc[data.exc_num==exc, 'day'].unique()\n",
    "        \n",
    "        for day in days:\n",
    "            tmp2 = uid_p[uid_p['day'] == day]\n",
    "            uids = data.set_index(['day', 'exc_num']).loc[(day, exc), 'uid'].unique()\n",
    "            tmp2 = tmp2.set_index(['uid']).loc[uids].reset_index(drop=True)\n",
    "            \n",
    "            expected_value = model_exc_df(tmp2, alpha, b, cl)\n",
    "            y = data.loc[(data.day==day)&(data.exc_num==exc), ['uid', 'performance']]\n",
    "            y = y.set_index(['uid']).loc[uids, 'performance'].reset_index(drop=True)\n",
    "#             loss = np.sum(expected_value-y)\n",
    "            diff = expected_value-y\n",
    "            g_alpha, g_b = partial_derivative_model_alpha2_b2_df(tmp2, alpha, b, cl)\n",
    "            g_alpha = 2*np.sum(diff*g_alpha)\n",
    "            g_b = 2*np.sum(diff*g_b)\n",
    "            \n",
    "            \n",
    "            curr = pd.DataFrame(np.array([exc, g_alpha, g_b]).reshape(1,-1), columns=['exc_num', 'alpha', 'b'])\n",
    "            gradient_alpha_b = pd.concat([gradient_alpha_b, curr])\n",
    "            \n",
    "    gradient_alpha_b = gradient_alpha_b.groupby(['exc_num']).sum().reset_index()\n",
    "    \n",
    "    return gradient_alpha_b\n",
    "    \n",
    "    \n",
    "    \n",
    "def calc_gradient_vector(data, parameters):\n",
    "    # consider the ability of the same person in different training day as different\n",
    "    uid_p, exc_p = parameters\n",
    "    cl = [1.359, 1.1, 0.919, 0.782, 0.62, 0.418, 0.388, 0.075, -0.054, -0.056, \n",
    "          -0.059, -0.103, -0.156, -0.394, -0.415,\n",
    "          -0.478, -1.377, -1.471, -1.989]\n",
    "    \n",
    "    sum_loss = 0\n",
    "    \n",
    "    uid_gradient = pd.DataFrame(np.array([]).reshape(0, 3), columns=['uid', 'day', 'theta_g'])\n",
    "    exc_gradient = pd.DataFrame(np.array([]).reshape(0, 3), columns=['exc_num', 'alpha_g', 'b_g'])\n",
    "    \n",
    "    for day in xrange(5):\n",
    "        day_data = data_day1= data[data['day']==day+1]\n",
    "        if len(day_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        groups = day_data.groupby(['exc_num', 'uid'])\n",
    "        \n",
    "        for index, group in groups:\n",
    "            \n",
    "            exc_num, uid = index\n",
    "            theta = uid_p.set_index(['uid', 'day']).loc[(uid, day+1)].tolist()[0]\n",
    "            \n",
    "            alpha = exc_p.set_index(['exc_num']).loc[exc_num, 'alpha']\n",
    "            b = exc_p.set_index(['exc_num']).loc[exc_num, 'b']\n",
    "            y = group['performance'].tolist()[0]\n",
    "        \n",
    "            \n",
    "            gradient_alpha, gradient_b, gradient_theta = partial_derivative_loss(theta, alpha, b, cl, y)\n",
    "            curr1 = pd.DataFrame(np.array([uid, day+1, gradient_theta]).reshape(1,3), \n",
    "                                 columns=['uid', 'day', 'theta_g'])\n",
    "            curr2 = pd.DataFrame(np.array([exc_num, gradient_alpha, gradient_b]).reshape(1,3), \n",
    "                                 columns=['exc_num', 'alpha_g', 'b_g'])\n",
    "            \n",
    "            \n",
    "            uid_gradient = uid_gradient.append(curr1, ignore_index=True)\n",
    "            exc_gradient = exc_gradient.append(curr2, ignore_index=True)\n",
    "            \n",
    "            sum_loss += loss(theta, alpha, b, cl, y)\n",
    "            \n",
    "    mean_loss = sum_loss/len(data)\n",
    "    return ([uid_gradient, exc_gradient], sum_loss, mean_loss)\n",
    "\n",
    "def calc_gradient_vector_df(data, parameters, cl):\n",
    "    gradient_alpha \n",
    "    alphas = sum_loss_exc(data, parameters, cl)*calc_\n",
    "\n",
    "def update(parameter, gradient, alpha=0.001):\n",
    "    uid_p, exc_p = parameters\n",
    "    uid_gradient, exc_gradient = gradient\n",
    "    \n",
    "    # calc average theta gradient by each user in each day\n",
    "    uid_avg = uid_gradient.groupby(['uid', 'day']).mean()    \n",
    "    # calc average alpha and b by each exercise\n",
    "    exc_avg = exc_gradient.groupby(['exc_num']).mean()\n",
    "    \n",
    "    uid_step = uid_avg*alpha\n",
    "    exc_step = exc_avg*alpha\n",
    "    \n",
    "    uid_result = pd.concat([uid_p.set_index(['uid', 'day']), uid_step], join='inner', axis=1)\n",
    "    exc_result = pd.concat([exc_p.set_index(['exc_num']), exc_step], join='inner', axis=1)\n",
    "    \n",
    "    uid_result['theta'] -= uid_result['theta_g']\n",
    "    exc_result['alpha'] -= exc_result['alpha_g']\n",
    "    exc_result['b'] -= exc_result['b_g']\n",
    "    \n",
    "    return [uid_result.reset_index()[['uid', 'day', 'theta']], exc_result.reset_index()[['exc_num', 'alpha', 'b']]]\n",
    "\n",
    "def update2(parameter, g_alpha_b, g_theta, alpha=0.001):\n",
    "    uid_p, exc_p = parameters\n",
    "    uid_p = uid_p.set_index(['day','uid']) + alpha*g_theta.set_index(['day', 'uid'])\n",
    "    exc_p = exc_p.set_index(['exc_num']) + alpha*g_alpha_b.set_index(['exc_num'])\n",
    "    return uid_p.reset_index(), exc_p.reset_index()\n",
    "    \n",
    "    \n",
    "def initial_parameters(data):\n",
    "    \n",
    "    \n",
    "    uid_p = pd.DataFrame(np.array([]).reshape(0, 3), columns=['uid', 'day', 'theta'])\n",
    "    exc_p = pd.DataFrame(np.array([]).reshape(0, 3), columns=['exc_num', 'alpha', 'b'])\n",
    "    days = data['day'].unique()\n",
    "    \n",
    "    # compare average score over all exercises of different user\n",
    "    avg_perf = data[['exc_num', 'uid', 'performance']].groupby(['exc_num', 'uid']).mean()\n",
    "    count = avg_perf.reset_index()[['exc_num']]\n",
    "    count['count'] = 0\n",
    "    count = count[['exc_num', 'count']].groupby(['exc_num']).count()\n",
    "    excs = count[count['count']== count['count'].max()].reset_index()['exc_num'].unique()\n",
    "    avg_perf = avg_perf.reset_index().set_index(['exc_num']).loc[excs]\n",
    "    avg_perf = avg_perf.reset_index().groupby(['exc_num', 'uid']).mean()\n",
    "    avg_perf = avg_perf.reset_index().groupby(['uid']).mean()\n",
    "    \n",
    "    # transform average score in range of [-3, 3]\n",
    "    max_perf = avg_perf['performance'].max()\n",
    "    min_perf = avg_perf['performance'].min()\n",
    "    interval = max_perf-min_perf\n",
    "    \n",
    "    avg_perf['theta'] = (avg_perf['performance']-min_perf)/interval*6-3\n",
    "    \n",
    "    curr = avg_perf.reset_index()[['uid', 'theta']]\n",
    "    \n",
    "    for day in xrange(5):\n",
    "        day += 1\n",
    "        if day not in days:\n",
    "            continue\n",
    "        curr['day'] = day\n",
    "        uid_p = pd.concat([uid_p, curr])\n",
    "        \n",
    "    \n",
    "    #################################\n",
    "    # compare average score over all user of different exercises\n",
    "    avg_perf = data[['exc_num', 'uid', 'performance']].groupby(['exc_num', 'uid']).mean()\n",
    "    count = avg_perf.reset_index()[['uid', 'performance']].groupby(['uid']).count()\n",
    "    uids = count[count['performance']== count['performance'].max()].reset_index()['uid'].unique()\n",
    "    avg_perf = avg_perf.reset_index().set_index(['uid']).loc[uids]\n",
    "    avg_perf = avg_perf.reset_index().groupby(['exc_num', 'uid']).mean()\n",
    "    avg_perf = avg_perf.reset_index().groupby(['exc_num']).mean()\n",
    "    \n",
    "    \n",
    "    # use average score to represent exercise difficulty, and transform them in range of [-2, 2]\n",
    "    max_perf = avg_perf['performance'].max()\n",
    "    min_perf = avg_perf['performance'].min()\n",
    "    interval = max_perf-min_perf\n",
    "    \n",
    "    avg_perf['b'] = (avg_perf['performance']-min_perf)/interval*4-2\n",
    "    avg_perf['b'] = avg_perf['b']*-1\n",
    "    \n",
    "    avg_perf = avg_perf.reset_index()\n",
    "    avg_perf['alpha'] = pd.Series(1.5*np.random.random_sample(len(avg_perf)))\n",
    "    \n",
    "    exc_p = avg_perf[['exc_num', 'alpha', 'b']]\n",
    "    uid_p = uid_p.reset_index(drop=True)\n",
    "    exc_p = exc_p.reset_index(drop=True)\n",
    "    \n",
    "    return uid_p, exc_p\n",
    "    \n",
    "    \n",
    "def select_exc(data, selected_excs=None, selected_day=None, delete=None):\n",
    "        \n",
    "    if selected_excs is None or len(selected_excs)==0:\n",
    "        pass\n",
    "    else:\n",
    "        data = data.set_index(['exc_num'])\n",
    "        data = data.loc[selected_excs]\n",
    "        data = data.reset_index()\n",
    "        \n",
    "    if delete is not None:\n",
    "        tmp = pd.DataFrame()\n",
    "        groups = data.groupby(['day', 'exc_num'])\n",
    "        for index, group in groups:\n",
    "            if index in delete:\n",
    "                continue\n",
    "            tmp = pd.concat([tmp, group])\n",
    "        data = tmp\n",
    "        del tmp\n",
    "    \n",
    "    if selected_day is not None:\n",
    "        data = data[data['day']==selected_day]\n",
    "        data = data.reset_index(drop=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:307: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:148: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss is: 23.0034288108\n",
      "sum loss is: 598.089149081\n"
     ]
    }
   ],
   "source": [
    "cl = [1.359, 1.1, 0.919, 0.782, 0.62, 0.418, 0.388, 0.075, -0.054, -0.056, \n",
    "          -0.059, -0.103, -0.156, -0.394, -0.415,\n",
    "          -0.478, -1.377, -1.471, -1.989]\n",
    "parameters = initial_parameters(expected_performance)\n",
    "uid, exc = parameters\n",
    "g_theta = calc_gradient_user(expected_performance, parameters, cl)\n",
    "g_alpha_b = calc_gradient_exc(expected_performance, parameters, cl)\n",
    "# print(g_alpha_b)\n",
    "parameters = update2(parameters, g_alpha_b, g_theta)\n",
    "\n",
    "s_loss, mean_loss = sum_loss(expected_performance, parameters, cl)\n",
    "print('mean loss is: '+str(mean_loss))\n",
    "print('sum loss is: '+str(s_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# uid_p, exc_p = parameters\n",
    "# cl = [1.359, 1.1, 0.919, 0.782, 0.62, 0.418, 0.388, 0.075, -0.054, -0.056, \n",
    "#           -0.059, -0.103, -0.156, -0.394, -0.415,\n",
    "#           -0.478, -1.377, -1.471, -1.989]\n",
    "# tmp = partial_derivative_model_alpha2_b2_df(uid_p, 1,1, cl)\n",
    "# tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>exc_num</th>\n",
       "      <th>uid</th>\n",
       "      <th>performance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7</td>\n",
       "      <td>4.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day  exc_num  uid  performance\n",
       "0    1      0.1    1     3.000000\n",
       "1    1      0.1    4     3.000000\n",
       "2    1      0.1    5     6.000000\n",
       "3    1      0.1    7     4.333333\n",
       "4    1      1.1    1     3.000000"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_performance = pd.read_csv('../data/step2_expected_performance.csv')\n",
    "expected_performance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exc_num</th>\n",
       "      <th>day</th>\n",
       "      <th>uid</th>\n",
       "      <th>performance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.2</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.3</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>5.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    exc_num  day  uid  performance\n",
       "0       0.1    2    7     4.500000\n",
       "1       0.1    2   10     3.000000\n",
       "2       0.1    2   11    11.000000\n",
       "3       1.2    2    3     7.000000\n",
       "4       1.2    2    6     4.000000\n",
       "5       1.2    2    7     6.000000\n",
       "6       1.2    2   11     8.000000\n",
       "7       1.3    2    1     8.000000\n",
       "8       1.3    2    2     4.000000\n",
       "9       1.3    2    3     5.000000\n",
       "10      1.3    2    5     6.000000\n",
       "11      1.3    2    6     4.000000\n",
       "12      1.3    2    7     6.000000\n",
       "13      1.3    2    8     6.000000\n",
       "14      1.3    2   10     4.000000\n",
       "15      1.3    2   11     7.000000\n",
       "16      1.5    2    1     6.666667\n",
       "17      1.5    2    2     4.000000\n",
       "18      1.5    2    3     6.000000\n",
       "19      1.5    2    4     7.000000\n",
       "20      1.5    2    5     7.000000\n",
       "21      1.5    2    6     5.000000\n",
       "22      1.5    2    7     3.750000\n",
       "23      1.5    2    8     8.000000\n",
       "24      1.5    2   10     2.666667\n",
       "25      1.5    2   11     5.500000"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_excs = [0.1, 1.2, 1.3, 1.5]\n",
    "selected_day = 2\n",
    "expected_performance = select_exc(expected_performance, selected_excs, selected_day)\n",
    "expected_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no parameter file\n",
      "(   day    theta   uid\n",
      "0  2.0  1.50000   1.0\n",
      "1  2.0 -1.50000   2.0\n",
      "2  2.0  0.75000   3.0\n",
      "3  2.0  1.87500   4.0\n",
      "4  2.0  1.87500   5.0\n",
      "5  2.0 -0.37500   6.0\n",
      "6  2.0 -1.78125   7.0\n",
      "7  2.0  3.00000   8.0\n",
      "8  2.0 -3.00000  10.0\n",
      "9  2.0  0.18750  11.0,    exc_num     alpha     b\n",
      "0      0.1  0.303230 -2.00\n",
      "1      1.2  0.604228 -1.04\n",
      "2      1.3  0.593663 -0.40\n",
      "3      1.5  1.045874  2.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:307: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:148: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss is: 18.0832870596\n",
      "sum loss is: 470.165463549\n",
      "mean loss is: 0.732746164798\n",
      "sum loss is: 19.0514002848\n",
      "mean loss is: 0.493980812414\n",
      "sum loss is: 12.8435011228\n",
      "mean loss is: 0.644523973389\n",
      "sum loss is: 16.7576233081\n",
      "mean loss is: 0.563176029729\n",
      "sum loss is: 14.642576773\n",
      "mean loss is: 0.515875630209\n",
      "sum loss is: 13.4127663854\n",
      "mean loss is: 0.484800193518\n",
      "sum loss is: 12.6048050315\n",
      "mean loss is: 0.467443939554\n",
      "sum loss is: 12.1535424284\n",
      "mean loss is: 0.457953170984\n",
      "sum loss is: 11.9067824456\n",
      "mean loss is: 0.451457139096\n",
      "sum loss is: 11.7378856165\n"
     ]
    }
   ],
   "source": [
    "cl = [1.359, 1.1, 0.919, 0.782, 0.62, 0.418, 0.388, 0.075, -0.054, -0.056, \n",
    "          -0.059, -0.103, -0.156, -0.394, -0.415,\n",
    "          -0.478, -1.377, -1.471, -1.989]\n",
    "\n",
    "try:\n",
    "    parameters = pickle.load(open('../data/irt_parameters.p', 'rb'))\n",
    "    \n",
    "except:\n",
    "    print('no parameter file')\n",
    "    parameters = initial_parameters(expected_performance)\n",
    "    print(parameters)\n",
    "\n",
    "for i in xrange(1000):\n",
    "    g_alpha_b = calc_gradient_exc(expected_performance, parameters, cl)\n",
    "    g_theta = calc_gradient_user(expected_performance, parameters, cl)\n",
    "    parameters = update2(parameters, g_alpha_b, g_theta)\n",
    "    \n",
    "    if i%100 ==1:\n",
    "        s_loss, mean_loss = sum_loss(expected_performance, parameters, cl)\n",
    "        print('mean loss is: '+str(mean_loss))\n",
    "        print('sum loss is: '+str(s_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss is: 0.40275715681\n",
      "sum loss is: 10.4716860771\n",
      "mean loss is: 0.402748083188\n",
      "sum loss is: 10.4714501629\n",
      "mean loss is: 0.402739257204\n",
      "sum loss is: 10.4712206873\n",
      "mean loss is: 0.402730675186\n",
      "sum loss is: 10.4709975548\n",
      "mean loss is: 0.402722333516\n",
      "sum loss is: 10.4707806714\n",
      "mean loss is: 0.40271422863\n",
      "sum loss is: 10.4705699444\n",
      "mean loss is: 0.402706357014\n",
      "sum loss is: 10.4703652824\n",
      "mean loss is: 0.402698715203\n",
      "sum loss is: 10.4701665953\n",
      "mean loss is: 0.402691299782\n",
      "sum loss is: 10.4699737943\n",
      "mean loss is: 0.402684107386\n",
      "sum loss is: 10.469786792\n",
      "mean loss is: 0.402677134692\n",
      "sum loss is: 10.469605502\n",
      "mean loss is: 0.402670378427\n",
      "sum loss is: 10.4694298391\n",
      "mean loss is: 0.402663835361\n",
      "sum loss is: 10.4692597194\n",
      "mean loss is: 0.402657502308\n",
      "sum loss is: 10.46909506\n",
      "mean loss is: 0.402651376126\n",
      "sum loss is: 10.4689357793\n",
      "mean loss is: 0.402645453715\n",
      "sum loss is: 10.4687817966\n",
      "mean loss is: 0.402639732015\n",
      "sum loss is: 10.4686330324\n",
      "mean loss is: 0.402634208009\n",
      "sum loss is: 10.4684894082\n",
      "mean loss is: 0.402628878719\n",
      "sum loss is: 10.4683508467\n",
      "mean loss is: 0.402623741207\n",
      "sum loss is: 10.4682172714\n",
      "mean loss is: 0.402618792571\n",
      "sum loss is: 10.4680886069\n",
      "mean loss is: 0.402614029952\n",
      "sum loss is: 10.4679647787\n",
      "mean loss is: 0.402609450522\n",
      "sum loss is: 10.4678457136\n",
      "mean loss is: 0.402605051496\n",
      "sum loss is: 10.4677313389\n",
      "mean loss is: 0.402600830121\n",
      "sum loss is: 10.4676215831\n",
      "mean loss is: 0.40259678368\n",
      "sum loss is: 10.4675163757\n",
      "mean loss is: 0.402592909494\n",
      "sum loss is: 10.4674156468\n",
      "mean loss is: 0.402589204915\n",
      "sum loss is: 10.4673193278\n",
      "mean loss is: 0.402585667331\n",
      "sum loss is: 10.4672273506\n",
      "mean loss is: 0.402582294162\n",
      "sum loss is: 10.4671396482\n",
      "mean loss is: 0.402579082863\n",
      "sum loss is: 10.4670561544\n",
      "mean loss is: 0.40257603092\n",
      "sum loss is: 10.4669768039\n",
      "mean loss is: 0.402573135851\n",
      "sum loss is: 10.4669015321\n",
      "mean loss is: 0.402570395207\n",
      "sum loss is: 10.4668302754\n",
      "mean loss is: 0.402567806569\n",
      "sum loss is: 10.4667629708\n",
      "mean loss is: 0.402565367549\n",
      "sum loss is: 10.4666995563\n",
      "mean loss is: 0.402563075789\n",
      "sum loss is: 10.4666399705\n",
      "mean loss is: 0.402560928963\n",
      "sum loss is: 10.466584153\n",
      "mean loss is: 0.402558924771\n",
      "sum loss is: 10.4665320441\n",
      "mean loss is: 0.402557060946\n",
      "sum loss is: 10.4664835846\n",
      "mean loss is: 0.402555335248\n",
      "sum loss is: 10.4664387164\n",
      "mean loss is: 0.402553745464\n",
      "sum loss is: 10.4663973821\n",
      "mean loss is: 0.402552289411\n",
      "sum loss is: 10.4663595247\n",
      "mean loss is: 0.402550964934\n",
      "sum loss is: 10.4663250883\n",
      "mean loss is: 0.402549769904\n",
      "sum loss is: 10.4662940175\n",
      "mean loss is: 0.40254870222\n",
      "sum loss is: 10.4662662577\n",
      "mean loss is: 0.402547759808\n",
      "sum loss is: 10.466241755\n",
      "mean loss is: 0.402546940619\n",
      "sum loss is: 10.4662204561\n",
      "mean loss is: 0.402546242632\n",
      "sum loss is: 10.4662023084\n",
      "mean loss is: 0.402545663849\n",
      "sum loss is: 10.4661872601\n",
      "mean loss is: 0.402545202301\n",
      "sum loss is: 10.4661752598\n",
      "mean loss is: 0.402544856042\n",
      "sum loss is: 10.4661662571\n",
      "mean loss is: 0.40254462315\n",
      "sum loss is: 10.4661602019\n",
      "mean loss is: 0.40254450173\n",
      "sum loss is: 10.466157045\n",
      "mean loss is: 0.402544489909\n",
      "sum loss is: 10.4661567376\n",
      "mean loss is: 0.402544585841\n",
      "sum loss is: 10.4661592319\n",
      "mean loss is: 0.4025447877\n",
      "sum loss is: 10.4661644802\n",
      "mean loss is: 0.402545093686\n",
      "sum loss is: 10.4661724358\n",
      "mean loss is: 0.402545502021\n",
      "sum loss is: 10.4661830526\n",
      "mean loss is: 0.402546010951\n",
      "sum loss is: 10.4661962847\n",
      "mean loss is: 0.402546618744\n",
      "sum loss is: 10.4662120873\n",
      "mean loss is: 0.40254732369\n",
      "sum loss is: 10.4662304159\n",
      "mean loss is: 0.402548124101\n",
      "sum loss is: 10.4662512266\n",
      "mean loss is: 0.402549018312\n",
      "sum loss is: 10.4662744761\n",
      "mean loss is: 0.40255000468\n",
      "sum loss is: 10.4663001217\n",
      "mean loss is: 0.40255108158\n",
      "sum loss is: 10.4663281211\n",
      "mean loss is: 0.402552247413\n",
      "sum loss is: 10.4663584327\n",
      "mean loss is: 0.402553500596\n",
      "sum loss is: 10.4663910155\n",
      "mean loss is: 0.402554839571\n",
      "sum loss is: 10.4664258288\n",
      "mean loss is: 0.402556262798\n",
      "sum loss is: 10.4664628327\n",
      "mean loss is: 0.402557768757\n",
      "sum loss is: 10.4665019877\n",
      "mean loss is: 0.402559355949\n",
      "sum loss is: 10.4665432547\n",
      "mean loss is: 0.402561022895\n",
      "sum loss is: 10.4665865953\n",
      "mean loss is: 0.402562768134\n",
      "sum loss is: 10.4666319715\n",
      "mean loss is: 0.402564590226\n",
      "sum loss is: 10.4666793459\n",
      "mean loss is: 0.402566487748\n",
      "sum loss is: 10.4667286814\n",
      "mean loss is: 0.402568459298\n",
      "sum loss is: 10.4667799417\n",
      "mean loss is: 0.402570503491\n",
      "sum loss is: 10.4668330908\n",
      "mean loss is: 0.402572618961\n",
      "sum loss is: 10.466888093\n",
      "mean loss is: 0.402574804361\n",
      "sum loss is: 10.4669449134\n",
      "mean loss is: 0.402577058361\n",
      "sum loss is: 10.4670035174\n",
      "mean loss is: 0.402579379649\n",
      "sum loss is: 10.4670638709\n",
      "mean loss is: 0.402581766931\n",
      "sum loss is: 10.4671259402\n",
      "mean loss is: 0.402584218929\n",
      "sum loss is: 10.4671896922\n",
      "mean loss is: 0.402586734384\n",
      "sum loss is: 10.467255094\n",
      "mean loss is: 0.402589312053\n",
      "sum loss is: 10.4673221134\n",
      "mean loss is: 0.40259195071\n",
      "sum loss is: 10.4673907185\n",
      "mean loss is: 0.402594649146\n",
      "sum loss is: 10.4674608778\n",
      "mean loss is: 0.402597406166\n",
      "sum loss is: 10.4675325603\n",
      "mean loss is: 0.402600220596\n",
      "sum loss is: 10.4676057355\n",
      "mean loss is: 0.402603091273\n",
      "sum loss is: 10.4676803731\n",
      "mean loss is: 0.402606017053\n",
      "sum loss is: 10.4677564434\n",
      "mean loss is: 0.402608996806\n",
      "sum loss is: 10.467833917\n",
      "mean loss is: 0.402612029419\n",
      "sum loss is: 10.4679127649\n",
      "mean loss is: 0.402615113793\n",
      "sum loss is: 10.4679929586\n",
      "mean loss is: 0.402618248845\n",
      "sum loss is: 10.46807447\n",
      "mean loss is: 0.402621433505\n",
      "sum loss is: 10.4681572711\n",
      "mean loss is: 0.40262466672\n",
      "sum loss is: 10.4682413347\n",
      "mean loss is: 0.402627947452\n",
      "sum loss is: 10.4683266337\n",
      "mean loss is: 0.402631274674\n",
      "sum loss is: 10.4684131415\n",
      "mean loss is: 0.402634647377\n",
      "sum loss is: 10.4685008318\n",
      "mean loss is: 0.402638064563\n",
      "sum loss is: 10.4685896786\n",
      "mean loss is: 0.402641525251\n",
      "sum loss is: 10.4686796565\n",
      "mean loss is: 0.402645028472\n",
      "sum loss is: 10.4687707403\n",
      "mean loss is: 0.402648573269\n",
      "sum loss is: 10.468862905\n",
      "mean loss is: 0.402652158702\n",
      "sum loss is: 10.4689561263\n",
      "mean loss is: 0.402655783842\n",
      "sum loss is: 10.4690503799\n",
      "mean loss is: 0.402659447773\n",
      "sum loss is: 10.4691456421\n",
      "mean loss is: 0.402663149592\n",
      "sum loss is: 10.4692418894\n",
      "mean loss is: 0.402666888409\n",
      "sum loss is: 10.4693390986\n",
      "mean loss is: 0.402670663349\n",
      "sum loss is: 10.4694372471\n",
      "mean loss is: 0.402674473544\n",
      "sum loss is: 10.4695363122\n",
      "mean loss is: 0.402678318145\n",
      "sum loss is: 10.4696362718\n",
      "mean loss is: 0.402682196309\n",
      "sum loss is: 10.469737104\n",
      "mean loss is: 0.402686107209\n",
      "sum loss is: 10.4698387874\n",
      "mean loss is: 0.402690050029\n",
      "sum loss is: 10.4699413007\n",
      "mean loss is: 0.402694023963\n",
      "sum loss is: 10.470044623\n",
      "mean loss is: 0.40269802822\n",
      "sum loss is: 10.4701487337\n",
      "mean loss is: 0.402702062017\n",
      "sum loss is: 10.4702536124\n",
      "mean loss is: 0.402706124584\n",
      "sum loss is: 10.4703592392\n",
      "mean loss is: 0.402710215163\n",
      "sum loss is: 10.4704655942\n",
      "mean loss is: 0.402714333004\n",
      "sum loss is: 10.4705726581\n",
      "mean loss is: 0.402718477372\n",
      "sum loss is: 10.4706804117\n",
      "mean loss is: 0.402722647539\n",
      "sum loss is: 10.470788836\n",
      "mean loss is: 0.402726842791\n",
      "sum loss is: 10.4708979126\n",
      "mean loss is: 0.402731062421\n",
      "sum loss is: 10.471007623\n",
      "mean loss is: 0.402735305736\n",
      "sum loss is: 10.4711179491\n",
      "mean loss is: 0.402739572049\n",
      "sum loss is: 10.4712288733\n",
      "mean loss is: 0.402743860688\n",
      "sum loss is: 10.4713403779\n",
      "mean loss is: 0.402748170987\n",
      "sum loss is: 10.4714524457\n",
      "mean loss is: 0.402752502292\n",
      "sum loss is: 10.4715650596\n",
      "mean loss is: 0.402756853959\n",
      "sum loss is: 10.4716782029\n",
      "mean loss is: 0.402761225351\n",
      "sum loss is: 10.4717918591\n",
      "mean loss is: 0.402765615843\n",
      "sum loss is: 10.4719060119\n",
      "mean loss is: 0.402770024819\n",
      "sum loss is: 10.4720206453\n",
      "mean loss is: 0.402774451672\n",
      "sum loss is: 10.4721357435\n",
      "mean loss is: 0.402778895804\n",
      "sum loss is: 10.4722512909\n",
      "mean loss is: 0.402783356625\n",
      "sum loss is: 10.4723672723\n",
      "mean loss is: 0.402787833557\n",
      "sum loss is: 10.4724836725\n",
      "mean loss is: 0.402792326028\n",
      "sum loss is: 10.4726004767\n",
      "mean loss is: 0.402796833475\n",
      "sum loss is: 10.4727176703\n",
      "mean loss is: 0.402801355344\n",
      "sum loss is: 10.472835239\n",
      "mean loss is: 0.402805891091\n",
      "sum loss is: 10.4729531684\n",
      "mean loss is: 0.402810440178\n",
      "sum loss is: 10.4730714446\n",
      "mean loss is: 0.402815002077\n",
      "sum loss is: 10.473190054\n",
      "mean loss is: 0.402819576266\n",
      "sum loss is: 10.4733089829\n",
      "mean loss is: 0.402824162234\n",
      "sum loss is: 10.4734282181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss is: 0.402828759475\n",
      "sum loss is: 10.4735477464\n",
      "mean loss is: 0.402833367494\n",
      "sum loss is: 10.4736675548\n",
      "mean loss is: 0.402837985801\n",
      "sum loss is: 10.4737876308\n",
      "mean loss is: 0.402842613915\n",
      "sum loss is: 10.4739079618\n",
      "mean loss is: 0.402847251362\n",
      "sum loss is: 10.4740285354\n",
      "mean loss is: 0.402851897675\n",
      "sum loss is: 10.4741493396\n",
      "mean loss is: 0.402856552397\n",
      "sum loss is: 10.4742703623\n",
      "mean loss is: 0.402861215074\n",
      "sum loss is: 10.4743915919\n",
      "mean loss is: 0.402865885263\n",
      "sum loss is: 10.4745130168\n",
      "mean loss is: 0.402870562527\n",
      "sum loss is: 10.4746346257\n",
      "mean loss is: 0.402875246434\n",
      "sum loss is: 10.4747564073\n",
      "mean loss is: 0.402879936562\n",
      "sum loss is: 10.4748783506\n",
      "mean loss is: 0.402884632493\n",
      "sum loss is: 10.4750004448\n",
      "mean loss is: 0.402889333818\n",
      "sum loss is: 10.4751226793\n",
      "mean loss is: 0.402894040133\n",
      "sum loss is: 10.4752450435\n",
      "mean loss is: 0.402898751042\n",
      "sum loss is: 10.4753675271\n",
      "mean loss is: 0.402903466153\n",
      "sum loss is: 10.47549012\n",
      "mean loss is: 0.402908185083\n",
      "sum loss is: 10.4756128122\n",
      "mean loss is: 0.402912907455\n",
      "sum loss is: 10.4757355938\n",
      "mean loss is: 0.402917632896\n",
      "sum loss is: 10.4758584553\n",
      "mean loss is: 0.402922361042\n",
      "sum loss is: 10.4759813871\n",
      "mean loss is: 0.402927091533\n",
      "sum loss is: 10.4761043799\n",
      "mean loss is: 0.402931824016\n",
      "sum loss is: 10.4762274244\n",
      "mean loss is: 0.402936558143\n",
      "sum loss is: 10.4763505117\n",
      "mean loss is: 0.402941293572\n",
      "sum loss is: 10.4764736329\n",
      "mean loss is: 0.402946029968\n",
      "sum loss is: 10.4765967792\n",
      "mean loss is: 0.402950767001\n",
      "sum loss is: 10.476719942\n",
      "mean loss is: 0.402955504346\n",
      "sum loss is: 10.476843113\n",
      "mean loss is: 0.402960241683\n",
      "sum loss is: 10.4769662837\n",
      "mean loss is: 0.402964978699\n",
      "sum loss is: 10.4770894462\n",
      "mean loss is: 0.402969715085\n",
      "sum loss is: 10.4772125922\n",
      "mean loss is: 0.402974450539\n",
      "sum loss is: 10.477335714\n",
      "mean loss is: 0.402979184762\n",
      "sum loss is: 10.4774588038\n",
      "mean loss is: 0.402983917462\n",
      "sum loss is: 10.477581854\n",
      "mean loss is: 0.402988648352\n",
      "sum loss is: 10.4777048571\n",
      "mean loss is: 0.402993377148\n",
      "sum loss is: 10.4778278058\n",
      "mean loss is: 0.402998103573\n",
      "sum loss is: 10.4779506929\n",
      "mean loss is: 0.403002827354\n",
      "sum loss is: 10.4780735112\n",
      "mean loss is: 0.403007548223\n",
      "sum loss is: 10.4781962538\n",
      "mean loss is: 0.403012265918\n",
      "sum loss is: 10.4783189139\n",
      "mean loss is: 0.403016980179\n",
      "sum loss is: 10.4784414846\n",
      "mean loss is: 0.403021690752\n",
      "sum loss is: 10.4785639596\n",
      "mean loss is: 0.403026397389\n",
      "sum loss is: 10.4786863321\n",
      "mean loss is: 0.403031099844\n",
      "sum loss is: 10.4788085959\n",
      "mean loss is: 0.403035797877\n",
      "sum loss is: 10.4789307448\n",
      "mean loss is: 0.403040491252\n",
      "sum loss is: 10.4790527725\n",
      "mean loss is: 0.403045179737\n",
      "sum loss is: 10.4791746732\n",
      "mean loss is: 0.403049863105\n",
      "sum loss is: 10.4792964407\n",
      "mean loss is: 0.403054541132\n",
      "sum loss is: 10.4794180694\n",
      "mean loss is: 0.4030592136\n",
      "sum loss is: 10.4795395536\n",
      "mean loss is: 0.403063880294\n",
      "sum loss is: 10.4796608876\n",
      "mean loss is: 0.403068541001\n",
      "sum loss is: 10.479782066\n",
      "mean loss is: 0.403073195516\n",
      "sum loss is: 10.4799030834\n",
      "mean loss is: 0.403077843636\n",
      "sum loss is: 10.4800239345\n",
      "mean loss is: 0.40308248516\n",
      "sum loss is: 10.4801446142\n",
      "mean loss is: 0.403087119894\n",
      "sum loss is: 10.4802651173\n",
      "mean loss is: 0.403091747647\n",
      "sum loss is: 10.4803854388\n",
      "mean loss is: 0.403096368229\n",
      "sum loss is: 10.480505574\n",
      "mean loss is: 0.403100981457\n",
      "sum loss is: 10.4806255179\n",
      "mean loss is: 0.40310558715\n",
      "sum loss is: 10.4807452659\n",
      "mean loss is: 0.403110185131\n",
      "sum loss is: 10.4808648134\n",
      "mean loss is: 0.403114775227\n",
      "sum loss is: 10.4809841559\n",
      "mean loss is: 0.403119357267\n",
      "sum loss is: 10.4811032889\n",
      "mean loss is: 0.403123931084\n",
      "sum loss is: 10.4812222082\n",
      "mean loss is: 0.403128496515\n",
      "sum loss is: 10.4813409094\n",
      "mean loss is: 0.403133053401\n",
      "sum loss is: 10.4814593884\n",
      "mean loss is: 0.403137601583\n",
      "sum loss is: 10.4815776412\n",
      "mean loss is: 0.40314214091\n",
      "sum loss is: 10.4816956637\n",
      "mean loss is: 0.403146671229\n",
      "sum loss is: 10.481813452\n",
      "mean loss is: 0.403151192395\n",
      "sum loss is: 10.4819310023\n",
      "mean loss is: 0.403155704263\n",
      "sum loss is: 10.4820483108\n",
      "mean loss is: 0.403160206691\n",
      "sum loss is: 10.482165374\n",
      "mean loss is: 0.403164699542\n",
      "sum loss is: 10.4822821881\n",
      "mean loss is: 0.403169182681\n",
      "sum loss is: 10.4823987497\n",
      "mean loss is: 0.403173655975\n",
      "sum loss is: 10.4825150553\n",
      "mean loss is: 0.403178119295\n",
      "sum loss is: 10.4826311017\n",
      "mean loss is: 0.403182572514\n",
      "sum loss is: 10.4827468854\n",
      "mean loss is: 0.403187015509\n",
      "sum loss is: 10.4828624032\n",
      "mean loss is: 0.403191448159\n",
      "sum loss is: 10.4829776521\n",
      "mean loss is: 0.403195870345\n",
      "sum loss is: 10.483092629\n",
      "mean loss is: 0.403200281952\n",
      "sum loss is: 10.4832073307\n",
      "mean loss is: 0.403204682867\n",
      "sum loss is: 10.4833217545\n",
      "mean loss is: 0.403209072979\n",
      "sum loss is: 10.4834358975\n",
      "mean loss is: 0.403213452182\n",
      "sum loss is: 10.4835497567\n",
      "mean loss is: 0.403217820369\n",
      "sum loss is: 10.4836633296\n",
      "mean loss is: 0.403222177438\n",
      "sum loss is: 10.4837766134\n",
      "mean loss is: 0.403226523288\n",
      "sum loss is: 10.4838896055\n",
      "mean loss is: 0.403230857823\n",
      "sum loss is: 10.4840023034\n",
      "mean loss is: 0.403235180945\n",
      "sum loss is: 10.4841147046\n",
      "mean loss is: 0.403239492563\n",
      "sum loss is: 10.4842268066\n",
      "mean loss is: 0.403243792585\n",
      "sum loss is: 10.4843386072\n",
      "mean loss is: 0.403248080922\n",
      "sum loss is: 10.484450104\n",
      "mean loss is: 0.403252357489\n",
      "sum loss is: 10.4845612947\n",
      "mean loss is: 0.403256622202\n",
      "sum loss is: 10.4846721772\n",
      "mean loss is: 0.403260874978\n",
      "sum loss is: 10.4847827494\n",
      "mean loss is: 0.403265115737\n",
      "sum loss is: 10.4848930092\n",
      "mean loss is: 0.403269344403\n",
      "sum loss is: 10.4850029545\n",
      "mean loss is: 0.403273560899\n",
      "sum loss is: 10.4851125834\n",
      "mean loss is: 0.403277765153\n",
      "sum loss is: 10.485221894\n",
      "mean loss is: 0.403281957092\n",
      "sum loss is: 10.4853308844\n",
      "mean loss is: 0.403286136647\n",
      "sum loss is: 10.4854395528\n",
      "mean loss is: 0.403290303751\n",
      "sum loss is: 10.4855478975\n",
      "mean loss is: 0.403294458338\n",
      "sum loss is: 10.4856559168\n",
      "mean loss is: 0.403298600345\n",
      "sum loss is: 10.485763609\n",
      "mean loss is: 0.403302729709\n",
      "sum loss is: 10.4858709724\n",
      "mean loss is: 0.403306846372\n",
      "sum loss is: 10.4859780057\n",
      "mean loss is: 0.403310950274\n",
      "sum loss is: 10.4860847071\n",
      "mean loss is: 0.403315041359\n",
      "sum loss is: 10.4861910753\n",
      "mean loss is: 0.403319119574\n",
      "sum loss is: 10.4862971089\n",
      "mean loss is: 0.403323184864\n",
      "sum loss is: 10.4864028065\n",
      "mean loss is: 0.40332723718\n",
      "sum loss is: 10.4865081667\n",
      "mean loss is: 0.403331276472\n",
      "sum loss is: 10.4866131883\n",
      "mean loss is: 0.403335302692\n",
      "sum loss is: 10.48671787\n",
      "mean loss is: 0.403339315793\n",
      "sum loss is: 10.4868222106\n",
      "mean loss is: 0.403343315733\n",
      "sum loss is: 10.486926209\n",
      "mean loss is: 0.403347302467\n",
      "sum loss is: 10.4870298641\n",
      "mean loss is: 0.403351275954\n",
      "sum loss is: 10.4871331748\n",
      "mean loss is: 0.403355236155\n",
      "sum loss is: 10.48723614\n",
      "mean loss is: 0.403359183032\n",
      "sum loss is: 10.4873387588\n",
      "mean loss is: 0.403363116547\n",
      "sum loss is: 10.4874410302\n",
      "mean loss is: 0.403367036666\n",
      "sum loss is: 10.4875429533\n",
      "mean loss is: 0.403370943354\n",
      "sum loss is: 10.4876445272\n",
      "mean loss is: 0.403374836579\n",
      "sum loss is: 10.4877457511\n",
      "mean loss is: 0.403378716311\n",
      "sum loss is: 10.4878466241\n",
      "mean loss is: 0.40338258252\n",
      "sum loss is: 10.4879471455\n",
      "mean loss is: 0.403386435176\n",
      "sum loss is: 10.4880473146\n",
      "mean loss is: 0.403390274255\n",
      "sum loss is: 10.4881471306\n",
      "mean loss is: 0.403394099729\n",
      "sum loss is: 10.4882465929\n",
      "mean loss is: 0.403397911574\n",
      "sum loss is: 10.4883457009\n",
      "mean loss is: 0.403401709769\n",
      "sum loss is: 10.488444454\n",
      "mean loss is: 0.40340549429\n",
      "sum loss is: 10.4885428515\n",
      "mean loss is: 0.403409265117\n",
      "sum loss is: 10.488640893\n",
      "mean loss is: 0.403413022231\n",
      "sum loss is: 10.488738578\n",
      "mean loss is: 0.403416765614\n",
      "sum loss is: 10.488835906\n",
      "mean loss is: 0.403420495249\n",
      "sum loss is: 10.4889328765\n",
      "mean loss is: 0.40342421112\n",
      "sum loss is: 10.4890294891\n",
      "mean loss is: 0.403427913213\n",
      "sum loss is: 10.4891257435\n",
      "mean loss is: 0.403431601513\n",
      "sum loss is: 10.4892216393\n",
      "mean loss is: 0.403435276009\n",
      "sum loss is: 10.4893171762\n",
      "mean loss is: 0.403438936689\n",
      "sum loss is: 10.4894123539\n",
      "mean loss is: 0.403442583543\n",
      "sum loss is: 10.4895071721\n",
      "mean loss is: 0.403446216562\n",
      "sum loss is: 10.4896016306\n",
      "mean loss is: 0.403449835737\n",
      "sum loss is: 10.4896957292\n",
      "mean loss is: 0.403453441062\n",
      "sum loss is: 10.4897894676\n",
      "mean loss is: 0.403457032529\n",
      "sum loss is: 10.4898828458\n",
      "mean loss is: 0.403460610135\n",
      "sum loss is: 10.4899758635\n",
      "mean loss is: 0.403464173875\n",
      "sum loss is: 10.4900685207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss is: 0.403467723744\n",
      "sum loss is: 10.4901608174\n",
      "mean loss is: 0.403471259742\n",
      "sum loss is: 10.4902527533\n",
      "mean loss is: 0.403474781867\n",
      "sum loss is: 10.4903443285\n",
      "mean loss is: 0.403478290118\n",
      "sum loss is: 10.4904355431\n",
      "mean loss is: 0.403481784495\n",
      "sum loss is: 10.4905263969\n",
      "mean loss is: 0.403485265001\n",
      "sum loss is: 10.49061689\n",
      "mean loss is: 0.403488731636\n",
      "sum loss is: 10.4907070225\n",
      "mean loss is: 0.403492184403\n",
      "sum loss is: 10.4907967945\n",
      "mean loss is: 0.403495623308\n",
      "sum loss is: 10.490886206\n",
      "mean loss is: 0.403499048353\n",
      "sum loss is: 10.4909752572\n",
      "mean loss is: 0.403502459545\n",
      "sum loss is: 10.4910639482\n",
      "mean loss is: 0.40350585689\n",
      "sum loss is: 10.4911522791\n",
      "mean loss is: 0.403509240394\n",
      "sum loss is: 10.4912402502\n",
      "mean loss is: 0.403512610065\n",
      "sum loss is: 10.4913278617\n",
      "mean loss is: 0.403515965912\n",
      "sum loss is: 10.4914151137\n",
      "mean loss is: 0.403519307944\n",
      "sum loss is: 10.4915020065\n",
      "mean loss is: 0.403522636171\n",
      "sum loss is: 10.4915885404\n",
      "mean loss is: 0.403525950602\n",
      "sum loss is: 10.4916747157\n",
      "mean loss is: 0.403529251251\n",
      "sum loss is: 10.4917605325\n",
      "mean loss is: 0.403532538128\n",
      "sum loss is: 10.4918459913\n",
      "mean loss is: 0.403535811246\n",
      "sum loss is: 10.4919310924\n",
      "mean loss is: 0.403539070619\n",
      "sum loss is: 10.4920158361\n",
      "mean loss is: 0.40354231626\n",
      "sum loss is: 10.4921002228\n",
      "mean loss is: 0.403545548184\n",
      "sum loss is: 10.4921842528\n",
      "mean loss is: 0.403548766406\n",
      "sum loss is: 10.4922679265\n",
      "mean loss is: 0.403551970941\n",
      "sum loss is: 10.4923512445\n",
      "mean loss is: 0.403555161808\n",
      "sum loss is: 10.492434207\n",
      "mean loss is: 0.403558339021\n",
      "sum loss is: 10.4925168145\n",
      "mean loss is: 0.403561502599\n",
      "sum loss is: 10.4925990676\n",
      "mean loss is: 0.40356465256\n",
      "sum loss is: 10.4926809666\n",
      "mean loss is: 0.403567788923\n",
      "sum loss is: 10.492762512\n",
      "mean loss is: 0.403570911706\n",
      "sum loss is: 10.4928437044\n",
      "mean loss is: 0.40357402093\n",
      "sum loss is: 10.4929245442\n",
      "mean loss is: 0.403577116615\n",
      "sum loss is: 10.493005032\n",
      "mean loss is: 0.403580198781\n",
      "sum loss is: 10.4930851683\n",
      "mean loss is: 0.40358326745\n",
      "sum loss is: 10.4931649537\n",
      "mean loss is: 0.403586322644\n",
      "sum loss is: 10.4932443887\n",
      "mean loss is: 0.403589364385\n",
      "sum loss is: 10.493323474\n",
      "mean loss is: 0.403592392695\n",
      "sum loss is: 10.4934022101\n",
      "mean loss is: 0.403595407597\n",
      "sum loss is: 10.4934805975\n",
      "mean loss is: 0.403598409117\n",
      "sum loss is: 10.493558637\n",
      "mean loss is: 0.403601397277\n",
      "sum loss is: 10.4936363292\n",
      "mean loss is: 0.403604372101\n",
      "sum loss is: 10.4937136746\n",
      "mean loss is: 0.403607333616\n",
      "sum loss is: 10.493790674\n",
      "mean loss is: 0.403610281846\n",
      "sum loss is: 10.493867328\n",
      "mean loss is: 0.403613216817\n",
      "sum loss is: 10.4939436373\n",
      "mean loss is: 0.403616138556\n",
      "sum loss is: 10.4940196025\n",
      "mean loss is: 0.403619047088\n",
      "sum loss is: 10.4940952243\n",
      "mean loss is: 0.403621942442\n",
      "sum loss is: 10.4941705035\n",
      "mean loss is: 0.403624824643\n",
      "sum loss is: 10.4942454407\n",
      "mean loss is: 0.403627693721\n",
      "sum loss is: 10.4943200367\n",
      "mean loss is: 0.403630549702\n",
      "sum loss is: 10.4943942922\n",
      "mean loss is: 0.403633392616\n",
      "sum loss is: 10.494468208\n",
      "mean loss is: 0.40363622249\n",
      "sum loss is: 10.4945417847\n",
      "mean loss is: 0.403639039355\n",
      "sum loss is: 10.4946150232\n",
      "mean loss is: 0.40364184324\n",
      "sum loss is: 10.4946879242\n",
      "mean loss is: 0.403644634174\n",
      "sum loss is: 10.4947604885\n",
      "mean loss is: 0.403647412188\n",
      "sum loss is: 10.4948327169\n",
      "mean loss is: 0.403650177311\n",
      "sum loss is: 10.4949046101\n",
      "mean loss is: 0.403652929575\n",
      "sum loss is: 10.494976169\n",
      "mean loss is: 0.40365566901\n",
      "sum loss is: 10.4950473943\n",
      "mean loss is: 0.403658395649\n",
      "sum loss is: 10.4951182869\n",
      "mean loss is: 0.403661109521\n",
      "sum loss is: 10.4951888475\n",
      "mean loss is: 0.403663810659\n",
      "sum loss is: 10.4952590771\n",
      "mean loss is: 0.403666499096\n",
      "sum loss is: 10.4953289765\n",
      "mean loss is: 0.403669174863\n",
      "sum loss is: 10.4953985464\n",
      "mean loss is: 0.403671837993\n",
      "sum loss is: 10.4954677878\n",
      "mean loss is: 0.403674488519\n",
      "sum loss is: 10.4955367015\n",
      "mean loss is: 0.403677126474\n",
      "sum loss is: 10.4956052883\n",
      "mean loss is: 0.403679751891\n",
      "sum loss is: 10.4956735492\n",
      "mean loss is: 0.403682364804\n",
      "sum loss is: 10.4957414849\n",
      "mean loss is: 0.403684965247\n",
      "sum loss is: 10.4958090964\n",
      "mean loss is: 0.403687553253\n",
      "sum loss is: 10.4958763846\n",
      "mean loss is: 0.403690128857\n",
      "sum loss is: 10.4959433503\n",
      "mean loss is: 0.403692692092\n",
      "sum loss is: 10.4960099944\n",
      "mean loss is: 0.403695242995\n",
      "sum loss is: 10.4960763179\n",
      "mean loss is: 0.403697781599\n",
      "sum loss is: 10.4961423216\n",
      "mean loss is: 0.40370030794\n",
      "sum loss is: 10.4962080064\n",
      "mean loss is: 0.403702822052\n",
      "sum loss is: 10.4962733733\n",
      "mean loss is: 0.403705323971\n",
      "sum loss is: 10.4963384232\n",
      "mean loss is: 0.403707813732\n",
      "sum loss is: 10.496403157\n",
      "mean loss is: 0.403710291372\n",
      "sum loss is: 10.4964675757\n",
      "mean loss is: 0.403712756925\n",
      "sum loss is: 10.4965316801\n",
      "mean loss is: 0.403715210429\n",
      "sum loss is: 10.4965954712\n",
      "mean loss is: 0.403717651919\n",
      "sum loss is: 10.4966589499\n",
      "mean loss is: 0.403720081431\n",
      "sum loss is: 10.4967221172\n",
      "mean loss is: 0.403722499001\n",
      "sum loss is: 10.496784974\n",
      "mean loss is: 0.403724904668\n",
      "sum loss is: 10.4968475214\n",
      "mean loss is: 0.403727298466\n",
      "sum loss is: 10.4969097601\n",
      "mean loss is: 0.403729680434\n",
      "sum loss is: 10.4969716913\n",
      "mean loss is: 0.403732050607\n",
      "sum loss is: 10.4970333158\n",
      "mean loss is: 0.403734409023\n",
      "sum loss is: 10.4970946346\n",
      "mean loss is: 0.40373675572\n",
      "sum loss is: 10.4971556487\n",
      "mean loss is: 0.403739090734\n",
      "sum loss is: 10.4972163591\n",
      "mean loss is: 0.403741414103\n",
      "sum loss is: 10.4972767667\n",
      "mean loss is: 0.403743725864\n",
      "sum loss is: 10.4973368725\n",
      "mean loss is: 0.403746026056\n",
      "sum loss is: 10.4973966775\n",
      "mean loss is: 0.403748314715\n",
      "sum loss is: 10.4974561826\n",
      "mean loss is: 0.40375059188\n",
      "sum loss is: 10.4975153889\n",
      "mean loss is: 0.403752857589\n",
      "sum loss is: 10.4975742973\n",
      "mean loss is: 0.403755111879\n",
      "sum loss is: 10.4976329088\n",
      "mean loss is: 0.403757354788\n",
      "sum loss is: 10.4976912245\n",
      "mean loss is: 0.403759586356\n",
      "sum loss is: 10.4977492452\n",
      "mean loss is: 0.403761806619\n",
      "sum loss is: 10.4978069721\n",
      "mean loss is: 0.403764015616\n",
      "sum loss is: 10.497864406\n",
      "mean loss is: 0.403766213386\n",
      "sum loss is: 10.497921548\n",
      "mean loss is: 0.403768399967\n",
      "sum loss is: 10.4979783991\n",
      "mean loss is: 0.403770575397\n",
      "sum loss is: 10.4980349603\n",
      "mean loss is: 0.403772739715\n",
      "sum loss is: 10.4980912326\n",
      "mean loss is: 0.40377489296\n",
      "sum loss is: 10.498147217\n",
      "mean loss is: 0.403777035169\n",
      "sum loss is: 10.4982029144\n",
      "mean loss is: 0.403779166383\n",
      "sum loss is: 10.4982583259\n",
      "mean loss is: 0.403781286638\n",
      "sum loss is: 10.4983134526\n",
      "mean loss is: 0.403783395976\n",
      "sum loss is: 10.4983682954\n",
      "mean loss is: 0.403785494432\n",
      "sum loss is: 10.4984228552\n",
      "mean loss is: 0.403787582048\n",
      "sum loss is: 10.4984771333\n",
      "mean loss is: 0.403789658862\n",
      "sum loss is: 10.4985311304\n",
      "mean loss is: 0.403791724912\n",
      "sum loss is: 10.4985848477\n",
      "mean loss is: 0.403793780237\n",
      "sum loss is: 10.4986382862\n",
      "mean loss is: 0.403795824876\n",
      "sum loss is: 10.4986914468\n",
      "mean loss is: 0.403797858869\n",
      "sum loss is: 10.4987443306\n",
      "mean loss is: 0.403799882254\n",
      "sum loss is: 10.4987969386\n",
      "mean loss is: 0.40380189507\n",
      "sum loss is: 10.4988492718\n",
      "mean loss is: 0.403803897356\n",
      "sum loss is: 10.4989013313\n",
      "mean loss is: 0.403805889151\n",
      "sum loss is: 10.4989531179\n",
      "mean loss is: 0.403807870495\n",
      "sum loss is: 10.4990046329\n",
      "mean loss is: 0.403809841425\n",
      "sum loss is: 10.499055877\n",
      "mean loss is: 0.403811801981\n",
      "sum loss is: 10.4991068515\n",
      "mean loss is: 0.403813752203\n",
      "sum loss is: 10.4991575573\n",
      "mean loss is: 0.403815692128\n",
      "sum loss is: 10.4992079953\n",
      "mean loss is: 0.403817621796\n",
      "sum loss is: 10.4992581667\n",
      "mean loss is: 0.403819541246\n",
      "sum loss is: 10.4993080724\n",
      "mean loss is: 0.403821450517\n",
      "sum loss is: 10.4993577134\n",
      "mean loss is: 0.403823349648\n",
      "sum loss is: 10.4994070908\n",
      "mean loss is: 0.403825238678\n",
      "sum loss is: 10.4994562056\n",
      "mean loss is: 0.403827117645\n",
      "sum loss is: 10.4995050588\n",
      "mean loss is: 0.403828986588\n",
      "sum loss is: 10.4995536513\n",
      "mean loss is: 0.403830845547\n",
      "sum loss is: 10.4996019842\n",
      "mean loss is: 0.40383269456\n",
      "sum loss is: 10.4996500586\n",
      "mean loss is: 0.403834533667\n",
      "sum loss is: 10.4996978753\n",
      "mean loss is: 0.403836362905\n",
      "sum loss is: 10.4997454355\n",
      "mean loss is: 0.403838182313\n",
      "sum loss is: 10.4997927401\n",
      "mean loss is: 0.403839991931\n",
      "sum loss is: 10.4998397902\n",
      "mean loss is: 0.403841791796\n",
      "sum loss is: 10.4998865867\n",
      "mean loss is: 0.403843581948\n",
      "sum loss is: 10.4999331307\n",
      "mean loss is: 0.403845362426\n",
      "sum loss is: 10.4999794231\n",
      "mean loss is: 0.403847133267\n",
      "sum loss is: 10.5000254649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss is: 0.40384889451\n",
      "sum loss is: 10.5000712573\n",
      "mean loss is: 0.403850646194\n",
      "sum loss is: 10.5001168011\n",
      "mean loss is: 0.403852388358\n",
      "sum loss is: 10.5001620973\n",
      "mean loss is: 0.403854121039\n",
      "sum loss is: 10.500207147\n",
      "mean loss is: 0.403855844275\n",
      "sum loss is: 10.5002519512\n",
      "mean loss is: 0.403857558106\n",
      "sum loss is: 10.5002965108\n",
      "mean loss is: 0.40385926257\n",
      "sum loss is: 10.5003408268\n",
      "mean loss is: 0.403860957704\n",
      "sum loss is: 10.5003849003\n",
      "mean loss is: 0.403862643547\n",
      "sum loss is: 10.5004287322\n",
      "mean loss is: 0.403864320137\n",
      "sum loss is: 10.5004723236\n",
      "mean loss is: 0.403865987512\n",
      "sum loss is: 10.5005156753\n",
      "mean loss is: 0.40386764571\n",
      "sum loss is: 10.5005587884\n",
      "mean loss is: 0.403869294768\n",
      "sum loss is: 10.500601664\n",
      "mean loss is: 0.403870934725\n",
      "sum loss is: 10.5006443029\n",
      "mean loss is: 0.403872565619\n",
      "sum loss is: 10.5006867061\n",
      "mean loss is: 0.403874187486\n",
      "sum loss is: 10.5007288746\n",
      "mean loss is: 0.403875800366\n",
      "sum loss is: 10.5007708095\n",
      "mean loss is: 0.403877404295\n",
      "sum loss is: 10.5008125117\n",
      "mean loss is: 0.403878999311\n",
      "sum loss is: 10.5008539821\n",
      "mean loss is: 0.403880585451\n",
      "sum loss is: 10.5008952217\n",
      "mean loss is: 0.403882162753\n",
      "sum loss is: 10.5009362316\n",
      "mean loss is: 0.403883731254\n",
      "sum loss is: 10.5009770126\n",
      "mean loss is: 0.403885290991\n",
      "sum loss is: 10.5010175658\n",
      "mean loss is: 0.403886842002\n",
      "sum loss is: 10.501057892\n",
      "mean loss is: 0.403888384323\n",
      "sum loss is: 10.5010979924\n",
      "mean loss is: 0.403889917992\n",
      "sum loss is: 10.5011378678\n",
      "mean loss is: 0.403891443045\n",
      "sum loss is: 10.5011775192\n",
      "mean loss is: 0.40389295952\n",
      "sum loss is: 10.5012169475\n",
      "mean loss is: 0.403894467453\n",
      "sum loss is: 10.5012561538\n",
      "mean loss is: 0.403895966881\n",
      "sum loss is: 10.5012951389\n",
      "mean loss is: 0.403897457841\n",
      "sum loss is: 10.5013339039\n",
      "mean loss is: 0.403898940368\n",
      "sum loss is: 10.5013724496\n",
      "mean loss is: 0.4039004145\n",
      "sum loss is: 10.501410777\n",
      "mean loss is: 0.403901880273\n",
      "sum loss is: 10.5014488871\n",
      "mean loss is: 0.403903337723\n",
      "sum loss is: 10.5014867808\n",
      "mean loss is: 0.403904786887\n",
      "sum loss is: 10.501524459\n",
      "mean loss is: 0.4039062278\n",
      "sum loss is: 10.5015619228\n",
      "mean loss is: 0.403907660498\n",
      "sum loss is: 10.501599173\n",
      "mean loss is: 0.403909085018\n",
      "sum loss is: 10.5016362105\n",
      "mean loss is: 0.403910501396\n",
      "sum loss is: 10.5016730363\n",
      "mean loss is: 0.403911909667\n",
      "sum loss is: 10.5017096513\n",
      "mean loss is: 0.403913309866\n",
      "sum loss is: 10.5017460565\n",
      "mean loss is: 0.40391470203\n",
      "sum loss is: 10.5017822528\n",
      "mean loss is: 0.403916086194\n",
      "sum loss is: 10.501818241\n",
      "mean loss is: 0.403917462393\n",
      "sum loss is: 10.5018540222\n",
      "mean loss is: 0.403918830664\n",
      "sum loss is: 10.5018895973\n",
      "mean loss is: 0.40392019104\n",
      "sum loss is: 10.501924967\n",
      "mean loss is: 0.403921543557\n",
      "sum loss is: 10.5019601325\n",
      "mean loss is: 0.40392288825\n",
      "sum loss is: 10.5019950945\n",
      "mean loss is: 0.403924225154\n",
      "sum loss is: 10.502029854\n",
      "mean loss is: 0.403925554304\n",
      "sum loss is: 10.5020644119\n",
      "mean loss is: 0.403926875735\n",
      "sum loss is: 10.5020987691\n",
      "mean loss is: 0.403928189481\n",
      "sum loss is: 10.5021329265\n",
      "mean loss is: 0.403929495577\n",
      "sum loss is: 10.502166885\n",
      "mean loss is: 0.403930794057\n",
      "sum loss is: 10.5022006455\n",
      "mean loss is: 0.403932084956\n",
      "sum loss is: 10.5022342089\n",
      "mean loss is: 0.403933368308\n",
      "sum loss is: 10.502267576\n",
      "mean loss is: 0.403934644148\n",
      "sum loss is: 10.5023007478\n",
      "mean loss is: 0.403935912508\n",
      "sum loss is: 10.5023337252\n",
      "mean loss is: 0.403937173424\n",
      "sum loss is: 10.502366509\n",
      "mean loss is: 0.403938426928\n",
      "sum loss is: 10.5023991001\n",
      "mean loss is: 0.403939673056\n",
      "sum loss is: 10.5024314995\n",
      "mean loss is: 0.40394091184\n",
      "sum loss is: 10.5024637078\n",
      "mean loss is: 0.403942143314\n",
      "sum loss is: 10.5024957262\n",
      "mean loss is: 0.403943367512\n",
      "sum loss is: 10.5025275553\n",
      "mean loss is: 0.403944584467\n",
      "sum loss is: 10.5025591961\n",
      "mean loss is: 0.403945794212\n",
      "sum loss is: 10.5025906495\n",
      "mean loss is: 0.40394699678\n",
      "sum loss is: 10.5026219163\n",
      "mean loss is: 0.403948192205\n",
      "sum loss is: 10.5026529973\n",
      "mean loss is: 0.403949380519\n",
      "sum loss is: 10.5026838935\n",
      "mean loss is: 0.403950561755\n",
      "sum loss is: 10.5027146056\n",
      "mean loss is: 0.403951735946\n",
      "sum loss is: 10.5027451346\n",
      "mean loss is: 0.403952903125\n",
      "sum loss is: 10.5027754812\n",
      "mean loss is: 0.403954063324\n",
      "sum loss is: 10.5028056464\n",
      "mean loss is: 0.403955216575\n",
      "sum loss is: 10.502835631\n",
      "mean loss is: 0.403956362912\n",
      "sum loss is: 10.5028654357\n",
      "mean loss is: 0.403957502365\n",
      "sum loss is: 10.5028950615\n",
      "mean loss is: 0.403958634968\n",
      "sum loss is: 10.5029245092\n",
      "mean loss is: 0.403959760752\n",
      "sum loss is: 10.5029537795\n",
      "mean loss is: 0.403960879749\n",
      "sum loss is: 10.5029828735\n",
      "mean loss is: 0.403961991991\n",
      "sum loss is: 10.5030117918\n",
      "mean loss is: 0.403963097509\n",
      "sum loss is: 10.5030405352\n",
      "mean loss is: 0.403964196336\n",
      "sum loss is: 10.5030691047\n",
      "mean loss is: 0.403965288503\n",
      "sum loss is: 10.5030975011\n",
      "mean loss is: 0.40396637404\n",
      "sum loss is: 10.503125725\n",
      "mean loss is: 0.40396745298\n",
      "sum loss is: 10.5031537775\n",
      "mean loss is: 0.403968525354\n",
      "sum loss is: 10.5031816592\n",
      "mean loss is: 0.403969591192\n",
      "sum loss is: 10.503209371\n",
      "mean loss is: 0.403970650525\n",
      "sum loss is: 10.5032369136\n",
      "mean loss is: 0.403971703385\n",
      "sum loss is: 10.503264288\n",
      "mean loss is: 0.403972749801\n",
      "sum loss is: 10.5032914948\n",
      "mean loss is: 0.403973789806\n",
      "sum loss is: 10.503318535\n",
      "mean loss is: 0.403974823428\n",
      "sum loss is: 10.5033454091\n",
      "mean loss is: 0.403975850699\n",
      "sum loss is: 10.5033721182\n",
      "mean loss is: 0.403976871649\n",
      "sum loss is: 10.5033986629\n",
      "mean loss is: 0.403977886308\n",
      "sum loss is: 10.503425044\n",
      "mean loss is: 0.403978894707\n",
      "sum loss is: 10.5034512624\n",
      "mean loss is: 0.403979896874\n",
      "sum loss is: 10.5034773187\n",
      "mean loss is: 0.40398089284\n",
      "sum loss is: 10.5035032138\n",
      "mean loss is: 0.403981882635\n",
      "sum loss is: 10.5035289485\n",
      "mean loss is: 0.403982866288\n",
      "sum loss is: 10.5035545235\n",
      "mean loss is: 0.403983843829\n",
      "sum loss is: 10.5035799396\n",
      "mean loss is: 0.403984815288\n",
      "sum loss is: 10.5036051975\n",
      "mean loss is: 0.403985780693\n",
      "sum loss is: 10.503630298\n",
      "mean loss is: 0.403986740073\n",
      "sum loss is: 10.5036552419\n",
      "mean loss is: 0.403987693459\n",
      "sum loss is: 10.5036800299\n",
      "mean loss is: 0.403988640879\n",
      "sum loss is: 10.5037046629\n",
      "mean loss is: 0.403989582362\n",
      "sum loss is: 10.5037291414\n",
      "mean loss is: 0.403990517936\n",
      "sum loss is: 10.5037534663\n",
      "mean loss is: 0.40399144763\n",
      "sum loss is: 10.5037776384\n",
      "mean loss is: 0.403992371474\n",
      "sum loss is: 10.5038016583\n",
      "mean loss is: 0.403993289494\n",
      "sum loss is: 10.5038255269\n",
      "mean loss is: 0.403994201721\n",
      "sum loss is: 10.5038492447\n",
      "mean loss is: 0.403995108181\n",
      "sum loss is: 10.5038728127\n",
      "mean loss is: 0.403996008903\n",
      "sum loss is: 10.5038962315\n",
      "mean loss is: 0.403996903915\n",
      "sum loss is: 10.5039195018\n",
      "mean loss is: 0.403997793245\n",
      "sum loss is: 10.5039426244\n",
      "mean loss is: 0.403998676921\n",
      "sum loss is: 10.5039655999\n",
      "mean loss is: 0.40399955497\n",
      "sum loss is: 10.5039884292\n",
      "mean loss is: 0.404000427421\n",
      "sum loss is: 10.5040111129\n",
      "mean loss is: 0.404001294299\n",
      "sum loss is: 10.5040336518\n",
      "mean loss is: 0.404002155634\n",
      "sum loss is: 10.5040560465\n",
      "mean loss is: 0.404003011452\n",
      "sum loss is: 10.5040782978\n",
      "mean loss is: 0.40400386178\n",
      "sum loss is: 10.5041004063\n",
      "mean loss is: 0.404004706646\n",
      "sum loss is: 10.5041223728\n",
      "mean loss is: 0.404005546075\n",
      "sum loss is: 10.504144198\n",
      "mean loss is: 0.404006380096\n",
      "sum loss is: 10.5041658825\n",
      "mean loss is: 0.404007208735\n",
      "sum loss is: 10.5041874271\n",
      "mean loss is: 0.404008032019\n",
      "sum loss is: 10.5042088325\n",
      "mean loss is: 0.404008849973\n",
      "sum loss is: 10.5042300993\n",
      "mean loss is: 0.404009662625\n",
      "sum loss is: 10.5042512283\n",
      "mean loss is: 0.404010470001\n",
      "sum loss is: 10.50427222\n",
      "mean loss is: 0.404011272126\n",
      "sum loss is: 10.5042930753\n",
      "mean loss is: 0.404012069028\n",
      "sum loss is: 10.5043137947\n",
      "mean loss is: 0.404012860732\n",
      "sum loss is: 10.504334379\n",
      "mean loss is: 0.404013647263\n",
      "sum loss is: 10.5043548288\n",
      "mean loss is: 0.404014428648\n",
      "sum loss is: 10.5043751449\n",
      "mean loss is: 0.404015204913\n",
      "sum loss is: 10.5043953277\n",
      "mean loss is: 0.404015976083\n",
      "sum loss is: 10.5044153782\n",
      "mean loss is: 0.404016742183\n",
      "sum loss is: 10.5044352968\n",
      "mean loss is: 0.404017503239\n",
      "sum loss is: 10.5044550842\n",
      "mean loss is: 0.404018259275\n",
      "sum loss is: 10.5044747412\n",
      "mean loss is: 0.404019010318\n",
      "sum loss is: 10.5044942683\n",
      "mean loss is: 0.404019756392\n",
      "sum loss is: 10.5045136662\n",
      "mean loss is: 0.404020497523\n",
      "sum loss is: 10.5045329356\n",
      "mean loss is: 0.404021233734\n",
      "sum loss is: 10.5045520771\n",
      "mean loss is: 0.404021965051\n",
      "sum loss is: 10.5045710913\n",
      "mean loss is: 0.404022691499\n",
      "sum loss is: 10.504589979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss is: 0.404023413101\n",
      "sum loss is: 10.5046087406\n",
      "mean loss is: 0.404024129883\n",
      "sum loss is: 10.5046273769\n",
      "mean loss is: 0.404024841868\n",
      "sum loss is: 10.5046458886\n",
      "mean loss is: 0.404025549081\n",
      "sum loss is: 10.5046642761\n",
      "mean loss is: 0.404026251546\n",
      "sum loss is: 10.5046825402\n",
      "mean loss is: 0.404026949287\n",
      "sum loss is: 10.5047006815\n",
      "mean loss is: 0.404027642328\n",
      "sum loss is: 10.5047187005\n",
      "mean loss is: 0.404028330693\n",
      "sum loss is: 10.504736598\n",
      "mean loss is: 0.404029014405\n",
      "sum loss is: 10.5047543745\n",
      "mean loss is: 0.404029693488\n",
      "sum loss is: 10.5047720307\n",
      "mean loss is: 0.404030367966\n",
      "sum loss is: 10.5047895671\n",
      "mean loss is: 0.404031037862\n",
      "sum loss is: 10.5048069844\n",
      "mean loss is: 0.404031703199\n",
      "sum loss is: 10.5048242832\n",
      "mean loss is: 0.404032364\n",
      "sum loss is: 10.504841464\n",
      "mean loss is: 0.404033020289\n",
      "sum loss is: 10.5048585275\n",
      "mean loss is: 0.404033672089\n",
      "sum loss is: 10.5048754743\n",
      "mean loss is: 0.404034319422\n",
      "sum loss is: 10.504892305\n",
      "mean loss is: 0.404034962311\n",
      "sum loss is: 10.5049090201\n",
      "mean loss is: 0.40403560078\n",
      "sum loss is: 10.5049256203\n",
      "mean loss is: 0.40403623485\n",
      "sum loss is: 10.5049421061\n",
      "mean loss is: 0.404036864544\n",
      "sum loss is: 10.5049584781\n",
      "mean loss is: 0.404037489885\n",
      "sum loss is: 10.504974737\n",
      "mean loss is: 0.404038110894\n",
      "sum loss is: 10.5049908832\n",
      "mean loss is: 0.404038727595\n",
      "sum loss is: 10.5050069175\n",
      "mean loss is: 0.404039340009\n",
      "sum loss is: 10.5050228402\n",
      "mean loss is: 0.404039948158\n",
      "sum loss is: 10.5050386521\n",
      "mean loss is: 0.404040552064\n",
      "sum loss is: 10.5050543537\n",
      "mean loss is: 0.404041151749\n",
      "sum loss is: 10.5050699455\n",
      "mean loss is: 0.404041747236\n",
      "sum loss is: 10.5050854281\n",
      "mean loss is: 0.404042338544\n",
      "sum loss is: 10.5051008021\n",
      "mean loss is: 0.404042925696\n",
      "sum loss is: 10.5051160681\n",
      "mean loss is: 0.404043508714\n",
      "sum loss is: 10.5051312266\n",
      "mean loss is: 0.404044087618\n",
      "sum loss is: 10.5051462781\n",
      "mean loss is: 0.404044662431\n",
      "sum loss is: 10.5051612232\n",
      "mean loss is: 0.404045233172\n",
      "sum loss is: 10.5051760625\n",
      "mean loss is: 0.404045799864\n",
      "sum loss is: 10.5051907965\n",
      "mean loss is: 0.404046362526\n",
      "sum loss is: 10.5052054257\n",
      "mean loss is: 0.404046921181\n",
      "sum loss is: 10.5052199507\n",
      "mean loss is: 0.404047475849\n",
      "sum loss is: 10.5052343721\n",
      "mean loss is: 0.40404802655\n",
      "sum loss is: 10.5052486903\n",
      "mean loss is: 0.404048573306\n",
      "sum loss is: 10.5052629059\n",
      "mean loss is: 0.404049116136\n",
      "sum loss is: 10.5052770195\n",
      "mean loss is: 0.404049655061\n",
      "sum loss is: 10.5052910316\n",
      "mean loss is: 0.404050190101\n",
      "sum loss is: 10.5053049426\n",
      "mean loss is: 0.404050721277\n",
      "sum loss is: 10.5053187532\n",
      "mean loss is: 0.404051248609\n",
      "sum loss is: 10.5053324638\n",
      "mean loss is: 0.404051772116\n",
      "sum loss is: 10.505346075\n",
      "mean loss is: 0.40405229182\n",
      "sum loss is: 10.5053595873\n",
      "mean loss is: 0.404052807738\n",
      "sum loss is: 10.5053730012\n",
      "mean loss is: 0.404053319893\n",
      "sum loss is: 10.5053863172\n",
      "mean loss is: 0.404053828302\n",
      "sum loss is: 10.5053995359\n",
      "mean loss is: 0.404054332986\n",
      "sum loss is: 10.5054126576\n",
      "mean loss is: 0.404054833964\n",
      "sum loss is: 10.5054256831\n",
      "mean loss is: 0.404055331256\n",
      "sum loss is: 10.5054386127\n",
      "mean loss is: 0.40405582488\n",
      "sum loss is: 10.5054514469\n",
      "mean loss is: 0.404056314857\n",
      "sum loss is: 10.5054641863\n",
      "mean loss is: 0.404056801205\n",
      "sum loss is: 10.5054768313\n",
      "mean loss is: 0.404057283944\n",
      "sum loss is: 10.5054893825\n",
      "mean loss is: 0.404057763091\n",
      "sum loss is: 10.5055018404\n",
      "mean loss is: 0.404058238667\n",
      "sum loss is: 10.5055142054\n",
      "mean loss is: 0.40405871069\n",
      "sum loss is: 10.5055264779\n",
      "mean loss is: 0.404059179179\n",
      "sum loss is: 10.5055386587\n",
      "mean loss is: 0.404059644152\n",
      "sum loss is: 10.505550748\n",
      "mean loss is: 0.404060105628\n",
      "sum loss is: 10.5055627463\n",
      "mean loss is: 0.404060563625\n",
      "sum loss is: 10.5055746543\n",
      "mean loss is: 0.404061018162\n",
      "sum loss is: 10.5055864722\n",
      "mean loss is: 0.404061469257\n",
      "sum loss is: 10.5055982007\n",
      "mean loss is: 0.404061916928\n",
      "sum loss is: 10.5056098401\n",
      "mean loss is: 0.404062361194\n",
      "sum loss is: 10.505621391\n",
      "mean loss is: 0.404062802072\n",
      "sum loss is: 10.5056328539\n",
      "mean loss is: 0.40406323958\n",
      "sum loss is: 10.5056442291\n",
      "mean loss is: 0.404063673736\n",
      "sum loss is: 10.5056555171\n",
      "mean loss is: 0.404064104558\n",
      "sum loss is: 10.5056667185\n",
      "mean loss is: 0.404064532063\n",
      "sum loss is: 10.5056778337\n",
      "mean loss is: 0.40406495627\n",
      "sum loss is: 10.505688863\n",
      "mean loss is: 0.404065377196\n",
      "sum loss is: 10.5056998071\n",
      "mean loss is: 0.404065794858\n",
      "sum loss is: 10.5057106663\n",
      "mean loss is: 0.404066209273\n",
      "sum loss is: 10.5057214411\n",
      "mean loss is: 0.404066620459\n",
      "sum loss is: 10.5057321319\n",
      "mean loss is: 0.404067028433\n",
      "sum loss is: 10.5057427393\n",
      "mean loss is: 0.404067433212\n",
      "sum loss is: 10.5057532635\n",
      "mean loss is: 0.404067834814\n",
      "sum loss is: 10.5057637052\n",
      "mean loss is: 0.404068233254\n",
      "sum loss is: 10.5057740646\n",
      "mean loss is: 0.404068628551\n",
      "sum loss is: 10.5057843423\n",
      "mean loss is: 0.40406902072\n",
      "sum loss is: 10.5057945387\n",
      "mean loss is: 0.404069409779\n",
      "sum loss is: 10.5058046543\n",
      "mean loss is: 0.404069795744\n",
      "sum loss is: 10.5058146893\n",
      "mean loss is: 0.404070178631\n",
      "sum loss is: 10.5058246444\n",
      "mean loss is: 0.404070558458\n",
      "sum loss is: 10.5058345199\n",
      "mean loss is: 0.404070935241\n",
      "sum loss is: 10.5058443163\n",
      "mean loss is: 0.404071308995\n",
      "sum loss is: 10.5058540339\n",
      "mean loss is: 0.404071679737\n",
      "sum loss is: 10.5058636732\n",
      "mean loss is: 0.404072047484\n",
      "sum loss is: 10.5058732346\n",
      "mean loss is: 0.404072412251\n",
      "sum loss is: 10.5058827185\n",
      "mean loss is: 0.404072774054\n",
      "sum loss is: 10.5058921254\n",
      "mean loss is: 0.40407313291\n",
      "sum loss is: 10.5059014557\n",
      "mean loss is: 0.404073488834\n",
      "sum loss is: 10.5059107097\n",
      "mean loss is: 0.404073841842\n",
      "sum loss is: 10.5059198879\n",
      "mean loss is: 0.40407419195\n",
      "sum loss is: 10.5059289907\n",
      "mean loss is: 0.404074539173\n",
      "sum loss is: 10.5059380185\n",
      "mean loss is: 0.404074883526\n",
      "sum loss is: 10.5059469717\n",
      "mean loss is: 0.404075225027\n",
      "sum loss is: 10.5059558507\n",
      "mean loss is: 0.404075563688\n",
      "sum loss is: 10.5059646559\n",
      "mean loss is: 0.404075899527\n",
      "sum loss is: 10.5059733877\n",
      "mean loss is: 0.404076232559\n",
      "sum loss is: 10.5059820465\n",
      "mean loss is: 0.404076562797\n",
      "sum loss is: 10.5059906327\n",
      "mean loss is: 0.404076890259\n",
      "sum loss is: 10.5059991467\n",
      "mean loss is: 0.404077214958\n",
      "sum loss is: 10.5060075889\n",
      "mean loss is: 0.40407753691\n",
      "sum loss is: 10.5060159596\n",
      "mean loss is: 0.404077856129\n",
      "sum loss is: 10.5060242593\n",
      "mean loss is: 0.40407817263\n",
      "sum loss is: 10.5060324884\n",
      "mean loss is: 0.404078486429\n",
      "sum loss is: 10.5060406472\n",
      "mean loss is: 0.404078797539\n",
      "sum loss is: 10.506048736\n",
      "mean loss is: 0.404079105976\n",
      "sum loss is: 10.5060567554\n",
      "mean loss is: 0.404079411754\n",
      "sum loss is: 10.5060647056\n",
      "mean loss is: 0.404079714887\n",
      "sum loss is: 10.506072587\n",
      "mean loss is: 0.404080015389\n",
      "sum loss is: 10.5060804001\n",
      "mean loss is: 0.404080313276\n",
      "sum loss is: 10.5060881452\n",
      "mean loss is: 0.404080608561\n",
      "sum loss is: 10.5060958226\n",
      "mean loss is: 0.404080901259\n",
      "sum loss is: 10.5061034327\n",
      "mean loss is: 0.404081191383\n",
      "sum loss is: 10.506110976\n",
      "mean loss is: 0.404081478948\n",
      "sum loss is: 10.5061184526\n",
      "mean loss is: 0.404081763967\n",
      "sum loss is: 10.5061258631\n",
      "mean loss is: 0.404082046455\n",
      "sum loss is: 10.5061332078\n",
      "mean loss is: 0.404082326425\n",
      "sum loss is: 10.5061404871\n",
      "mean loss is: 0.404082603892\n",
      "sum loss is: 10.5061477012\n",
      "mean loss is: 0.404082878868\n",
      "sum loss is: 10.5061548506\n",
      "mean loss is: 0.404083151368\n",
      "sum loss is: 10.5061619356\n",
      "mean loss is: 0.404083421405\n",
      "sum loss is: 10.5061689565\n",
      "mean loss is: 0.404083688992\n",
      "sum loss is: 10.5061759138\n",
      "mean loss is: 0.404083954143\n",
      "sum loss is: 10.5061828077\n",
      "mean loss is: 0.404084216872\n",
      "sum loss is: 10.5061896387\n",
      "mean loss is: 0.404084477191\n",
      "sum loss is: 10.506196407\n",
      "mean loss is: 0.404084735114\n",
      "sum loss is: 10.506203113\n",
      "mean loss is: 0.404084990654\n",
      "sum loss is: 10.506209757\n",
      "mean loss is: 0.404085243824\n",
      "sum loss is: 10.5062163394\n",
      "mean loss is: 0.404085494638\n",
      "sum loss is: 10.5062228606\n",
      "mean loss is: 0.404085743108\n",
      "sum loss is: 10.5062293208\n",
      "mean loss is: 0.404085989246\n",
      "sum loss is: 10.5062357204\n",
      "mean loss is: 0.404086233067\n",
      "sum loss is: 10.5062420597\n",
      "mean loss is: 0.404086474582\n",
      "sum loss is: 10.5062483391\n",
      "mean loss is: 0.404086713805\n",
      "sum loss is: 10.5062545589\n",
      "mean loss is: 0.404086950748\n",
      "sum loss is: 10.5062607194\n",
      "mean loss is: 0.404087185423\n",
      "sum loss is: 10.506266821\n",
      "mean loss is: 0.404087417844\n",
      "sum loss is: 10.5062728639\n",
      "mean loss is: 0.404087648022\n",
      "sum loss is: 10.5062788486\n",
      "mean loss is: 0.404087875971\n",
      "sum loss is: 10.5062847752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss is: 0.404088101702\n",
      "sum loss is: 10.5062906442\n",
      "mean loss is: 0.404088325227\n",
      "sum loss is: 10.5062964559\n",
      "mean loss is: 0.40408854656\n",
      "sum loss is: 10.5063022106\n",
      "mean loss is: 0.404088765711\n",
      "sum loss is: 10.5063079085\n",
      "mean loss is: 0.404088982694\n",
      "sum loss is: 10.50631355\n",
      "mean loss is: 0.40408919752\n",
      "sum loss is: 10.5063191355\n",
      "mean loss is: 0.404089410201\n",
      "sum loss is: 10.5063246652\n",
      "mean loss is: 0.40408962075\n",
      "sum loss is: 10.5063301395\n",
      "mean loss is: 0.404089829177\n",
      "sum loss is: 10.5063355586\n",
      "mean loss is: 0.404090035495\n",
      "sum loss is: 10.5063409229\n",
      "mean loss is: 0.404090239716\n",
      "sum loss is: 10.5063462326\n",
      "mean loss is: 0.404090441851\n",
      "sum loss is: 10.5063514881\n",
      "mean loss is: 0.404090641912\n",
      "sum loss is: 10.5063566897\n",
      "mean loss is: 0.40409083991\n",
      "sum loss is: 10.5063618377\n",
      "mean loss is: 0.404091035857\n",
      "sum loss is: 10.5063669323\n",
      "mean loss is: 0.404091229764\n",
      "sum loss is: 10.5063719739\n",
      "mean loss is: 0.404091421643\n",
      "sum loss is: 10.5063769627\n",
      "mean loss is: 0.404091611506\n",
      "sum loss is: 10.5063818991\n",
      "mean loss is: 0.404091799362\n",
      "sum loss is: 10.5063867834\n",
      "mean loss is: 0.404091985225\n",
      "sum loss is: 10.5063916158\n",
      "mean loss is: 0.404092169104\n",
      "sum loss is: 10.5063963967\n",
      "mean loss is: 0.40409235101\n",
      "sum loss is: 10.5064011263\n",
      "mean loss is: 0.404092530956\n",
      "sum loss is: 10.5064058049\n",
      "mean loss is: 0.404092708952\n",
      "sum loss is: 10.5064104327\n",
      "mean loss is: 0.404092885008\n",
      "sum loss is: 10.5064150102\n",
      "mean loss is: 0.404093059137\n",
      "sum loss is: 10.5064195376\n",
      "mean loss is: 0.404093231347\n",
      "sum loss is: 10.506424015\n",
      "mean loss is: 0.404093401652\n",
      "sum loss is: 10.5064284429\n",
      "mean loss is: 0.40409357006\n",
      "sum loss is: 10.5064328216\n",
      "mean loss is: 0.404093736583\n",
      "sum loss is: 10.5064371512\n",
      "mean loss is: 0.404093901231\n",
      "sum loss is: 10.506441432\n",
      "mean loss is: 0.404094064015\n",
      "sum loss is: 10.5064456644\n",
      "mean loss is: 0.404094224946\n",
      "sum loss is: 10.5064498486\n",
      "mean loss is: 0.404094384033\n",
      "sum loss is: 10.5064539849\n",
      "mean loss is: 0.404094541288\n",
      "sum loss is: 10.5064580735\n",
      "mean loss is: 0.40409469672\n",
      "sum loss is: 10.5064621147\n",
      "mean loss is: 0.404094850341\n",
      "sum loss is: 10.5064661089\n",
      "mean loss is: 0.404095002159\n",
      "sum loss is: 10.5064700561\n",
      "mean loss is: 0.404095152186\n",
      "sum loss is: 10.5064739568\n",
      "mean loss is: 0.404095300431\n",
      "sum loss is: 10.5064778112\n",
      "mean loss is: 0.404095446905\n",
      "sum loss is: 10.5064816195\n",
      "mean loss is: 0.404095591617\n",
      "sum loss is: 10.506485382\n",
      "mean loss is: 0.404095734578\n",
      "sum loss is: 10.506489099\n",
      "mean loss is: 0.404095875797\n",
      "sum loss is: 10.5064927707\n",
      "mean loss is: 0.404096015285\n",
      "sum loss is: 10.5064963974\n",
      "mean loss is: 0.404096153051\n",
      "sum loss is: 10.5064999793\n",
      "mean loss is: 0.404096289105\n",
      "sum loss is: 10.5065035167\n",
      "mean loss is: 0.404096423457\n",
      "sum loss is: 10.5065070099\n",
      "mean loss is: 0.404096556116\n",
      "sum loss is: 10.506510459\n",
      "mean loss is: 0.404096687092\n",
      "sum loss is: 10.5065138644\n",
      "mean loss is: 0.404096816395\n",
      "sum loss is: 10.5065172263\n",
      "mean loss is: 0.404096944034\n",
      "sum loss is: 10.5065205449\n",
      "mean loss is: 0.404097070019\n",
      "sum loss is: 10.5065238205\n",
      "mean loss is: 0.404097194358\n",
      "sum loss is: 10.5065270533\n",
      "mean loss is: 0.404097317063\n",
      "sum loss is: 10.5065302436\n",
      "mean loss is: 0.404097438141\n",
      "sum loss is: 10.5065333917\n",
      "mean loss is: 0.404097557602\n",
      "sum loss is: 10.5065364977\n",
      "mean loss is: 0.404097675455\n",
      "sum loss is: 10.5065395618\n",
      "mean loss is: 0.404097791711\n",
      "sum loss is: 10.5065425845\n",
      "mean loss is: 0.404097906376\n",
      "sum loss is: 10.5065455658\n",
      "mean loss is: 0.404098019462\n",
      "sum loss is: 10.506548506\n",
      "mean loss is: 0.404098130976\n",
      "sum loss is: 10.5065514054\n",
      "mean loss is: 0.404098240929\n",
      "sum loss is: 10.5065542641\n",
      "mean loss is: 0.404098349328\n",
      "sum loss is: 10.5065570825\n",
      "mean loss is: 0.404098456183\n",
      "sum loss is: 10.5065598608\n",
      "mean loss is: 0.404098561502\n",
      "sum loss is: 10.5065625991\n",
      "mean loss is: 0.404098665295\n",
      "sum loss is: 10.5065652977\n",
      "mean loss is: 0.404098767571\n",
      "sum loss is: 10.5065679568\n",
      "mean loss is: 0.404098868337\n",
      "sum loss is: 10.5065705768\n",
      "mean loss is: 0.404098967603\n",
      "sum loss is: 10.5065731577\n",
      "mean loss is: 0.404099065377\n",
      "sum loss is: 10.5065756998\n",
      "mean loss is: 0.404099161668\n",
      "sum loss is: 10.5065782034\n",
      "mean loss is: 0.404099256485\n",
      "sum loss is: 10.5065806686\n",
      "mean loss is: 0.404099349835\n",
      "sum loss is: 10.5065830957\n",
      "mean loss is: 0.404099441728\n",
      "sum loss is: 10.5065854849\n",
      "mean loss is: 0.404099532172\n",
      "sum loss is: 10.5065878365\n",
      "mean loss is: 0.404099621175\n",
      "sum loss is: 10.5065901506\n",
      "mean loss is: 0.404099708746\n",
      "sum loss is: 10.5065924274\n",
      "mean loss is: 0.404099794892\n",
      "sum loss is: 10.5065946672\n",
      "mean loss is: 0.404099879623\n",
      "sum loss is: 10.5065968702\n",
      "mean loss is: 0.404099962946\n",
      "sum loss is: 10.5065990366\n",
      "mean loss is: 0.40410004487\n",
      "sum loss is: 10.5066011666\n",
      "mean loss is: 0.404100125402\n",
      "sum loss is: 10.5066032605\n",
      "mean loss is: 0.404100204551\n",
      "sum loss is: 10.5066053183\n",
      "mean loss is: 0.404100282325\n",
      "sum loss is: 10.5066073404\n",
      "mean loss is: 0.404100358731\n",
      "sum loss is: 10.506609327\n",
      "mean loss is: 0.404100433779\n",
      "sum loss is: 10.5066112782\n",
      "mean loss is: 0.404100507475\n",
      "sum loss is: 10.5066131943\n",
      "mean loss is: 0.404100579827\n",
      "sum loss is: 10.5066150755\n",
      "mean loss is: 0.404100650844\n",
      "sum loss is: 10.5066169219\n",
      "mean loss is: 0.404100720533\n",
      "sum loss is: 10.5066187339\n",
      "mean loss is: 0.404100788902\n",
      "sum loss is: 10.5066205114\n",
      "mean loss is: 0.404100855958\n",
      "sum loss is: 10.5066222549\n",
      "mean loss is: 0.40410092171\n",
      "sum loss is: 10.5066239645\n",
      "mean loss is: 0.404100986165\n",
      "sum loss is: 10.5066256403\n",
      "mean loss is: 0.40410104933\n",
      "sum loss is: 10.5066272826\n",
      "mean loss is: 0.404101111213\n",
      "sum loss is: 10.5066288915\n",
      "mean loss is: 0.404101171822\n",
      "sum loss is: 10.5066304674\n",
      "mean loss is: 0.404101231164\n",
      "sum loss is: 10.5066320103\n",
      "mean loss is: 0.404101289246\n",
      "sum loss is: 10.5066335204\n",
      "mean loss is: 0.404101346077\n",
      "sum loss is: 10.506634998\n",
      "mean loss is: 0.404101401662\n",
      "sum loss is: 10.5066364432\n",
      "mean loss is: 0.404101456011\n",
      "sum loss is: 10.5066378563\n",
      "mean loss is: 0.404101509129\n",
      "sum loss is: 10.5066392373\n",
      "mean loss is: 0.404101561024\n",
      "sum loss is: 10.5066405866\n",
      "mean loss is: 0.404101611703\n",
      "sum loss is: 10.5066419043\n",
      "mean loss is: 0.404101661174\n",
      "sum loss is: 10.5066431905\n",
      "mean loss is: 0.404101709444\n",
      "sum loss is: 10.5066444455\n",
      "mean loss is: 0.40410175652\n",
      "sum loss is: 10.5066456695\n",
      "mean loss is: 0.404101802408\n",
      "sum loss is: 10.5066468626\n",
      "mean loss is: 0.404101847116\n",
      "sum loss is: 10.506648025\n",
      "mean loss is: 0.40410189065\n",
      "sum loss is: 10.5066491569\n",
      "mean loss is: 0.404101933019\n",
      "sum loss is: 10.5066502585\n",
      "mean loss is: 0.404101974228\n",
      "sum loss is: 10.5066513299\n",
      "mean loss is: 0.404102014285\n",
      "sum loss is: 10.5066523714\n",
      "mean loss is: 0.404102053196\n",
      "sum loss is: 10.5066533831\n",
      "mean loss is: 0.404102090968\n",
      "sum loss is: 10.5066543652\n",
      "mean loss is: 0.404102127608\n",
      "sum loss is: 10.5066553178\n",
      "mean loss is: 0.404102163123\n",
      "sum loss is: 10.5066562412\n",
      "mean loss is: 0.404102197519\n",
      "sum loss is: 10.5066571355\n",
      "mean loss is: 0.404102230803\n",
      "sum loss is: 10.5066580009\n",
      "mean loss is: 0.404102262982\n",
      "sum loss is: 10.5066588375\n",
      "mean loss is: 0.404102294063\n",
      "sum loss is: 10.5066596456\n",
      "mean loss is: 0.404102324051\n",
      "sum loss is: 10.5066604253\n",
      "mean loss is: 0.404102352953\n",
      "sum loss is: 10.5066611768\n",
      "mean loss is: 0.404102380776\n",
      "sum loss is: 10.5066619002\n",
      "mean loss is: 0.404102407527\n",
      "sum loss is: 10.5066625957\n",
      "mean loss is: 0.404102433211\n",
      "sum loss is: 10.5066632635\n",
      "mean loss is: 0.404102457835\n",
      "sum loss is: 10.5066639037\n",
      "mean loss is: 0.404102481406\n",
      "sum loss is: 10.5066645166\n",
      "mean loss is: 0.404102503929\n",
      "sum loss is: 10.5066651022\n",
      "mean loss is: 0.404102525412\n",
      "sum loss is: 10.5066656607\n",
      "mean loss is: 0.40410254586\n",
      "sum loss is: 10.5066661924\n",
      "mean loss is: 0.40410256528\n",
      "sum loss is: 10.5066666973\n",
      "mean loss is: 0.404102583677\n",
      "sum loss is: 10.5066671756\n",
      "mean loss is: 0.404102601058\n",
      "sum loss is: 10.5066676275\n",
      "mean loss is: 0.404102617429\n",
      "sum loss is: 10.5066680532\n",
      "mean loss is: 0.404102632797\n",
      "sum loss is: 10.5066684527\n",
      "mean loss is: 0.404102647166\n",
      "sum loss is: 10.5066688263\n",
      "mean loss is: 0.404102660544\n",
      "sum loss is: 10.5066691741\n",
      "mean loss is: 0.404102672936\n",
      "sum loss is: 10.5066694963\n",
      "mean loss is: 0.404102684348\n",
      "sum loss is: 10.506669793\n",
      "mean loss is: 0.404102694786\n",
      "sum loss is: 10.5066700644\n",
      "mean loss is: 0.404102704256\n",
      "sum loss is: 10.5066703107\n",
      "mean loss is: 0.404102712765\n",
      "sum loss is: 10.5066705319\n",
      "mean loss is: 0.404102720316\n",
      "sum loss is: 10.5066707282\n",
      "mean loss is: 0.404102726918\n",
      "sum loss is: 10.5066708999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss is: 0.404102732575\n",
      "sum loss is: 10.5066710469\n",
      "mean loss is: 0.404102737293\n",
      "sum loss is: 10.5066711696\n",
      "mean loss is: 0.404102741077\n",
      "sum loss is: 10.506671268\n",
      "mean loss is: 0.404102743934\n",
      "sum loss is: 10.5066713423\n",
      "mean loss is: 0.40410274587\n",
      "sum loss is: 10.5066713926\n",
      "mean loss is: 0.404102746889\n",
      "sum loss is: 10.5066714191\n",
      "mean loss is: 0.404102746997\n",
      "sum loss is: 10.5066714219\n",
      "mean loss is: 0.4041027462\n",
      "sum loss is: 10.5066714012\n",
      "mean loss is: 0.404102744504\n",
      "sum loss is: 10.5066713571\n",
      "mean loss is: 0.404102741914\n",
      "sum loss is: 10.5066712898\n",
      "mean loss is: 0.404102738435\n",
      "sum loss is: 10.5066711993\n",
      "mean loss is: 0.404102734074\n",
      "sum loss is: 10.5066710859\n",
      "mean loss is: 0.404102728834\n",
      "sum loss is: 10.5066709497\n",
      "mean loss is: 0.404102722723\n",
      "sum loss is: 10.5066707908\n",
      "mean loss is: 0.404102715744\n",
      "sum loss is: 10.5066706093\n",
      "mean loss is: 0.404102707904\n",
      "sum loss is: 10.5066704055\n",
      "mean loss is: 0.404102699208\n",
      "sum loss is: 10.5066701794\n",
      "mean loss is: 0.404102689661\n",
      "sum loss is: 10.5066699312\n",
      "mean loss is: 0.404102679268\n",
      "sum loss is: 10.506669661\n",
      "mean loss is: 0.404102668034\n",
      "sum loss is: 10.5066693689\n",
      "mean loss is: 0.404102655966\n",
      "sum loss is: 10.5066690551\n",
      "mean loss is: 0.404102643067\n",
      "sum loss is: 10.5066687197\n",
      "mean loss is: 0.404102629343\n",
      "sum loss is: 10.5066683629\n",
      "mean loss is: 0.4041026148\n",
      "sum loss is: 10.5066679848\n",
      "mean loss is: 0.404102599442\n",
      "sum loss is: 10.5066675855\n",
      "mean loss is: 0.404102583274\n",
      "sum loss is: 10.5066671651\n",
      "mean loss is: 0.404102566301\n",
      "sum loss is: 10.5066667238\n",
      "mean loss is: 0.404102548529\n",
      "sum loss is: 10.5066662618\n",
      "mean loss is: 0.404102529963\n",
      "sum loss is: 10.506665779\n",
      "mean loss is: 0.404102510606\n",
      "sum loss is: 10.5066652758\n",
      "mean loss is: 0.404102490465\n",
      "sum loss is: 10.5066647521\n",
      "mean loss is: 0.404102469544\n",
      "sum loss is: 10.5066642081\n",
      "mean loss is: 0.404102447848\n",
      "sum loss is: 10.506663644\n",
      "mean loss is: 0.404102425382\n",
      "sum loss is: 10.5066630599\n",
      "mean loss is: 0.40410240215\n",
      "sum loss is: 10.5066624559\n",
      "mean loss is: 0.404102378159\n",
      "sum loss is: 10.5066618321\n",
      "mean loss is: 0.404102353411\n",
      "sum loss is: 10.5066611887\n",
      "mean loss is: 0.404102327912\n",
      "sum loss is: 10.5066605257\n",
      "mean loss is: 0.404102301668\n",
      "sum loss is: 10.5066598434\n",
      "mean loss is: 0.404102274681\n",
      "sum loss is: 10.5066591417\n",
      "mean loss is: 0.404102246958\n",
      "sum loss is: 10.5066584209\n",
      "mean loss is: 0.404102218503\n",
      "sum loss is: 10.5066576811\n",
      "mean loss is: 0.40410218932\n",
      "sum loss is: 10.5066569223\n",
      "mean loss is: 0.404102159414\n",
      "sum loss is: 10.5066561448\n",
      "mean loss is: 0.404102128789\n",
      "sum loss is: 10.5066553485\n",
      "mean loss is: 0.404102097451\n",
      "sum loss is: 10.5066545337\n",
      "mean loss is: 0.404102065404\n",
      "sum loss is: 10.5066537005\n",
      "mean loss is: 0.404102032651\n",
      "sum loss is: 10.5066528489\n",
      "mean loss is: 0.404101999199\n",
      "sum loss is: 10.5066519792\n",
      "mean loss is: 0.40410196505\n",
      "sum loss is: 10.5066510913\n",
      "mean loss is: 0.40410193021\n",
      "sum loss is: 10.5066501855\n",
      "mean loss is: 0.404101894683\n",
      "sum loss is: 10.5066492618\n",
      "mean loss is: 0.404101858474\n",
      "sum loss is: 10.5066483203\n",
      "mean loss is: 0.404101821586\n",
      "sum loss is: 10.5066473612\n",
      "mean loss is: 0.404101784024\n",
      "sum loss is: 10.5066463846\n",
      "mean loss is: 0.404101745792\n",
      "sum loss is: 10.5066453906\n",
      "mean loss is: 0.404101706895\n",
      "sum loss is: 10.5066443793\n",
      "mean loss is: 0.404101667337\n",
      "sum loss is: 10.5066433508\n",
      "mean loss is: 0.404101627122\n",
      "sum loss is: 10.5066423052\n",
      "mean loss is: 0.404101586255\n",
      "sum loss is: 10.5066412426\n",
      "mean loss is: 0.404101544739\n",
      "sum loss is: 10.5066401632\n",
      "mean loss is: 0.404101502579\n",
      "sum loss is: 10.5066390671\n",
      "mean loss is: 0.404101459779\n",
      "sum loss is: 10.5066379543\n",
      "mean loss is: 0.404101416343\n",
      "sum loss is: 10.5066368249\n",
      "mean loss is: 0.404101372275\n",
      "sum loss is: 10.5066356792\n",
      "mean loss is: 0.40410132758\n",
      "sum loss is: 10.5066345171\n",
      "mean loss is: 0.404101282261\n",
      "sum loss is: 10.5066333388\n",
      "mean loss is: 0.404101236322\n",
      "sum loss is: 10.5066321444\n",
      "mean loss is: 0.404101189768\n",
      "sum loss is: 10.506630934\n",
      "mean loss is: 0.404101142602\n",
      "sum loss is: 10.5066297076\n",
      "mean loss is: 0.404101094828\n",
      "sum loss is: 10.5066284655\n",
      "mean loss is: 0.404101046451\n",
      "sum loss is: 10.5066272077\n",
      "mean loss is: 0.404100997474\n",
      "sum loss is: 10.5066259343\n",
      "mean loss is: 0.404100947902\n",
      "sum loss is: 10.5066246454\n",
      "mean loss is: 0.404100897738\n",
      "sum loss is: 10.5066233412\n",
      "mean loss is: 0.404100846985\n",
      "sum loss is: 10.5066220216\n",
      "mean loss is: 0.404100795649\n",
      "sum loss is: 10.5066206869\n",
      "mean loss is: 0.404100743732\n",
      "sum loss is: 10.506619337\n",
      "mean loss is: 0.404100691239\n",
      "sum loss is: 10.5066179722\n",
      "mean loss is: 0.404100638173\n",
      "sum loss is: 10.5066165925\n",
      "mean loss is: 0.404100584538\n",
      "sum loss is: 10.506615198\n",
      "mean loss is: 0.404100530338\n",
      "sum loss is: 10.5066137888\n",
      "mean loss is: 0.404100475576\n",
      "sum loss is: 10.506612365\n",
      "mean loss is: 0.404100420257\n",
      "sum loss is: 10.5066109267\n",
      "mean loss is: 0.404100364384\n",
      "sum loss is: 10.506609474\n",
      "mean loss is: 0.404100307961\n",
      "sum loss is: 10.506608007\n",
      "mean loss is: 0.404100250991\n",
      "sum loss is: 10.5066065258\n",
      "mean loss is: 0.404100193478\n",
      "sum loss is: 10.5066050304\n",
      "mean loss is: 0.404100135425\n",
      "sum loss is: 10.5066035211\n",
      "mean loss is: 0.404100076837\n",
      "sum loss is: 10.5066019978\n",
      "mean loss is: 0.404100017716\n",
      "sum loss is: 10.5066004606\n",
      "mean loss is: 0.404099958067\n",
      "sum loss is: 10.5065989097\n",
      "mean loss is: 0.404099897893\n",
      "sum loss is: 10.5065973452\n",
      "mean loss is: 0.404099837197\n",
      "sum loss is: 10.5065957671\n",
      "mean loss is: 0.404099775983\n",
      "sum loss is: 10.5065941756\n",
      "mean loss is: 0.404099714254\n",
      "sum loss is: 10.5065925706\n",
      "mean loss is: 0.404099652014\n",
      "sum loss is: 10.5065909524\n",
      "mean loss is: 0.404099589267\n",
      "sum loss is: 10.5065893209\n",
      "mean loss is: 0.404099526015\n",
      "sum loss is: 10.5065876764\n",
      "mean loss is: 0.404099462262\n",
      "sum loss is: 10.5065860188\n",
      "mean loss is: 0.404099398012\n",
      "sum loss is: 10.5065843483\n",
      "mean loss is: 0.404099333268\n",
      "sum loss is: 10.506582665\n",
      "mean loss is: 0.404099268033\n",
      "sum loss is: 10.5065809688\n",
      "mean loss is: 0.40409920231\n",
      "sum loss is: 10.5065792601\n",
      "mean loss is: 0.404099136104\n",
      "sum loss is: 10.5065775387\n",
      "mean loss is: 0.404099069416\n",
      "sum loss is: 10.5065758048\n",
      "mean loss is: 0.404099002251\n",
      "sum loss is: 10.5065740585\n",
      "mean loss is: 0.404098934612\n",
      "sum loss is: 10.5065722999\n",
      "mean loss is: 0.404098866502\n",
      "sum loss is: 10.5065705291\n",
      "mean loss is: 0.404098797924\n",
      "sum loss is: 10.506568746\n",
      "mean loss is: 0.404098728882\n",
      "sum loss is: 10.5065669509\n",
      "mean loss is: 0.404098659378\n",
      "sum loss is: 10.5065651438\n",
      "mean loss is: 0.404098589416\n",
      "sum loss is: 10.5065633248\n",
      "mean loss is: 0.404098518999\n",
      "sum loss is: 10.506561494\n",
      "mean loss is: 0.40409844813\n",
      "sum loss is: 10.5065596514\n",
      "mean loss is: 0.404098376813\n",
      "sum loss is: 10.5065577971\n",
      "mean loss is: 0.40409830505\n",
      "sum loss is: 10.5065559313\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    parameters = pickle.load(open('../data/irt_parameters.p', 'rb'))\n",
    "    \n",
    "except:\n",
    "    print('no parameter file')\n",
    "    parameters = initial_parameters(expected_performance)\n",
    "    print(parameters)\n",
    "\n",
    "for i in xrange(100000):\n",
    "    gradients, sum_loss, mean_loss = calc_gradient_vector(expected_performance, parameters)\n",
    "    parameters = update(parameters, gradients)\n",
    "    \n",
    "    if i%100 ==1:\n",
    "        print('mean loss is: '+str(mean_loss))\n",
    "        print('sum loss is: '+str(sum_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(parameters, open('../data/irt_parameters.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "theta_day = parameters[0]\n",
    "sns.set_style('whitegrid')\n",
    "f, ax= plt.subplots(figsize = (14, 10))\n",
    "\n",
    "ax = sns.lineplot(x='day', y='theta', hue='uid', data=theta_day)\n",
    "#     ax.set_title('exc_num is'+str(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   day   uid     theta\n",
       " 0  2.0   1.0  0.811170\n",
       " 1  2.0   2.0 -1.938701\n",
       " 2  2.0   3.0 -0.473703\n",
       " 3  2.0   4.0  0.641702\n",
       " 4  2.0   5.0  0.269241\n",
       " 5  2.0   6.0 -1.846867\n",
       " 6  2.0   7.0 -1.289662\n",
       " 7  2.0   8.0  0.631750\n",
       " 8  2.0  10.0 -2.290156\n",
       " 9  2.0  11.0  0.103315,    exc_num     alpha         b\n",
       " 0      0.1  1.194511  0.009296\n",
       " 1      1.2  0.525048  1.048780\n",
       " 2      1.3  0.312792  2.790258\n",
       " 3      1.5  0.382282  2.624825)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
